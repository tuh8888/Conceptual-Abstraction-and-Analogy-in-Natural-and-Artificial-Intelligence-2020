
@article{abburuSurveyOntologyReasoners2012,
  title = {A {{Survey}} on {{Ontology Reasoners}} and {{Comparison}}},
  author = {Abburu, Sunitha},
  year = {2012},
  volume = {57},
  pages = {33--39},
  doi = {10.5120/9208-3748},
  abstract = {Reasoner is a software that is used to derive new facts from the existing ontologies. Some of the popular reasoners developed in the last few years are: Pellet, RACER, FACT++, Snorocket, Hermit, CEL, ELK, SWRL-IQ, TrOWL and others. This survey describes the reasoners that can be used as plug-in for either prot\'eg\'e or NeOn toolkit since these are most widely used ontology development tools. The current study describes the reasoners with their important features such as soundness, completeness, reasoning method, incremental classification etc. Finally this paper presents comparison of the reasoners with respect to their features. General},
  annotation = {ZSCC: 0000145},
  journal = {International Journal of Computer Applications},
  keywords = {\#nosource,description logic,Lawrence Hunter,neon toolkit,ontology,ontology reasoners,protégé,reasoner attributes,Recommendation},
  number = {17}
}

@inproceedings{agichteinSnowballExtractingRelations2000,
  title = {Snowball: {{Extracting}} Relations from Large Plain-Text Collections},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Digital Libraries}}},
  author = {Agichtein, Eugene and Gravano, Luis},
  year = {2000},
  pages = {85--94},
  doi = {10.1145/336597.336644},
  abstract = {Text documents often contain valuable structured data that is hidden in regular English sentences. This data is best exploited if available as a relational table that we could use for answering precise queries or for running data mining tasks. We explore a technique for extracting such tables from document collections that requires only a handful of training examples from users. These examples are used to generate extraction patterns, that in turn result in new tuples being extracted from the document collection. We build on this idea and present our Snowball system. Snowball introduces novel strategies for generating patterns and extracting tuples from plain-text documents. At each iteration of the extraction process, Snowball evaluates the quality of these patterns and tuples without human intervention, and keeps only the most reliable ones for the next iteration. In this paper we also develop a scalable evaluation methodology and metrics for our task, and present a thorough experimental evaluation of Snowball and comparable techniques over a collection of more than 300,000 newspaper documents.},
  annotation = {ZSCC: 0001528},
  file = {/home/harrisonpl/Documents/PDFs/Agichtein, Gravano - 2000 - Snowball.pdf},
  keywords = {HPL comprehensive exam,Natural language processing,Relation extraction}
}

@article{akhmedovOmicsPlaygroundComprehensive2020,
  title = {Omics {{Playground}}: A Comprehensive Self-Service Platform for Visualization, Analytics and Exploration of {{Big Omics Data}}},
  author = {Akhmedov, Murodzhon and Martinelli, Axel and Geiger, Roger and Kwee, Ivo},
  year = {2020},
  volume = {2},
  pages = {1--10},
  doi = {10.1093/nargab/lqz019},
  abstract = {As the cost of sequencing drops rapidly, the amount of `omics data increases exponentially, making data visualization and interpretation\textemdash `tertiary` analysis a bottleneck. Specialized analytical tools requiring technical expertise are available. However, consolidated and multi-faceted tools that are easy to use for life scientists is highly needed and currently lacking. Here we present Omics Playground, a user-friendly and interactive self-service bioinformatics platform for the in-depth analysis, visualization and interpretation of transcriptomics and proteomics data. It provides a large number of different tools in which special attention has been paid to single cell data. With Omics Playground, life scientists can easily perform complex data analysis and visualization without coding, and significantly reduce the time to discovery.},
  annotation = {ZSCC: 0000000},
  file = {/home/harrisonpl/Documents/PDFs/Akhmedov et al. - 2020 - Omics Playground.pdf},
  journal = {NAR Genomics and Bioinformatics},
  number = {1}
}

@article{albertiSyntaxNetModelsCoNLL2017,
  title = {{{SyntaxNet Models}} for the {{CoNLL}} 2017 {{Shared Task}}},
  author = {Alberti, Chris and Andor, Daniel and Bogatyy, Ivan and Collins, Michael and Gillick, Dan and Kong, Lingpeng and Koo, Terry and Ma, Ji and Omernick, Mark and Petrov, Slav and Thanapirom, Chayut and Tung, Zora and Weiss, David},
  year = {2017},
  month = mar,
  abstract = {We describe a baseline dependency parsing system for the CoNLL2017 Shared Task. This system, which we call "ParseySaurus," uses the DRAGNN framework [Kong et al, 2017] to combine transition-based recurrent parsing and tagging with character-based word representations. On the v1.3 Universal Dependencies Treebanks, the new system outpeforms the publicly available, state-of-the-art "Parsey's Cousins" models by 3.47\% absolute Labeled Accuracy Score (LAS) across 52 treebanks.},
  archivePrefix = {arXiv},
  eprint = {1703.04929},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Alberti et al. - 2017 - SyntaxNet Models for the CoNLL 2017 Shared Task.pdf;/home/harrisonpl/Zotero/storage/SSQJT5VC/1703.html},
  journal = {arXiv:1703.04929 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{alimovaComparativeAnalysisContext2019,
  title = {Comparative Analysis of Context Representation Models in the Relation Extraction Task from Biomedical Texts},
  author = {Alimova, Ilseyar and Tutubalina, Elena},
  year = {2019},
  volume = {2525},
  pages = {1--9},
  issn = {16130073},
  abstract = {This paper focuses on the task of extracting relations between entities in biomedical texts. This study aims to identify the most effective method for representing context between entities. We compare several context representation methods such as a bag of words representation, average word embeddings, sentence embedding, representations obtained by convolutional, recurrent neural networks, and bidirectional encoder representations from Transformers (BERT). We conduct a set of experiments on two benchmark corpora of patient electronic health records and scientific articles in English. As expected, thehighestclassificationresultswereobtainedwiththestate-of-theart neural architecture BERT.},
  annotation = {ZSCC: 0000000},
  file = {/home/harrisonpl/Documents/PDFs/Alimova, Tutubalina - 2019 - Comparative analysis of context representation models in the relation.pdf},
  journal = {CEUR Workshop Proceedings}
}

@article{alkhateebCanScientificDiscovery2017,
  title = {Can {{Scientific Discovery Be Automated}}?},
  author = {Alkhateeb, Ahmed},
  year = {2017},
  issn = {1072-7825},
  abstract = {Progress in the sciences can only move as fast as humans can think\textemdash outsourcing to A.I. could change that.},
  annotation = {ZSCC: 0000003},
  journal = {The Atlantic},
  keywords = {\#nosource,Lawrence Hunter,Recommendation}
}

@article{allahyariTextSummarizationTechniques2017,
  title = {Text {{Summarization Techniques}}: {{A Brief Survey}}},
  shorttitle = {Text {{Summarization Techniques}}},
  author = {Allahyari, Mehdi and Pouriyeh, Seyedamin and Assefi, Mehdi and Safaei, Saeid and D., Elizabeth and B., Juan and Kochut, Krys},
  year = {2017},
  volume = {8},
  issn = {2158107X},
  doi = {10.14569/ijacsa.2017.081052},
  abstract = {In recent years, there has been a explosion in the amount of text data from a variety of sources. This volume of text is an invaluable source of information and knowledge which needs to be effectively summarized to be useful. In this review, the main approaches to automatic text summarization are described. We review the different processes for summarization and describe the effectiveness and shortcomings of the different methods.},
  annotation = {ZSCC: 0000159},
  file = {/home/harrisonpl/Documents/PDFs/Allahyari et al. - 2017 - Text Summarization Techniques.pdf},
  journal = {International Journal of Advanced Computer Science and Applications},
  keywords = {Computer Science - Computation and Language,hunter lab,Lawrence Hunter,Natural language processing,Recommendation},
  number = {10}
}

@article{amidRobustBiTemperedLogistic2019,
  title = {Robust {{Bi}}-{{Tempered Logistic Loss Based}} on {{Bregman Divergences}}},
  author = {Amid, Ehsan and Warmuth, Manfred K. and Anil, Rohan and Koren, Tomer},
  year = {2019},
  volume = {1},
  pages = {1--14},
  abstract = {We introduce a temperature into the exponential function and replace the softmax output layer of neural nets by a high temperature generalization. Similarly, the logarithm in the log loss we use for training is replaced by a low temperature logarithm. By tuning the two temperatures we create loss functions that are non-convex already in the single layer case. When replacing the last layer of the neural nets by our bi-temperature generalization of logistic loss, the training becomes more robust to noise. We visualize the effect of tuning the two temperatures in a simple setting and show the efficacy of our method on large data sets. Our methodology is based on Bregman divergences and is superior to a related two-temperature method using the Tsallis divergence.},
  annotation = {ZSCC: 0000003},
  file = {/home/harrisonpl/Documents/PDFs/Amid et al. - 2019 - Robust Bi-Tempered Logistic Loss Based on Bregman Divergences.pdf},
  number = {1}
}

@article{andersonGenEthGeneralEthical2018,
  title = {{{GenEth}}: {{A}} General Ethical Dilemma Analyzer},
  author = {Anderson, Michael and Anderson, Susan Leigh},
  year = {2018},
  volume = {9},
  pages = {337--357},
  issn = {20814836},
  doi = {10.1515/pjbr-2018-0024},
  abstract = {We argue that ethically significant behavior of autonomous systems should be guided by explicit ethical principles determined through a consensus of ethicists. Such a consensus is likely to emerge in many areas in which intelligent autonomous systems are apt to be deployed and for the actions they are liable to undertake, as we are more likely to agree on how machines ought to treat us than on how human beings ought to treat one another. Given such a consensus, particular cases of ethical dilemmas where ethicists agree on the ethically relevant features and the right course of action can be used to help discover principles needed for ethical guidance of the behavior of autonomous systems. Such principles help ensure the ethical behavior of complex and dynamic systems and further serve as a basis for justification of this behavior. To provide assistance in discovering ethical principles, we have developed GenEth, a general ethical dilemma analyzer that, through a dialog with ethicists, uses inductive logic programming to codify ethical principles in any given domain. GenEth has been used to codify principles in a number of domains pertinent to the behavior of autonomous systems and these principles have been verified using an Ethical Turing Test, a test devised to compare the judgments of codified principles with that of ethicists.},
  annotation = {ZSCC: 0000065},
  file = {/home/harrisonpl/Documents/PDFs/Anderson, Anderson - 2018 - GenEth.pdf},
  journal = {Paladyn},
  keywords = {ethical Turing test,inductive logic programming,machine ethics,machine learning},
  number = {1}
}

@article{angusCollaborationLeadsCooperation2020,
  title = {Collaboration Leads to Cooperation on Sparse Networks},
  author = {Angus, Simon D. and Newton, Jonathan},
  year = {2020},
  volume = {16},
  issn = {15537358},
  doi = {10.1371/journal.pcbi.1007557},
  abstract = {For almost four decades, cooperation has been studied through the lens of the prisoner's dilemma game, with cooperation modelled as the play of a specific strategy. However, an alternative approach to cooperative behavior has recently been proposed. Known as collaboration, the new approach considers mutualistic strategic choice and can be applied to any game. Here, we bring these approaches together and study the effect of collaboration on cooperative dynamics in the standard prisoner's dilemma setting. It turns out that, from a baseline of zero cooperation in the absence of collaboration, even relatively rare opportunities to collaborate can support material, and robust, levels of cooperation. This effect is mediated by the interaction structure, such that collaboration leads to greater levels of cooperation when each individual strategically interacts with relatively few other individuals, matching well-known characteristics of human interaction networks. Conversely, collaboratively induced cooperation vanishes from dense networks, thus placing environmental limits on collaboration's successful role in cooperation.},
  annotation = {ZSCC: 0000000},
  file = {/home/harrisonpl/Documents/PDFs/Angus, Newton - 2020 - Collaboration leads to cooperation on sparse networks.pdf},
  journal = {PLoS Computational Biology},
  number = {1}
}

@article{antunesSupervisedLearningKnowledgeBased2017,
  title = {Supervised {{Learning}} and {{Knowledge}}-{{Based Approaches Applied}} to {{Biomedical Word Sense Disambiguation}}},
  author = {Antunes, Rui and Matos, S{\'e}rgio},
  year = {2017},
  month = dec,
  volume = {14},
  issn = {16134516},
  doi = {10.1515/jib-2017-0051},
  abstract = {Word sense disambiguation (WSD) is an important step in biomedical text mining, which is responsible for assigning an unequivocal concept to an ambiguous term, improving the accuracy of biomedical information extraction systems. In this work we followed supervised and knowledge-based disambiguation approaches, with the best results obtained by supervised means. In the supervised method we used bag-of-words as local features, and word embeddings as global features. In the knowledge-based method we combined word embeddings, concept textual definitions extracted from the UMLS database, and concept association values calculated from the MeSH co-occurrence counts from MEDLINE articles. Also, in the knowledge-based method, we tested different word embedding averaging functions to calculate the surrounding context vectors, with the goal to give more importance to closest words of the ambiguous term. The MSH WSD dataset, the most common dataset used for evaluating biomedical concept disambiguation, was used to evaluate our methods. We obtained a top accuracy of 95.6 \% by supervised means, while the best knowledge-based accuracy was 87.4 \%. Our results show that word embedding models improved the disambiguation accuracy, proving to be a powerful resource in the WSD task.},
  annotation = {ZSCC: 0000007},
  file = {/home/harrisonpl/Documents/PDFs/Antunes, Matos - 2017 - Supervised Learning and Knowledge-Based Approaches Applied to Biomedical Word.pdf},
  journal = {Journal of integrative bioinformatics},
  keywords = {Biomedical text mining,information extraction,word embeddings},
  number = {4}
}

@article{anzaloneSearchandreplaceGenomeEditing2019,
  title = {Search-and-Replace Genome Editing without Double-Strand Breaks or Donor {{DNA}}},
  author = {Anzalone, Andrew V. and Randolph, Peyton B. and Davis, Jessie R. and Sousa, Alexander A. and Koblan, Luke W. and Levy, Jonathan M. and Chen, Peter J. and Wilson, Christopher and Newby, Gregory A. and Raguram, Aditya and Liu, David R.},
  year = {2019},
  volume = {576},
  pages = {149--157},
  issn = {14764687},
  doi = {10.1038/s41586-019-1711-4},
  abstract = {Most genetic variants that contribute to disease1 are challenging to correct efficiently and without excess byproducts2\textendash 5. Here we describe prime editing, a versatile and precise genome editing method that directly writes new genetic information into a specified DNA site using a catalytically impaired Cas9 endonuclease fused to an engineered reverse transcriptase, programmed with a prime editing guide RNA (pegRNA) that both specifies the target site and encodes the desired edit. We performed more than 175 edits in human cells, including targeted insertions, deletions, and all 12 types of point mutation, without requiring double-strand breaks or donor DNA templates. We used prime editing in human cells to correct, efficiently and with few byproducts, the primary genetic causes of sickle cell disease (requiring a transversion in HBB) and Tay\textendash Sachs disease (requiring a deletion in HEXA); to install a protective transversion in PRNP; and to insert various tags and epitopes precisely into target loci. Four human cell lines and primary post-mitotic mouse cortical neurons support prime editing with varying efficiencies. Prime editing shows higher or similar efficiency and fewer byproducts than homology-directed repair, has complementary strengths and weaknesses compared to base editing, and induces much lower off-target editing than Cas9 nuclease at known Cas9 off-target sites. Prime editing substantially expands the scope and capabilities of genome editing, and in principle could correct up to 89\% of known genetic variants associated with human diseases.},
  annotation = {ZSCC: 0000104},
  file = {/home/harrisonpl/Documents/PDFs/Anzalone et al. - 2019 - Search-and-replace genome editing without double-strand breaks or donor DNA.pdf},
  journal = {Nature},
  number = {7785}
}

@article{apweilerUniProtUniversalProtein2004,
  title = {{{UniProt}}: The Universal Protein Knowledgebase},
  author = {Apweiler, R.},
  year = {2004},
  volume = {32},
  pages = {115D-119},
  issn = {1362-4962},
  doi = {10.1093/nar/gkh131},
  abstract = {To provide the scientific community with a single, centralized, authoritative resource for protein sequences and functional information, the Swiss-Prot, TrEMBL and PIR protein database activities have united to form the Universal Protein Knowledgebase (UniProt) consortium. Our mission is to provide a comprehensive, fully classified, richly and accurately annotated protein sequence knowledgebase, with extensive cross-references and query interfaces. The central database will have two sections, corresponding to the familiar Swiss-Prot (fully manually curated entries) and TrEMBL (enriched with automated classification, annotation and extensive cross-references). For convenient sequence searches, UniProt also provides several non-redundant sequence databases. The UniProt NREF (UniRef) databases provide representative subsets of the knowledgebase suitable for efficient searching. The comprehensive UniProt Archive (UniParc) is updated daily from many public source databases. The UniProt databases can be accessed online (http://www.uniprot.org) or downloaded in several formats (ftp://ftp.uniprot.org/pub). The scientific community is encouraged to submit data for inclusion in UniProt.},
  annotation = {ZSCC: 0002618},
  file = {/home/harrisonpl/Documents/PDFs/Apweiler - 2004 - UniProt.pdf},
  journal = {Nucleic Acids Research},
  number = {90001}
}

@article{arakiInteroperableAnnotationEvents2018,
  title = {Interoperable {{Annotation}} of {{Events}} and {{Event Relations}} across {{Domains}}},
  author = {Araki, Jun and Mulaffer, Lamana and Pandian, Arun and Yamakawa, Yukari and Oflazer, Kemal and Mitamura, Teruko},
  year = {2018},
  pages = {10--20},
  abstract = {This paper presents methodologies for interoperable annotation of events and event relations across different domains, based on notions proposed in prior work. In addition to the interop-erability, our annotation scheme supports a wide coverage of events and event relations. We employ the methodologies to annotate events and event relations on Simple Wikipedia articles in 10 different domains. Our analysis demonstrates that the methodologies can allow us to annotate events and event relations in a principled manner against the wide variety of domains. Despite our relatively wide and flexible annotation of events, we achieve high inter-annotator agreement on event annotation. As for event relations, we obtain reasonable inter-annotator agreement. We also provide an analysis of issues on annotation of events and event relations that could lead to annotators' disagreement.},
  annotation = {ZSCC: 0000000},
  file = {/home/harrisonpl/Documents/PDFs/Araki et al. - 2018 - Interoperable Annotation of Events and Event Relations across Domains.pdf}
}

@article{aretzAdvantagesPitfallsMass2016,
  title = {Advantages and Pitfalls of Mass Spectrometry Based Metabolome Profiling in Systems Biology},
  author = {Aretz, Ina and Meierhofer, David},
  year = {2016},
  volume = {17},
  pages = {632},
  issn = {14220067},
  doi = {10.3390/ijms17050632},
  abstract = {Mass spectrometry-based metabolome profiling became the method of choice in systems biology approaches and aims to enhance biological understanding of complex biological systems. Genomics, transcriptomics, and proteomics are well established technologies and are commonly used by many scientists. In comparison, metabolomics is an emerging field and has not reached such high-throughput, routine and coverage than other omics technologies. Nevertheless, substantial improvements were achieved during the last years. Integrated data derived from multi-omics approaches will provide a deeper understanding of entire biological systems. Metabolome profiling is mainly hampered by its diversity, variation of metabolite concentration by several orders of magnitude and biological data interpretation. Thus, multiple approaches are required to cover most of the metabolites. No software tool is capable of comprehensively translating all the data into a biologically meaningful context yet. In this review, we discuss the advantages of metabolome profiling and main obstacles limiting progress in systems biology.},
  annotation = {ZSCC: 0000082},
  file = {/home/harrisonpl/Documents/PDFs/Aretz, Meierhofer - 2016 - Advantages and pitfalls of mass spectrometry based metabolome profiling in.pdf},
  journal = {International Journal of Molecular Sciences},
  keywords = {Derivatization,LC-MS,Mass spectrometry,Metabolome profiling,Multiple reaction monitoring (MRM),Nuclear magnetic resonance (NMR) spectroscopy,Systems biology},
  number = {5}
}

@inproceedings{arighiProceedingsBioCreativeVI2017,
  title = {Proceedings of the {{BioCreative VI Workshop}}},
  editor = {Arighi, Cecilia and Wang, Qinghua and Wu, Cathy},
  year = {2017},
  address = {{Bethestda}},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/2017 - Proceedings of the BioCreative VI Workshop.pdf},
  keywords = {Competition,HPL comprehensive exam,Natural language processing,Relation extraction}
}

@article{armitageMissingValueImputation2015,
  title = {Missing Value Imputation Strategies for Metabolomics Data},
  author = {Armitage, Emily Grace and Godzien, Joanna and {Alonso-Herranz}, Vanesa and {L{\'o}pez-Gonz{\'a}lvez}, {\'A}ngeles and Barbas, Coral},
  year = {2015},
  volume = {36},
  pages = {3050--3060},
  issn = {15222683},
  doi = {10.1002/elps.201500352},
  abstract = {The origin of missing values can be caused by different reasons and depending on these origins missing values should be considered differently and dealt with in different ways. In this research, four methods of imputation have been compared with respect to revealing their effects on the normality and variance of data, on statistical significance and on the approximation of a suitable threshold to accept missing data as truly missing. Additionally, the effects of different strategies for controlling familywise error rate or false discovery and how they work with the different strategies for missing value imputation have been evaluated. Missing values were found to affect normality and variance of data and k-means nearest neighbour imputation was the best method tested for restoring this. Bonferroni correction was the best method for maximizing true positives and minimizing false positives and it was observed that as low as 40\% missing data could be truly missing. The range between 40 and 70\% missing values was defined as a "gray area" and therefore a strategy has been proposed that provides a balance between the optimal imputation strategy that was k-means nearest neighbor and the best approximation of positioning real zeros.},
  annotation = {ZSCC: 0000063},
  file = {/home/harrisonpl/Documents/PDFs/Armitage et al. - 2015 - Missing value imputation strategies for metabolomics data.pdf},
  journal = {Electrophoresis},
  keywords = {CE-MS,Data,False-discovery rate,Imputation,K-nearest neighbour,Metabolomics,Missing values},
  number = {24}
}

@article{arpltCloserLookMemorization2017,
  title = {A Closer Look at Memorization in Deep Networks},
  author = {Arplt, Devansh and Jastrz{\c e}bskl, Stanislaw and Bailas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S. and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Benglo, Yoshua and {Lacoste-Julien}, Simon},
  year = {2017},
  volume = {1},
  pages = {350--359},
  abstract = {We examine the rote of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first, in our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/Arplt et al. - 2017 - A closer look at memorization in deep networks.pdf},
  journal = {34th International Conference on Machine Learning, ICML 2017}
}

@article{ashburnerGeneOntologyTool2000,
  title = {Gene Ontology: {{Tool}} for the Unification of Biology},
  shorttitle = {Gene {{Ontology}}},
  author = {Ashburner, Michael and Ball, Catherine A. and Blake, Judith A. and Botstein, David and Butler, Heather and Cherry, J. Michael and Davis, Allan P. and Dolinski, Kara and Dwight, Selina S. and Eppig, Janan T. and Harris, Midori A. and Hill, David P. and {Issel-Tarver}, Laurie and Kasarskis, Andrew and Lewis, Suzanna and Matese, John C. and Richardson, Joel E. and Ringwald, Martin and Rubin, Gerald M. and Sherlock, Gavin},
  year = {2000},
  month = may,
  volume = {25},
  pages = {25--29},
  issn = {10614036},
  doi = {10.1038/75556},
  abstract = {Genomic sequencing has made it clear that a large fraction of the genes specifying the core biological functions are shared by all eukaryotes. Knowledge of the biological role of such shared proteins in one organism can often be transferred to other organisms. The goal of the Gene Ontology Consortium is to produce a dynamic, controlled vocabulary that can be applied to all eukaryotes even as knowledge of gene and protein roles in cells is accumulating and changing. To this end, three independent ontologies accessible on the World-Wide Web (http://www.geneontology.org) are being constructed: biological process, molecular function and cellular component.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/Ashburner et al. - 2000 - Gene ontology.pdf},
  journal = {Nature Genetics},
  keywords = {comp-exam,HPL comprehensive exam},
  language = {en},
  number = {1}
}

@article{aydarNeuralRelationExtraction2020,
  title = {Neural Relation Extraction: A Survey},
  shorttitle = {Neural Relation Extraction},
  author = {Aydar, Mehmet and Bozal, Ozge and Ozbay, Furkan},
  year = {2020},
  month = jun,
  volume = {2007},
  pages = {arXiv:2007.04247},
  abstract = {Neural relation extraction discovers semantic relations between entities from unstructured text using deep learning methods. In this study, we present a comprehensive review of methods on neural network based relation extraction. We discuss advantageous and incompetent sides of existing studies and investigate additional research directions and improvement ideas in this field.},
  file = {/home/harrisonpl/Documents/PDFs/Aydar et al. - 2020 - Neural relation extraction.pdf},
  journal = {arXiv e-prints},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{azoulaychristianfons-rosenjoshuagraffzivinDOESSCIENCEADVANCE2015,
  title = {{{DOES SCIENCE ADVANCE ONE FUNERAL AT A TIME}}? {{Does Science Advance One Funeral}} at a {{Time}}?},
  author = {{Azoulay Christian Fons-Rosen Joshua Graff Zivin}, Pierre S and Williams, Heidi and Jaravel, Xavier and Li, Danielle and Srivastava, Sameer and Stern, Scott and Weinberg, Bruce and Azoulay, Pierre and {Fons-Rosen}, Christian and Graff Zivin, Joshua S and {Fons-Rosen Christian Fons-Rosen}, Christian},
  year = {2015},
  abstract = {We study the extent to which eminent scientists shape the vitality of their areas of scientific inquiry by examining entry rates into the subfields of 452 academic life scientists who pass away prematurely. Consistent with previous research, the flow of articles by collaborators into affected fields decreases precipitously after the death of a star scientist. In contrast, we find that the flow of articles by non-collaborators increases by 8.6\% on average. These additional contributions are disproportionately likely to be highly cited. They are also more likely to be authored by scientists who were not previously active in the deceased superstar's field. Intellectual, social, and resource barriers all impede entry, with outsiders only entering subfields that offer a less hostile landscape for the support and acceptance of "foreign" ideas. Overall, our results suggest that once in control of the commanding heights of their fields, star scientists tend to hold on to their exalted position a bit too long.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/Azoulay Christian Fons-Rosen Joshua Graff Zivin et al. - 2015 - DOES SCIENCE ADVANCE ONE FUNERAL AT A TIME.pdf}
}

@article{azurMultipleImputationChained2011,
  title = {Multiple Imputation by Chained Equations: {{What}} Is It and How Does It Work?},
  author = {Azur, Melissa J. and Stuart, Elizabeth A. and Frangakis, Constantine and Leaf, Philip J.},
  year = {2011},
  volume = {20},
  pages = {40--49},
  issn = {10498931},
  doi = {10.1002/mpr.329},
  abstract = {Multivariate imputation by chained equations (MICE) has emerged as a principled method of dealing with missing data. Despite properties that make MICE particularly useful for large imputation procedures and advances in software development that now make it accessible to many researchers, many psychiatric researchers have not been trained in these methods and few practical resources exist to guide researchers in the implementation of this technique. This paper provides an introduction to the MICE method with a focus on practical aspects and challenges in using this method. A brief review of software programs available to implement MICE and then analyze multiply imputed data is also provided. \textcopyright{} 2011 John Wiley \& Sons, Ltd.},
  annotation = {ZSCC: 0001047},
  file = {/home/harrisonpl/Documents/PDFs/Azur et al. - 2011 - Multiple imputation by chained equations.pdf},
  journal = {International Journal of Methods in Psychiatric Research},
  keywords = {Analyze,Missing data,Multiple imputation},
  number = {1}
}

@article{badaConceptAnnotationCRAFT2012,
  title = {Concept Annotation in the {{CRAFT}} Corpus},
  author = {Bada, Michael and Eckert, Miriam and Evans, Donald and Garcia, Kristin and Shipley, Krista and Sitnikov, Dmitry and Baumgartner, William A. and Cohen, K. B. and Verspoor, Karin and Blake, Judith A. and Hunter, Lawrence E.},
  year = {2012},
  month = jul,
  volume = {13},
  pages = {161},
  issn = {14712105},
  doi = {10.1186/1471-2105-13-161},
  abstract = {Background: Manually annotated corpora are critical for the training and evaluation of automated methods to identify concepts in biomedical text.Results: This paper presents the concept annotations of the Colorado Richly Annotated Full-Text (CRAFT) Corpus, a collection of 97 full-length, open-access biomedical journal articles that have been annotated both semantically and syntactically to serve as a research resource for the biomedical natural-language-processing (NLP) community. CRAFT identifies all mentions of nearly all concepts from nine prominent biomedical ontologies and terminologies: the Cell Type Ontology, the Chemical Entities of Biological Interest ontology, the NCBI Taxonomy, the Protein Ontology, the Sequence Ontology, the entries of the Entrez Gene database, and the three subontologies of the Gene Ontology. The first public release includes the annotations for 67 of the 97 articles, reserving two sets of 15 articles for future text-mining competitions (after which these too will be released). Concept annotations were created based on a single set of guidelines, which has enabled us to achieve consistently high interannotator agreement.Conclusions: As the initial 67-article release contains more than 560,000 tokens (and the full set more than 790,000 tokens), our corpus is among the largest gold-standard annotated biomedical corpora. Unlike most others, the journal articles that comprise the corpus are drawn from diverse biomedical disciplines and are marked up in their entirety. Additionally, with a concept-annotation count of nearly 100,000 in the 67-article subset (and more than 140,000 in the full collection), the scale of conceptual markup is also among the largest of comparable corpora. The concept annotations of the CRAFT Corpus have the potential to significantly advance biomedical text mining by providing a high-quality gold standard for NLP systems. The corpus, annotation guidelines, and other associated resources are freely available at http://bionlp-corpora.sourceforge.net/CRAFT/index.shtml. \textcopyright{} 2012 Bada et al.; licensee BioMed Central Ltd.},
  annotation = {ZSCC: 0000157},
  file = {/home/harrisonpl/Documents/PDFs/Bada et al. - 2012 - Concept annotation in the CRAFT corpus.pdf},
  journal = {BMC Bioinformatics},
  keywords = {Biomedical Text,Computational Biology,Controlled,CRAFT,Data Mining,Databases,Factual,Gene Ontology,hunter lab,Information Storage and Retrieval,Knowtator,Natural language processing,Natural Language Processing,NCBI Taxonomy,Open Biomedical Ontology,Resource Description Framework,Semantics,Vocabulary},
  number = {1}
}

@article{badaGoldstandardOntologybasedAnatomical2017,
  title = {Gold-Standard Ontology-Based Anatomical Annotation in the {{CRAFT Corpus}}},
  author = {Bada, Michael and Vasilevsky, Nicole and Baumgartner, William A. and Haendel, Melissa and Hunter, Lawrence E.},
  year = {2017},
  volume = {2017},
  pages = {1--13},
  issn = {17580463},
  doi = {10.1093/database/bax087},
  abstract = {Gold-standard annotated corpora have become important resources for the training and testing of natural-language-processing (NLP) systems designed to support biocuration efforts, and ontologies are increasingly used to facilitate curational consistency and semantic integration across disparate resources. Bringing together the respective power of these, the Colorado Richly Annotated Full-Text (CRAFT) Corpus, a collection of full-length, open-access biomedical journal articles with extensive manually created syntactic, formatting and semantic markup, was previously created and released. This initial public release has already been used in multiple projects to drive development of systems focused on a variety of biocuration, search, visualization, and semantic and syntactic NLP tasks. Building on its demonstrated utility, we have expanded the CRAFT Corpus with a large set of manually created semantic annotations relying on Uberon, an ontology representing anatomical entities and life-cycle stages of multicellular organisms across species as well as types of multicellular organisms defined in terms of life-cycle stage and sexual characteristics. This newly created set of annotations, which has been added for v2.1 of the corpus, is by far the largest publicly available collection of gold-standard anatomical markup and is the first large-scale effort at manual markup of biomedical text relying on the entirety of an anatomical terminology, as opposed to annotation with a small number of high-level anatomical categories, as performed in previous corpora. In addition to presenting and discussing this newly available resource, we apply it to provide a performance baseline for the automatic annotation of anatomical concepts in biomedical text using a prominent concept recognition system. The full corpus, released with a CC BY 3.0 license, may be downloaded from http://bionlp-corpora.sourceforge.net/CRAFT/index.shtml. Database URL: http://bionlp-corpora.sourceforge.net/CRAFT/index.shtml.},
  annotation = {ZSCC: 0000001},
  file = {/home/harrisonpl/Documents/PDFs/Bada et al. - 2017 - Gold-standard ontology-based anatomical annotation in the CRAFT Corpus.pdf},
  journal = {Database : the journal of biological databases and curation},
  keywords = {hunter lab}
}

@article{bajuszWhyTanimotoIndex2015,
  title = {Why Is {{Tanimoto}} Index an Appropriate Choice for Fingerprint-Based Similarity Calculations?},
  author = {Bajusz, D{\'a}vid and R{\'a}cz, Anita and H{\'e}berger, K{\'a}roly},
  year = {2015},
  volume = {7},
  pages = {20},
  issn = {17582946},
  doi = {10.1186/s13321-015-0069-3},
  abstract = {Background: Cheminformaticians are equipped with a very rich toolbox when carrying out molecular similarity calculations. A large number of molecular representations exist, and there are several methods (similarity and distance metrics) to quantify the similarity of molecular representations. In this work, eight well-known similarity/distance metrics are compared on a large dataset of molecular fingerprints with sum of ranking differences (SRD) and ANOVA analysis. The effects of molecular size, selection methods and data pretreatment methods on the outcome of the comparison are also assessed. Results: A supplier database (https://mcule.com/) was used as the source of compounds for the similarity calculations in this study. A large number of datasets, each consisting of one hundred compounds, were compiled, molecular fingerprints were generated and similarity values between a randomly chosen reference compound and the rest were calculated for each dataset. Similarity metrics were compared based on their ranking of the compounds within one experiment (one dataset) using sum of ranking differences (SRD), while the results of the entire set of experiments were summarized on box and whisker plots. Finally, the effects of various factors (data pretreatment, molecule size, selection method) were evaluated with analysis of variance (ANOVA). Conclusions: This study complements previous efforts to examine and rank various metrics for molecular similarity calculations. Here, however, an entirely general approach was taken to neglect any a priori knowledge on the compounds involved, as well as any bias introduced by examining only one or a few specific scenarios. The Tanimoto index, Dice index, Cosine coefficient and Soergel distance were identified to be the best (and in some sense equivalent) metrics for similarity calculations, i.e. these metrics could produce the rankings closest to the composite (average) ranking of the eight metrics. The similarity metrics derived from Euclidean and Manhattan distances are not recommended on their own, although their variability and diversity from other similarity metrics might be advantageous in certain cases (e.g. for data fusion). Conclusions are also drawn regarding the effects of molecule size, selection method and data pretreatment on the ranking behavior of the studied metrics.},
  annotation = {ZSCC: 0000300},
  file = {/home/harrisonpl/Documents/PDFs/Bajusz et al. - 2015 - Why is Tanimoto index an appropriate choice for fingerprint-based similarity.pdf},
  journal = {Journal of Cheminformatics},
  keywords = {Analysis of variance,Data fusion,Distance metrics,Fingerprint,Ranking,Similarity,Sum of ranking differences},
  number = {1}
}

@article{bandrowskiOntologyBiomedicalInvestigations2016,
  title = {The {{Ontology}} for {{Biomedical Investigations}}},
  author = {Bandrowski, Anita and Brinkman, Ryan and Brochhausen, Mathias and Brush, Matthew H. and Bug, Bill and Chibucos, Marcus C. and Clancy, Kevin and Courtot, M{\'e}lanie and Derom, Dirk and Dumontier, Michel and Fan, Liju and Fostel, Jennifer and Fragoso, Gilberto and Gibson, Frank and {Gonzalez-Beltran}, Alejandra and Haendel, Melissa A. and He, Yongqun and Heiskanen, Mervi and {Hernandez-Boussard}, Tina and Jensen, Mark and Lin, Yu and Lister, Allyson L. and Lord, Phillip and Malone, James and Manduchi, Elisabetta and McGee, Monnie and Morrison, Norman and Overton, James A. and Parkinson, Helen and Peters, Bjoern and {Rocca-Serra}, Philippe and Ruttenberg, Alan and Sansone, Susanna Assunta and Scheuermann, Richard H. and Schober, Daniel and Smith, Barry and Soldatova, Larisa N. and Stoeckert, Christian J. and Taylor, Chris F. and Torniai, Carlo and Turner, Jessica A. and Vita, Randi and Whetzel, Patricia L. and Zheng, Jie},
  year = {2016},
  volume = {11},
  abstract = {The Ontology for Biomedical Investigations (OBI) is an ontology that provides terms with precisely defined meanings to describe all aspects of how investigations in the biological and medical domains are conducted. OBI re-uses ontologies that provide a representation of biomedical knowledge from the Open Biological and Biomedical Ontologies (OBO) project and adds the ability to describe how this knowledge was derived. We here describe the state of OBI and several applications that are using it, such as adding semantic expressivity to existing databases, building data entry forms, and enabling interoperability between knowledge resources. OBI covers all phases of the investigation process, such as planning, execution and reporting. It represents information and material entities that participate in these processes, as well as roles and functions. Prior to OBI, it was not possible to use a single internally consistent resource that could be applied to multiple types of experiments for these applications. OBI has made this possible by creating terms for entities involved in biological and medical investigations and by importing parts of other biomedical ontologies such as GO, Chemical Entities of Biological Interest (ChEBI) and Phenotype Attribute and Trait Ontology (PATO) without altering their meaning. OBI is being used in a wide range of projects covering genomics, multi-omics, immunology, and catalogs of services. OBI has also spawned other ontologies (Information Artifact Ontology) and methods for importing parts of ontologies (Minimum information to reference an external ontology term (MIREOT)). The OBI project is an open cross-disciplinary collaborative effort, encompassing multiple research communities from around the globe. To date, OBI has created 2366 classes and 40 relations along with textual and formal definitions. The OBI Consortium maintains a web resource (http://obi-ontology.org) providing details on the people, policies, and issues being addressed in association with OBI. The current release of OBI is available at http://purl. obolibrary.org/obo/obi.owl.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/Bandrowski et al. - 2016 - The Ontology for Biomedical Investigations.pdf},
  keywords = {ontology}
}

@article{barbeSevereSideEffects1999,
  title = {Severe Side Effects and Drug Plasma Concentrations in Preterm Infants Treated with Doxapram},
  author = {Barb{\'e}, Fran{\c c}oise and Hansen, Catherine and Badonnel, Yves and Legagneur, Henryse and Vert, Paul and Boutroy, Marie Jeanne},
  year = {1999},
  issn = {01634356},
  doi = {10.1097/00007691-199910000-00011},
  abstract = {A high-performance liquid chromatography method has been developed for simultaneous determination of doxapram and its metabolites including ketodoxapram, the main and only active metabolite. The aim of the study was to evaluate this microtechnique and to report the cases involving severe adverse effects to determine toxic plasma levels in neonates. The method was found to be selective, and showed a good baseline separation of doxapram and metabolites. Recovery, linearity, intraday/interday precision, and limit of detection determined in aqueous solutions and in spiked plasma were satisfactory. The assay is simple, rapid, and plasma-sparing, which represents a true advantage in managing neonates. Case analysis was performed in two consecutive periods: 124 preterm infants in the first period and 173 in the second period. Severe toxic effects were observed in 4 cases in the first period, with doxapram plus keto-doxapram levels 9 mg/L. In the second period, only one case was observed. High-range plasma concentrations were significantly less frequent in the second period than in the first one. The authors conclude that measuring doxapram plus keto-doxapram in plasma may be of interest to avoid severe toxic effects in preterm neonates treated with doxapram.},
  annotation = {ZSCC: 0000021},
  journal = {Therapeutic Drug Monitoring},
  keywords = {\#nosource,Analysis,Doxapram,High performance liquid chromatography,Newborn,Toxicity}
}

@article{bardOntologyCellTypes2005,
  title = {An Ontology for Cell Types.},
  author = {Bard, Jonathan and Rhee, Seung Y. and Ashburner, Michael},
  year = {2005},
  volume = {6},
  pages = {R21},
  issn = {14656914},
  doi = {10.1186/gb-2005-6-2-r21},
  abstract = {We describe an ontology for cell types that covers the prokaryotic, fungal, animal and plant worlds. It includes over 680 cell types. These cell types are classified under several generic categories and are organized as a directed acyclic graph. The ontology is available in the formats adopted by the Open Biological Ontologies umbrella and is designed to be used in the context of model organism genome and other biological databases. The ontology is freely available at http://obo.sourceforge.net/ and can be viewed using standard ontology visualization tools such as OBO-Edit and COBrA.},
  annotation = {ZSCC: 0000422},
  file = {/home/harrisonpl/Documents/PDFs/Bard et al. - 2005 - An ontology for cell types.pdf},
  journal = {Genome biology},
  keywords = {ontology},
  number = {2}
}

@article{barthaAnalogyNaturalSciences2015,
  title = {Analogy in the Natural Sciences: Meeting {{Hesse}}'s Challenge},
  shorttitle = {Analogy in the Natural Sciences},
  author = {Bartha, Paul},
  year = {2015},
  volume = {3},
  pages = {47--68},
  issn = {2282-0248},
  doi = {10.4454/philinq.v3i1.114},
  abstract = {Hesse's challenge, over a period of many years, was to provide a theory of scientific concepts and a logic of science, both of which were based on analogies and metaphors. We can equally well understand her challenge as the development of a theory of metaphor and analogy, with the natural sciences serving as an important special case. This paper explores two distinct ways in which we might see analogies in science as a special case in relation to a general theory of analogy.On the leading special case view, understanding how analogies work in science is the key to developing a general theory. On the limiting case view, providing a general theory, especially of how analogies work in everyday contexts, is a precursor to understanding their specialized role in science. While both approaches are present in Hesse's work, I suggest that the former is associated with her early (1966) work on analogical arguments and the latter with her later (1974; 1988) theories of metaphor and meaning. Her shift towards the limiting case view is associated with growing pessimism about the prospects for inductive logic. Yet the distinction remains important in current work on analogy: Hesse's challenge is to reconcile normative theories of analogical reasoning with computational models of analogical cognition.},
  copyright = {Copyright (c)},
  file = {/home/harrisonpl/Documents/PDFs/Bartha - 2015 - Analogy in the natural sciences.pdf;/home/harrisonpl/Zotero/storage/5PCVCXD2/114.html},
  journal = {Philosophical Inquiries},
  keywords = {analogical reasoning,analogy,Mary Hesse.,metaphor},
  language = {en},
  number = {1}
}

@article{batchelorOntologicalDependenceDispositions2010,
  title = {Ontological Dependence, Dispositions and Institutional Reality in Chemistry},
  author = {Batchelor, Colin and Hastings, Janna and Steinbeck, Christoph},
  year = {2010},
  volume = {209},
  pages = {271--284},
  issn = {09226389},
  doi = {10.3233/978-1-60750-535-8-271},
  abstract = {Biochemical 'small molecules' are involved in all living processes across all biological domains. Chemical ontologies provide structured chemical data and thereby support cross-disciplinary and integrative research across systems biology, chemogenomics and metabolomics. Efforts are underway to align ChEBI with upper level ontologies such as BFO, but confusion persists as to the ontological status of the ChEBI 'role' entities, which refer to continuants which inhere in chemical entities by virtue of the activity of the chemical entities. We provide a formal classification of these 'role' entities according to the continuants and endurants on which they ontologically depend, discuss the nature of chemical dispositions and the relevance of institutional reality, and address granularity issues in modelling chemical activity. \textcopyright{} 2010 The authors and IOS Press. All rights reserved.},
  annotation = {ZSCC: 0000018},
  journal = {Frontiers in Artificial Intelligence and Applications},
  keywords = {\#nosource,Chemistry,Disposition,ontology,Ontology}
}

@article{batemanUniProtUniversalProtein2017,
  title = {{{UniProt}}: {{The}} Universal Protein Knowledgebase},
  author = {Bateman, Alex and Martin, Maria Jesus and O'Donovan, Claire and Magrane, Michele and Alpi, Emanuele and Antunes, Ricardo and Bely, Benoit and Bingley, Mark and Bonilla, Carlos and Britto, Ramona and Bursteinas, Borisas and {Bye-AJee}, Hema and Cowley, Andrew and Da Silva, Alan and De Giorgi, Maurizio and Dogan, Tunca and Fazzini, Francesco and Castro, Leyla Garcia and Figueira, Luis and Garmiri, Penelope and Georghiou, George and Gonzalez, Daniel and {Hatton-Ellis}, Emma and Li, Weizhong and Liu, Wudong and Lopez, Rodrigo and Luo, Jie and Lussi, Yvonne and MacDougall, Alistair and Nightingale, Andrew and Palka, Barbara and Pichler, Klemens and Poggioli, Diego and Pundir, Sangya and Pureza, Luis and Qi, Guoying and Rosanoff, Steven and Saidi, Rabie and Sawford, Tony and Shypitsyna, Aleksandra and Speretta, Elena and Turner, Edward and Tyagi, Nidhi and Volynkin, Vladimir and Wardell, Tony and Warner, Kate and Watkins, Xavier and Zaru, Rossana and Zellner, Hermann and Xenarios, Ioannis and Bougueleret, Lydie and Bridge, Alan and Poux, Sylvain and Redaschi, Nicole and Aimo, Lucila and ArgoudPuy, Ghislaine and Auchincloss, Andrea and Axelsen, Kristian and Bansal, Parit and Baratin, Delphine and Blatter, Marie Claude and Boeckmann, Brigitte and Bolleman, Jerven and Boutet, Emmanuel and Breuza, Lionel and {Casal-Casas}, Cristina and De Castro, Edouard and Coudert, Elisabeth and Cuche, Beatrice and Doche, Mikael and Dornevil, Dolnide and Duvaud, Severine and Estreicher, Anne and Famiglietti, Livia and Feuermann, Marc and Gasteiger, Elisabeth and Gehant, Sebastien and Gerritsen, Vivienne and Gos, Arnaud and {Gruaz-Gumowski}, Nadine and Hinz, Ursula and Hulo, Chantal and Jungo, Florence and Keller, Guillaume and Lara, Vicente and Lemercier, Philippe and Lieberherr, Damien and Lombardot, Thierry and Martin, Xavier and Masson, Patrick and Morgat, Anne and Neto, Teresa and Nouspikel, Nevila and Paesano, Salvo and Pedruzzi, Ivo and Pilbout, Sandrine and Pozzato, Monica and Pruess, Manuela and Rivoire, Catherine and Roechert, Bernd and Schneider, Michel and Sigrist, Christian and Sonesson, Karin and Staehli, Sylvie and Stutz, Andre and Sundaram, Shyamala and Tognolli, Michael and Verbregue, Laure and Veuthey, Anne Lise and Wu, Cathy H. and Arighi, Cecilia N. and Arminski, Leslie and Chen, Chuming and Chen, Yongxing and Garavelli, John S. and Huang, Hongzhan and Laiho, Kati and McGarvey, Peter and Natale, Darren A. and Ross, Karen and Vinayaka, C. R. and Wang, Qinghua and Wang, Yuqi and Yeh, Lai Su and Zhang, Jian},
  year = {2017},
  volume = {45},
  pages = {D158--D169},
  issn = {13624962},
  doi = {10.1093/nar/gkw1099},
  abstract = {The UniProt knowledgebase is a large resource of protein sequences and associated detailed annotation. The database contains over 60 million sequences, of which over half a million sequences have been curated by experts who critically review experimental and predicted data for each protein. The remainder are automatically annotated based on rule systems that rely on the expert curated knowledge. Since our last update in 2014, we have more than doubled the number of reference proteomes to 5631, giving a greater coverage of taxonomic diversity. We implemented a pipeline to remove redundant highly similar proteomes that were causing excessive redundancy in UniProt. The initial run of this pipeline reduced the number of sequences in UniProt by 47 million. For our users interested in the accessory proteomes, we have made available sets of pan proteome sequences that cover the diversity of sequences for each species that is found in its strains and sub-strains. To help interpretation of genomic variants, we provide tracks of detailed protein information for the major genome browsers. We provide a SPARQL endpoint that allows complex queries of the more than 22 billion triples of data in UniProt (http://sparql.uniprot.org/). UniProt resources can be accessed via the website at http://www.uniprot.org/.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/Bateman et al. - 2017 - UniProt.pdf},
  journal = {Nucleic Acids Research},
  number = {D1}
}

@article{batistaLargeScaleSemanticRelationship2016,
  title = {Large-{{Scale Semantic Relationship Extraction}} for {{Information Discovery}}},
  author = {Batista, David Soares and Gaspar, Costa},
  year = {2016},
  pages = {193},
  issn = {0001-7884},
  abstract = {This paper investigates the role of envelope fluctuations in simultaneous masking conditions. Thresholds for tones in noise with a flat temporal envelope (low-noise noise, LNN) were compared with those in Gaussian noise. All measurements were performed with a running-noise presentation of 500-ms maskers. The sinusoidal signal was spectrally and temporally centered in the masker. The main findings were: (a) The 5.5-dB threshold difference between 100-Hz-wide Gaussian and LNN maskers at 1 kHz that was previously observed using frozen noise (cf. Hartmann and Pumplin [J. Acoust. Soc. Am. 83, 2277-2289 (1988)]) is also apparant for running noise, although thresholds are generally higher in the latter condition. (b) The threshold difference between Gaussian and LNN maskers at 1 kHz reaches a maximum of 9.4 dB at a masker bandwidth of 25 Hz, while at 10 kHz, the difference reaches a maximum of 15 dB at bandwidths of 50 and 100 Hz. For a 100-Hz-wide masker presented at different center frequencies, there is no advantage for LNN maskers below 1 kHz. Towards higher frequencies, the difference between the two noises increases and reaches about 15 dB at 10 kHz. (c) At 1 kHz with a 100-Hz bandwidth, decreasing the signal duration from 500 to 20 ms increases the threshold difference to 7.6 dB. (d) Thresholds in a dichotic condition, in which the masker is in phase and the signal is out of phase, lie within 2 dB for the two noise types, and are nearly constant for masker bandwidths between 5 and 100 Hz. It is argued that the primary detection cue in LNN is not an increase in energy, but rather an increase in envelope fluctuations due to the addition of the signal. This hypotheses is supported by simulations with an auditory-filterbank model. The simulations further suggest that, for a large LNN advantage, it is not sufficient that the LNN envelope is hat at the output of the on-frequency filter. In addition, it is crucial that off-frequency filters also yield a flat temporal envelope.},
  annotation = {ZSCC: NoCitationData[s0]},
  keywords = {\#nosource,Natural language processing}
}

@inproceedings{batistaSemisupervisedBootstrappingRelationship2015,
  title = {Semi-Supervised Bootstrapping of Relationship Extractors with Distributional Semantics},
  booktitle = {Conference {{Proceedings}} - {{EMNLP}} 2015: {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Batista, David S. and Martins, Bruno and Silva, M{\'a}rio J.},
  year = {2015},
  month = sep,
  pages = {499--504},
  publisher = {{Association for Computational Linguistics}},
  address = {{Lisbon, Portugal}},
  doi = {10.18653/v1/d15-1056},
  abstract = {Semi-supervised bootstrapping techniques for relationship extraction from text iteratively expand a set of initial seed relationships while limiting the semantic drift. We research bootstrapping for relationship extraction using word embeddings to find similar relationships. Experimental results show that relying on word embeddings achieves a better performance on the task of extracting four types of relationships from a collection of newswire documents when compared with a baseline using TF-IDF to find similar relationships.},
  annotation = {ZSCC: 0000025},
  file = {/home/harrisonpl/Documents/PDFs/Batista et al. - 2015 - Semi-supervised bootstrapping of relationship extractors with distributional.pdf},
  isbn = {978-1-941643-32-7},
  keywords = {HPL comprehensive exam,Relation extraction,Word embedding}
}

@article{bauerOntologizerMultifunctionalTool2008,
  title = {Ontologizer 2.0 - {{A}} Multifunctional Tool for {{GO}} Term Enrichment Analysis and Data Exploration},
  author = {Bauer, Sebastian and Grossmann, Steffen and Vingron, Martin and Robinson, Peter N.},
  year = {2008},
  volume = {24},
  pages = {1650--1651},
  issn = {13674803},
  doi = {10.1093/bioinformatics/btn250},
  abstract = {The Ontologizer is a Java application that can be used to perform statistical analysis for overrepresentation of Gene Ontology (GO) terms in sets of genes or proteins derived from an experiment. The Ontologizer implements the standard approach to statistical analysis based on the one-sided Fisher's exact test, the novel parent-child method, as well as topology-based algorithms. A number of multiple-testing correction procedures are provided. The Ontologizer allows users to visualize data as a graph including all significantly overrepresented GO terms and to explore the data by linking GO terms to all genes/proteins annotated to the term and by linking individual terms to child terms. \textcopyright{} The Author 2008. Published by Oxford University Press. All rights reserved.},
  annotation = {ZSCC: 0000459},
  file = {/home/harrisonpl/Documents/PDFs/Bauer et al. - 2008 - Ontologizer 2.pdf},
  journal = {Bioinformatics},
  keywords = {\#nosource},
  number = {14}
}

@article{baumgartnerCRAFTSharedTasks2019,
  title = {{{CRAFT Shared Tasks}} 2019 {{Overview}} \textendash - {{Integrated Structure}}, {{Semantics}}, and {{Coreference}}},
  author = {Baumgartner, William and Bada, Michael and Pyysalo, Sampo and Ciosici, Manuel R. and Hailu, Negacy and {Pielke-Lombardo}, Harrison and Regan, Michael and Hunter, Lawrence},
  year = {2019},
  pages = {174--184},
  doi = {10.18653/v1/d19-5725},
  abstract = {As part of the BioNLP Open Shared Tasks 2019, the CRAFT Shared Tasks 2019 provides a platform to gauge the state of the art for three fundamental language processing tasks-dependency parse construction, coref-erence resolution, and ontology concept identification over full-text biomedical articles. The structural annotation task requires the automatic generation of dependency parses for each sentence of an article given only the article text. The coreference resolution task fo-cuses on linking coreferring base noun phrase mentions into chains using the symmetrical and transitive identity relation. The ontology concept annotation task involves the identification of concept mentions within text using the classes of ten distinct ontologies in the biomedical domain, both unmodified and augmented with extension classes. This paper provides an overview of each task, including descriptions of the data provided to participants and the evaluation metrics used, and discusses participant results relative to baseline performances for each of the three tasks.},
  annotation = {ZSCC: 0000001},
  file = {/home/harrisonpl/Documents/PDFs/Baumgartner et al. - 2019 - CRAFT Shared Tasks 2019 Overview –- Integrated Structure, Semantics, and.pdf},
  journal = {Proceedings of The 5th Workshop on BioNLP Open Shared Tasks},
  keywords = {Challenge,Concept recognition,Coreference resolution,CRAFT,Entity recognition,hunter lab,Natural language processing,Relation extraction,Syntax parsing}
}

@article{beachProcesstracingMethodsFoundations2013,
  title = {Process-Tracing Methods: {{Foundations}} and Guidelines},
  shorttitle = {Process-Tracing {{Methods}}},
  author = {Beach, Derek and Pedersen, Rasmus Brun},
  year = {2013},
  month = jan,
  abstract = {Process-tracing in social science is a method for studying causal mechanisms linking causes with outcomes. This enables the researcher to make strong inferences about how a cause (or set of causes) contributes to producing an outcome. Derek Beach an. Copyright \textcopyright{} by the University of Michigan 2013. All rights reserved.},
  annotation = {ZSCC: 0000002},
  keywords = {\#nosource,HPL comprehensive exam,Social Science / Methodology},
  language = {en}
}

@article{bechtelHierarchyLevelsAnalysing2020,
  title = {Hierarchy and Levels: Analysing Networks to Study Mechanisms in Molecular Biology},
  author = {Bechtel, William},
  year = {2020},
  volume = {375},
  pages = {20190320},
  issn = {0962-8436},
  doi = {10.1098/rstb.2019.0320},
  abstract = {\textbackslash textlessp\textbackslash textgreaterNetwork representations are flat while mechanisms are organized into a hierarchy of levels, suggesting that the two are fundamentally opposed. I challenge this opposition by focusing on two aspects of the ways in which large-scale networks constructed from high-throughput data are analysed in systems biology: identifying clusters of nodes that operate as modules or mechanisms and using bio-ontologies such as gene ontology (GO) to annotate nodes with information about where entities appear in cells and the biological functions in which they participate. Of particular importance, GO organizes biological knowledge about cell components and functions hierarchically. I illustrate how this supports mechanistic interpretation of networks with two examples of network studies, one using epistatic interactions among genes to identify mechanisms and their parts and the other using deep learning to predict phenotypes. As illustrated in these examples, when network research draws upon hierarchical information such as provided by GO, the results not only can be interpreted mechanistically but provide new mechanistic knowledge.\textbackslash textless/p\textbackslash textgreater},
  annotation = {ZSCC: 0000000},
  file = {/home/harrisonpl/Documents/PDFs/Bechtel - 2020 - Hierarchy and levels.pdf},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  keywords = {deep learning,gene ontology,hierarchy,levels,mechanistic explanation,network analysis},
  number = {1796}
}

@article{bellamyPreeclampsiaRiskCardiovascular2007,
  title = {Pre-Eclampsia and Risk of Cardiovascular Disease and Cancer in Later Life: {{Systematic}} Review and Meta-Analysis},
  shorttitle = {Pre-Eclampsia and Risk of Cardiovascular Disease a},
  author = {Bellamy, Leanne and Casas, Juan Pablo and Hingorani, Aroon D. and Williams, David J.},
  year = {2007},
  month = nov,
  volume = {335},
  pages = {974--977},
  issn = {09598146},
  doi = {10.1136/bmj.39335.385301.BE},
  abstract = {Objective: To quantify the risk of future cardiovascular diseases, cancer, and mortality after pre-eclampsia. Design: Systematic review and meta-analysis. Data sources: Embase and Medline without language restrictions, including papers published between 1960 and December 2006, and hand searching of reference lists of relevant articles and reviews for additional reports. Review methods: Prospective and retrospective cohort studies were included, providing a dataset of 3 488 160 women with 198 252 affected by pre-eclampsia (exposure group) and 29 495 episodes of cardiovascular disease and cancer (study outcomes). Results: After pre-eclampsia women have an increased risk of vascular disease. The relative risks (95\% confidence intervals) for hypertension were 3.70 (2.70 to 5.05) after 14.1 years weighted mean follow-up, for ischaemic heart disease 2.16 (1.86 to 2.52) after 11.7 years, for stroke 1.81 (1.45 to 2.27) after 10.4 years, and for venous thromboembolism 1.79 (1.37 to 2.33) after 4.7 years. No increase in risk of any cancer was found (0.96, 0.73 to 1.27), including breast cancer (1.04,0.78 to 1.39) 17 years after pre-eclampsia. Overall mortality after pre-eclampsia was increased: 1.49 (1.05 to 2.14) after 14.5 years. Conclusions: A history of pre-eclampsia should be considered when evaluating risk of cardiovascular disease in women. This association might reflect a common cause for pre-eclampsia and cardiovascular disease, or an effect of pre-eclampsia on disease development, or both. No association was found between pre-eclampsia and future cancer.},
  annotation = {ZSCC: 0002076},
  file = {/home/harrisonpl/Documents/PDFs/Bellamy et al. - 2007 - Pre-eclampsia and risk of cardiovascular disease and cancer in later life.pdf},
  journal = {British Medical Journal},
  keywords = {cardiovascular disease},
  language = {en},
  number = {7627}
}

@article{beltagySciBERTPretrainedLanguage2019,
  title = {{{SciBERT}}: {{A Pretrained Language Model}} for {{Scientific Text}}},
  author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  year = {2019},
  pages = {3613--3618},
  doi = {10.18653/v1/d19-1371},
  abstract = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.},
  annotation = {ZSCC: 0000016},
  file = {/home/harrisonpl/Documents/PDFs/Beltagy et al. - 2019 - SciBERT.pdf}
}

@article{berantModelingBiologicalProcesses2014,
  title = {Modeling Biological Processes for Reading Comprehension},
  author = {Berant, Jonathan and Srikumar, Vivek and Chen, Pei Chun and Huang, Brad and Manning, Christopher D. and Vander Linden, Abby and Harding, Brittany},
  year = {2014},
  pages = {1499--1510},
  doi = {10.3115/v1/d14-1159},
  abstract = {Machine reading calls for programs that read and understand text, but most current work only attempts to extract facts from redundant web-scale corpora. In this paper, we focus on a new reading comprehension task that requires complex reasoning over a single document. The input is a paragraph describing a biological process, and the goal is to answer questions that require an understanding of the relations between entities and events in the process. To answer the questions, we first predict a rich structure representing the process in the paragraph. Then, we map the question to a formal query, which is executed against the predicted structure. We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations.},
  annotation = {ZSCC: 0000110},
  file = {/home/harrisonpl/Documents/PDFs/Berant et al. - 2014 - Modeling biological processes for reading comprehension.pdf},
  journal = {EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference}
}

@incollection{bertolasoIntroductionHumanPerspectives2020,
  title = {Introduction. {{Human Perspectives}} on the {{Quest}} for {{Knowledge}}},
  author = {Bertolaso, Marta and Sterpetti, Fabio},
  year = {2020},
  pages = {1--8},
  publisher = {{Springer, Cham}},
  annotation = {ZSCC: 0000000},
  keywords = {\#nosource}
}

@article{billeSurveyTreeEdit2005,
  title = {A Survey on Tree Edit Distance and Related Problems},
  author = {Bille, Philip},
  year = {2005},
  month = jun,
  volume = {337},
  pages = {217--239},
  issn = {03043975},
  doi = {10.1016/j.tcs.2004.12.030},
  abstract = {We survey the problem of comparing labeled trees based on simple local operations of deleting, inserting, and relabeling nodes. These operations lead to the tree edit distance, alignment distance, and inclusion problem. For each problem we review the results available and present, in detail, one or more of the central algorithms for solving the problem.},
  file = {/home/harrisonpl/Zotero/storage/UJWSCRQX/editsurvey_bille.pdf},
  journal = {Theoretical Computer Science},
  language = {en},
  number = {1-3}
}

@book{birdNaturalLanguageProcessing2009,
  title = {Natural Language Processing with {{Python}}},
  author = {Bird, Steven and Klein, Ewan and Loper, Edward},
  year = {2009},
  edition = {1st ed},
  publisher = {{O'Reilly}},
  address = {{Beijing ; Cambridge [Mass.]}},
  abstract = {This is an introduction to natural language processing, which supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation},
  annotation = {OCLC: ocn301885973},
  isbn = {978-0-596-51649-9},
  keywords = {Natural language processing (Computer science),Python (Computer program language),Python <Programmiersprache>,Sprachverarbeitung},
  lccn = {QA76.73.P98 B57 2009}
}

@article{bleakleySupervisedPredictionDrugtarget2009,
  title = {Supervised Prediction of Drug-Target Interactions Using Bipartite Local Models},
  author = {Bleakley, Kevin and Yamanishi, Yoshihiro},
  year = {2009},
  volume = {25},
  pages = {2397--2403},
  issn = {13674803},
  doi = {10.1093/bioinformatics/btp433},
  abstract = {Motivation: In silico prediction of drug-target interactions from heterogeneous biological data is critical in the search for drugs for known diseases. This problem is currently being attacked from many different points of view, a strong indication of its current importance. Precisely, being able to predict new drug-target interactions with both high precision and accuracy is the holy grail, a fundamental requirement for in silico methods to be useful in a biological setting. This, however, remains extremely challenging due to, amongst other things, the rarity of known drug-target interactions. Results: We propose a novel supervised inference method to predict unknown drug-target interactions, represented as a bipartite graph. We use this method, known as bipartite local models to first predict target proteins of a given drug, then to predict drugs targeting a given protein. This gives two independent predictions for each putative drug-target interaction, which we show can be combined to give a definitive prediction for each interaction. We demonstrate the excellent performance of the proposed method in the prediction of four classes of drug-target interaction networks involving enzymes, ion channels, G protein-coupled receptors (GPCRs) and nuclear receptors in human. This enables us to suggest a number of new potential drug-target interactions. \textcopyright{} 2009 The Author(s).},
  annotation = {ZSCC: 0000389},
  file = {/home/harrisonpl/Documents/PDFs/Bleakley, Yamanishi - 2009 - Supervised prediction of drug-target interactions using bipartite local models.pdf},
  journal = {Bioinformatics},
  number = {18}
}

@article{bleiLatentDirichletAllocation2002,
  title = {Latent Dirichlet Allocation},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael T.},
  year = {2002},
  volume = {3},
  pages = {993--1022},
  issn = {10495258},
  abstract = {We propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models including naive Bayes/unigram, mixture of unigrams [6], and TTof- rnarm's aspect model, also known as probabilistic latent semantic indexing (pLSI) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable. Inference and learning are carried out efficiently via variational algorithms. We present empirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification.},
  annotation = {ZSCC: 0000450},
  journal = {Advances in Neural Information Processing Systems},
  keywords = {\#nosource},
  number = {Jan}
}

@article{bolstadComparisonNormalizationMethods2003,
  title = {A Comparison of Normalization Methods for High Density Oligonucleotide Array Data Based on Variance and Bias},
  author = {Bolstad, B. M. and Irizarry, R. A. and {\textbackslash}AAstrand, M. and Speed, T. P.},
  year = {2003},
  volume = {19},
  pages = {185--193},
  issn = {13674803},
  doi = {10.1093/bioinformatics/19.2.185},
  abstract = {Motivation: When running experiments that involve multiple high density oligonucleotide arrays, it is important to remove sources of variation between arrays of non-biological origin. Normalization is a process for reducing this variation. It is common to see non-linear relations between arrays and the standard normalization provided by Affymetrix does not perform well in these situations. Results: We present three methods of performing normalization at the probe intensity level. These methods are called complete data methods because they make use of data from all arrays in an experiment to form the normalizing relation. These algorithms are compared to two methods that make use of a baseline array: a one number scaling based algorithm and a method that uses a non-linear normalizing relation by comparing the variability and bias of an expression measure. Two publicly available datasets are used to carry out the comparisons. The simplest and quickest complete data method is found to perform favorably.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/Bolstad et al. - 2003 - A comparison of normalization methods for high density oligonucleotide array.pdf},
  journal = {Bioinformatics},
  keywords = {Algorithms,Calibration,DNA,Genetic,Models,Molecular Probes,Nonlinear Dynamics,Oligonucleotide Array Sequence Analysis,Quality Control,Sequence Analysis,Stochastic Processes},
  number = {2}
}

@article{bowlerPlasmaSphingolipidsAssociated2015,
  title = {Plasma Sphingolipids Associated with Chronic Obstructive Pulmonary Disease Phenotypes},
  author = {Bowler, Russell P. and Jacobson, Sean and Cruickshank, Charmion and Hughes, Grant J. and Siska, Charlotte and Ory, Daniel S. and Petrache, Irina and Schaffer, Jean E. and Reisdorph, Nichole and Kechris, Katerina},
  year = {2015},
  volume = {191},
  pages = {275--284},
  issn = {15354970},
  doi = {10.1164/rccm.201410-1771OC},
  abstract = {Rationale: Chronic obstructive pulmonary disease (COPD) occurs in a minority of smokers and is characterized by intermittent exacerbations and clinical subphenotypes such as emphysema and chronic bronchitis. Although sphingolipids as a class are implicated in the pathogenesis of COPD, the particular sphingolipid species associated with COPD subphenotypes remain unknown. Objectives: To use mass spectrometry to determine which plasma sphingolipids are associated with subphenotypes of COPD. Methods: One hundred twenty-nine current and former smokers from the COPDGene cohort had 69 distinct sphingolipid species detected in plasma by targeted mass spectrometry. Of these, 23 were also measured in 131 plasma samples (117 independent subjects) using an untargeted platform in an independent laboratory. Regression analysis with adjustment for clinical covariates, correction for false discovery rate, and metaanalysis were used to test associations between COPD subphenotypes and sphingolipids. Peripheral blood mononuclear cells were used to test associations between sphingolipid gene expression and plasma sphingolipids. Measurements and Main Results: Of the measured plasma sphingolipids, five sphingomyelins were associated with emphysema; four trihexosylceramides and three dihexosylceramides were associated with COPD exacerbations. Three sphingolipids were strongly associated with sphingolipid gene expression, and 15 sphingolipid gene/metabolite pairs were differentially regulated between COPD cases and control subjects. Conclusions: There is evidence of systemic dysregulation of sphingolipid metabolism in patients with COPD. Subphenotyping suggests that sphingomyelins are strongly associated with emphysema and glycosphingolipids are associated with COPD exacerbations.},
  annotation = {ZSCC: 0000063},
  file = {/home/harrisonpl/Documents/PDFs/Bowler et al. - 2015 - Plasma sphingolipids associated with chronic obstructive pulmonary disease.pdf},
  journal = {American Journal of Respiratory and Critical Care Medicine},
  keywords = {Ceramides,Emphysema,Genomics,Metabolomics,Sphingomyelins},
  number = {3}
}

@article{braunAutomaticRelationExtraction2018,
  title = {Automatic Relation Extraction for Building Smart City Ecosystems Using Dependency Parsing},
  author = {Braun, Daniel and Faber, Anne and {Hernandez-Mendez}, Adrian and Matthes, Florian},
  year = {2018},
  volume = {2244},
  pages = {28--39},
  issn = {16130073},
  abstract = {Understanding and analysing rapidly changing and growing business ecosystems, like smart city and mobility ecosystems, becomes increasingly difficult. However, the understanding of these ecosystems is the key to being successful for all involved parties, like companies and public institutions. Modern Natural Language Processing technologies can help to automatically identify and extract relevant information from sources like online news and blog articles and hence support the analysis of complex ecosystems. In this paper, we present an approach to automatically extract directed relations between entities within business ecosystems from online news and blog articles by using dependency parsing.},
  annotation = {ZSCC: 0000001},
  journal = {CEUR Workshop Proceedings},
  keywords = {\#nosource,Dependency parsing,Ecosystem,Natural language processing,Relation extraction,Smart city}
}

@article{brigandtSystemsBiologyIntegration2013,
  title = {Systems Biology and the Integration of Mechanistic Explanation and Mathematical Explanation},
  author = {Brigandt, Ingo},
  year = {2013},
  month = dec,
  volume = {44},
  pages = {477--492},
  issn = {13698486},
  doi = {10.1016/j.shpsc.2013.06.002},
  abstract = {The paper discusses how systems biology is working toward complex accounts that integrate explanation in terms of mechanisms and explanation by mathematical models\textemdash which some philosophers have viewed as rival models of explanation. Systems biology is an integrative approach, and it strongly relies on mathematical modeling. Philosophical accounts of mechanisms capture integrative in the sense of multilevel and multifield explanations, yet accounts of mechanistic explanation (as the analysis of a whole in terms of its structural parts and their qualitative interactions) have failed to address how a mathematical model could contribute to such explanations. I discuss how mathematical equations can be explanatorily relevant. Several cases from systems biology are discussed to illustrate the interplay between mechanistic research and mathematical modeling, and I point to questions about qualitative phenomena (rather than the explanation of quantitative details), where quantitative models are still indispensable to the explanation. Systems biology shows that a broader philosophical conception of mechanisms is needed, which takes into account functional-dynamical aspects, interaction in complex networks with feedback loops, system-wide functional properties such as distributed functionality and robustness, and a mechanism's ability to respond to perturbations (beyond its actual operation). I offer general conclusions for philosophical accounts of explanation.},
  file = {/home/harrisonpl/Documents/PDFs/Brigandt - 2013 - Systems biology and the integration of mechanistic explanation and mathematical.pdf;/home/harrisonpl/Zotero/storage/PY5INXKP/1-s2.0-S1369848613000903-main.pdf},
  journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
  language = {en},
  number = {4}
}

@article{bristowMechanismActionBetablocking1997,
  title = {Mechanism of Action of Beta-Blocking Agents in Heart Failure},
  author = {Bristow, Michael R.},
  year = {1997},
  month = dec,
  volume = {80},
  pages = {26L--40L},
  issn = {00029149},
  doi = {10.1016/S0002-9149(97)00846-1},
  abstract = {Antiadrenergic treatment is currently an emerging and very promising approach to the treatment of chronic heart failure. Although the adrenergic nervous system can be pharmacologically inhibited at multiple levels, it is the use of receptor-blocking agents that has generated the most interest and provided the most data for the 'proof of concept' of Ibis approach. In part because antiadrenergic treatment of chronic heart failure has developed in an atmosphere in which it was initially considered to be contraindicated (i.e., before Phase III clinical trials could be initiated), a large body of hypothesis-driven basic and clinical investigation was required to define the overall rationale and demonstrate feasibility. This article will review these data and propose a single primary mechanism of action to explain most the clinical benefits of these agents.},
  annotation = {ZSCC: 0000557},
  file = {/home/harrisonpl/Documents/PDFs/Bristow - 1997 - Mechanism of action of beta-blocking agents in heart failure.pdf},
  journal = {American Journal of Cardiology},
  keywords = {borg,HPL comprehensive exam},
  language = {en},
  number = {11 A}
}

@article{broderickCommonOriginImmunity2015,
  title = {A Common Origin for Immunity and Digestion},
  author = {Broderick, Nichole A.},
  year = {2015},
  volume = {6},
  issn = {16643224},
  doi = {10.3389/fimmu.2015.00072},
  abstract = {Historically, the digestive and immune systems were viewed and studied as separate entities. However, there are remarkable similarities and shared functions in both nutrient acquisition and host defense. Here, I propose a common origin for both systems. This association provides a new prism for viewing the emergence and evolution of host defense mechanisms.},
  annotation = {ZSCC: 0000026},
  file = {/home/harrisonpl/Documents/PDFs/Broderick - 2015 - A common origin for immunity and digestion.pdf},
  journal = {Frontiers in Immunology},
  keywords = {Digestion physiology,Evolution,Gut microbiome,Host-microbe interactions,Innate immunity,Metabolism},
  number = {FEB}
}

@article{bruleWhittemoreEmbeddedDomain2018,
  title = {Whittemore: {{An}} Embedded Domain Specific Language for Causal Programming},
  author = {Brul{\'e}, Joshua},
  year = {2018},
  abstract = {This paper introduces Whittemore, a language for causal programming. Causal programming is based on the theory of structural causal models and consists of two primary operations: identification, which finds formulas that compute causal queries, and estimation, which applies formulas to transform probability distributions to other probability distribution. Causal programming provides abstractions to declare models, queries, and distributions with syntax similar to standard mathematical notation, and conducts rigorous causal inference, without requiring detailed knowledge of the underlying algorithms. Examples of causal inference with real data are provided, along with discussion of the implementation and possibilities for future extension.},
  annotation = {ZSCC: 0000001},
  file = {/home/harrisonpl/Documents/PDFs/Brulé - 2018 - Whittemore.pdf}
}

@article{brunoMicroarrayDataMining2010,
  title = {Microarray {{Data Mining}}},
  author = {Bruno, Giulia and Fiori, Alessandro},
  year = {2010},
  volume = {102},
  pages = {23--47},
  doi = {10.4018/978-1-60960-067-9.ch002},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/Bruno, Fiori - 2010 - Microarray Data Mining.pdf},
  number = {43}
}

@article{budzianowskiHelloItGPT22019,
  title = {Hello, {{It}}'s {{GPT}}-2 - {{How Can I Help You}}? {{Towards}} the {{Use}} of {{Pretrained Language Models}} for {{Task}}-{{Oriented Dialogue Systems}}},
  author = {Budzianowski, Pawe{\textbackslash}l and Vuli{\'c}, Ivan},
  year = {2019},
  pages = {15--22},
  doi = {10.18653/v1/d19-5602},
  abstract = {Data scarcity is a long-standing and crucial challenge that hinders quick development of task-oriented dialogue systems across multiple domains: task-oriented dialogue models are expected to learn grammar, syntax, dialogue reasoning, decision making, and language generation from absurdly small amounts of task-specific data. In this paper, we demonstrate that recent progress in language modeling pre-training and transfer learning shows promise to overcome this problem. We propose a task-oriented dialogue model that operates solely on text input: it effectively bypasses explicit policy and language generation modules. Building on top of the TransferTransfo framework (Wolf et al., 2019) and generative model pre-training (Radford et al., 2019), we validate the approach on complex multi-domain task-oriented dialogues from the MultiWOZ dataset. Our automatic and human evaluations show that the proposed model is on par with a strong task-specific neural baseline. In the long run, our approach holds promise to mitigate the data scarcity problem, and to support the construction of more engaging and more eloquent task-oriented conversational agents.},
  annotation = {ZSCC: 0000010},
  file = {/home/harrisonpl/Documents/PDFs/Budzianowski, Vulić - 2019 - Hello, It's GPT-2 - How Can I Help You.pdf}
}

@article{budzianowskiUsePretrainedLanguage2019,
  title = {Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems},
  author = {Budzianowski, Pawe{\l} and Vuli, Ivan},
  year = {2019},
  pages = {15--22},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/Budzianowski, Vuli - 2019 - Towards the use of pretrained language models for task-oriented dialogue systems.pdf},
  number = {Wngt}
}

@article{buiLearningRegularExpressions2014,
  title = {Learning Regular Expressions for Clinical Text Classification},
  author = {Bui, Duy Duc An and {Zeng-Treitler}, Qing},
  year = {2014},
  volume = {21},
  pages = {850--857},
  issn = {1527974X},
  doi = {10.1136/amiajnl-2013-002411},
  abstract = {Objectives: Natural language processing (NLP) applications typically use regular expressions that have been developed manually by human experts. Our goal is to automate both the creation and utilization of regular expressions in text classification. Methods: We designed a novel regular expression discovery (RED) algorithm and implemented two text classifiers based on RED. The RED+ALIGN classifier combines RED with an alignment algorithm, and RED +SVM combines RED with a support vector machine (SVM) classifier. Two clinical datasets were used for testing and evaluation: the SMOKE dataset, containing 1091 text snippets describing smoking status; and the PAIN dataset, containing 702 snippets describing pain status. We performed 10-fold cross-validation to calculate accuracy, precision, recall, and F-measure metrics. In the evaluation, an SVM classifier was trained as the control. Results: The two RED classifiers achieved 80.9-83.0\% in overall accuracy on the two datasets, which is 1.3-3\% higher than SVM's accuracy (p\textbackslash textless0.001). Similarly, small but consistent improvements have been observed in precision, recall, and F-measure when RED classifiers are compared with SVM alone. More significantly, RED+ALIGN correctly classified many instances that were misclassified by the SVM classifier (8.1-10.3\% of the total instances and 43.8-53.0\% of SVM's misclassifications). Conclusions: Machine-generated regular expressions can be effectively used in clinical text classification. The regular expression-based classifier can be combined with other classifiers, like SVM, to improve classification performance.},
  annotation = {ZSCC: 0000051},
  file = {/home/harrisonpl/Documents/PDFs/Bui, Zeng-Treitler - 2014 - Learning regular expressions for clinical text classification.pdf},
  journal = {Journal of the American Medical Informatics Association},
  keywords = {Algorithms,Artificial Intelligence,Automatic Data Processing,Computerized/*classification,Humans,Machine Learning,Medical Records Systems,Natural Language Processing,Natural Language Processing*,Pain/classification,Regular Expressions,Smoking,Support Vector Machine,Support Vector Machines,Text Classification},
  number = {5}
}

@article{bunneyOrexinActivationCounteracts2017,
  title = {Orexin Activation Counteracts Decreases in Nonexercise Activity Thermogenesis ({{NEAT}}) Caused by High-Fat Diet},
  author = {Bunney, P. E. and Zink, A. N. and Holm, A. A. and Billington, C. J. and Kotz, C. M.},
  year = {2017},
  volume = {176},
  pages = {139--148},
  issn = {1873507X},
  doi = {10.1016/j.physbeh.2017.03.040},
  abstract = {Overweight and obesity result from an imbalance between caloric intake and energy expenditure, including expenditure from spontaneous physical activity (SPA). Changes in SPA and resulting changes in non-exercise activity thermogenesis (NEAT) likely interact with diet to influence risk for obesity. However, previous research on the relationship between diet, physical activity, and energy expenditure has been mixed. The neuropeptide orexin is a driver of SPA, and orexin neuron activity can be manipulated using DREADDs (Designer Receptors Exclusively Activated by Designer Drugs). We hypothesized that HFD decreases SPA and NEAT, and that DREADD-mediated activation of orexin neuron signaling would abolish this decrease and produce an increase in NEAT instead. To test these ideas, we characterized behaviors to determine the extent to which access to a high-fat diet (HFD) influences the proportion and probability of engaging in food intake and activity. We then measured NEAT following access to HFD and following a DREADD intervention targeting orexin neurons. Two cohorts of orexin-cre male mice were injected with an excitatory DREADD virus into the caudal hypothalamus, where orexin neurons are concentrated. Mice were then housed in continuous metabolic phenotyping cages (Sable Promethion). Food intake, indirect calorimetry, and SPA were automatically measured every second. For cohort 1 (n = 8), animals were given access to chow, then switched to HFD. For cohort 2 (n = 4/group), half of the animals were given access to HFD, the other access to chow. Then, among animals on HFD, orexin neurons were activated following injections of clozapine n-oxide (CNO). Mice on HFD spent significantly less time eating (p \textbackslash textless 0.01) and more time inactive compared to mice on chow (p \textbackslash textless 0.01). Following a meal, mice on HFD were significantly more likely to engage in periods of inactivity compared to those on chow (p \textbackslash textless 0.05). NEAT was decreased in animals on HFD, and was increased to the NEAT level of control animals following activation of orexin neurons with DREADDs. Food intake (kilocalories) was not significantly different between mice on chow and HFD, yet mice on chow expended more energy per unit of SPA, relative to that in mice consuming HFD. These results suggest that HFD consumption reduces SPA and NEAT, and increases inactivity following a meal. Together, the data suggest a change in the efficiency of energy expenditure based upon diet, such that SPA during HFD burns fewer calories compared to SPA on a standard chow diet.},
  annotation = {ZSCC: 0000018},
  file = {/home/harrisonpl/Documents/PDFs/Bunney et al. - 2017 - Orexin activation counteracts decreases in nonexercise activity thermogenesis.pdf},
  journal = {Physiology and Behavior},
  keywords = {Designer receptors exclusively activated by design,High-fat diet,Non-exercise activity thermogenesis (NEAT),Orexin,Spontaneous physical activity (SPA)},
  number = {5}
}

@article{callahanKnowledgeBasedBiomedicalData2020,
  ids = {callahanKnowledgebasedBiomedicalData2019},
  title = {Knowledge-{{Based Biomedical Data Science}}},
  author = {Callahan, Tiffany J. and Tripodi, Ignacio J. and {Pielke-Lombardo}, Harrison and Hunter, Lawrence E.},
  year = {2020},
  volume = {3},
  pages = {23--41},
  doi = {10.1146/annurev-biodatasci-010820-091627},
  abstract = {Knowledge-based biomedical data science involves the design and implementation of computer systems that act as if they knew about biomedicine. Such systems depend on formally represented knowledge in computer systems, often in the form of knowledge graphs. Here we survey recent progress in systems that use formally represented knowledge to address data science problems in both clinical and biological domains, as well as progress on approaches for creating knowledge graphs. Major themes include the relationships between knowledge graphs and machine learning, the use of natural language processing to construct knowledge graphs, and the expansion of novel knowledge-based approaches to clinical and biological domains.},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-biodatasci-010820-091627},
  file = {/home/harrisonpl/Documents/PDFs/Callahan et al. - 2020 - Knowledge-Based Biomedical Data Science.pdf},
  journal = {Annual Review of Biomedical Data Science},
  keywords = {explanation,inference,Knowledge Discovery,Knowledge Graph,Knowledge Graph Embeddings,knowledge representation,machine learning,Natural Language Processing,ontology,Ontology,reasoning,Semantic Web,text mining},
  number = {1}
}

@article{cangeaStructureBasedNetworksDrug2018,
  title = {Structure-{{Based Networks}} for {{Drug Validation}}},
  author = {Cangea, C{\u a}t{\u a}lina and Grauslys, Arturas and Li{\`o}, Pietro and Falciani, Francesco},
  year = {2018},
  pages = {1--5},
  abstract = {Classifying chemicals according to putative modes of action (MOAs) is of paramount importance in the context of risk assessment. However, current methods are only able to handle a very small proportion of the existing chemicals. We address this issue by proposing an integrative deep learning architecture that learns a joint representation from molecular structures of drugs and their effects on human cells. Our choice of architecture is motivated by the significant influence of a drug's chemical structure on its MOA. We improve on the strong ability of a unimodal architecture (F1 score of 0.803) to classify drugs by their toxic MOAs (Verhaar scheme) through adding another learning stream that processes transcriptional responses of human cells affected by drugs. Our integrative model achieves an even higher classification performance on the LINCS L1000 dataset - the error is reduced by 4.6\%. We believe that our method can be used to extend the current Verhaar scheme and constitute a basis for fast drug validation and risk assessment.},
  annotation = {ZSCC: 0000000},
  file = {/home/harrisonpl/Documents/PDFs/Cangea et al. - 2018 - Structure-Based Networks for Drug Validation.pdf}
}

@article{canImprovingSemanticRelation2019,
  title = {Improving {{Semantic Relation Extraction System}} with {{Compositional Dependency Unit}} on {{Enriched Shortest Dependency Path}}},
  author = {Can, Duy Cat and Le, Hoang Quynh and Ha, Quang Thuy},
  year = {2019},
  volume = {11431 LNAI},
  pages = {140--152},
  issn = {16113349},
  doi = {10.1007/978-3-030-14799-0_12},
  abstract = {Experimental performance on the task of relation extraction/classification has generally improved using deep neural network architectures. In which, data representation has been proven to be one of the most influential factors to the model's performance but still has many limitations. In this work, we take advantage of compressed information in the shortest dependency path (SDP) between two corresponding entities to classify the relation between them. We propose (i) a compositional embedding that combines several dominant linguistic as well as architectural features and (ii) dependency tree normalization techniques for generating rich representations for both words and dependency relations in the SDP. We also present a Convolutional Neural Network (CNN) model to process the proposed SDP enriched representation. Experimental results for both general and biomedical data demonstrate the effectiveness of compositional embedding, dependency tree normalization technique as well as the suitability of the CNN model.},
  annotation = {ZSCC: 0000000},
  file = {/home/harrisonpl/Documents/PDFs/Can et al. - 2019 - Improving Semantic Relation Extraction System with Compositional Dependency.pdf},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords = {Convolutional neural network,Dependency unit,Relation extraction,Shortest dependency path}
}

@article{carbonellComputationalModelAnalogical1981,
  title = {Computational {{Model}} of {{Analogical Problem Solving}}.},
  author = {Carbonell, Jaime G.},
  year = {1981},
  month = jan,
  volume = {1},
  pages = {147--152},
  abstract = {This paper outlines a theory of analogical reasoning based on a process-model of problem solving by analogy and the hypothesis that problem solving and learning are inalienable, concurrent processes in the human cognitive system. The analogical problem solver exploits prior experience in solving similar problems, and, in the process, augments a hierarchically -structured epsiodic long term memory. An analogical transformation process is developed based on a modified version of Means-Ends Analysis in order to map past solutions from similar problems into solutions satisfying the requirements of the new problem.1},
  annotation = {ZSCC: 0000086},
  file = {/home/harrisonpl/Documents/PDFs/Carbonell - 1981 - Computational Model of Analogical Problem Solving.pdf},
  keywords = {comp-exam,HPL comprehensive exam},
  number = {January 1981}
}

@article{cardozoChemistrybasedMolecularSignature2017,
  title = {Chemistry-Based Molecular Signature Underlying the Atypia of Clozapine},
  author = {Cardozo, T. and Shmelkov, E. and Felsovalyi, K. and Swetnam, J. and Butler, T. and Malaspina, D. and Shmelkov, S. V.},
  year = {2017},
  volume = {7},
  pages = {e1036},
  issn = {21583188},
  doi = {10.1038/tp.2017.6},
  abstract = {The central nervous system is functionally organized as a dynamic network of interacting neural circuits that underlies observable behaviors. At higher resolution, these behaviors, or phenotypes, are defined by the activity of a specific set of biomolecules within those circuits. Identification of molecules that govern psychiatric phenotypes is a major challenge. The only organic molecular entities objectively associated with psychiatric phenotypes in humans are drugs that induce psychiatric phenotypes and drugs used for treatment of specific psychiatric conditions. Here, we identified candidate biomolecules contributing to the organic basis for psychosis by deriving an in vivo biomolecule-tissue signature for the atypical pharmacologic action of the antipsychotic drug clozapine. Our novel in silico approach identifies the ensemble of potential drug targets based on the drug's chemical structure and the region-specific gene expression profile of each target in the central nervous system. We subtracted the signature of the action of clozapine from that of a typical antipsychotic, chlorpromazine. Our results implicate dopamine D4 receptors in the pineal gland and muscarinic acetylcholine M1 (CHRM1) and M3 (CHRM3) receptors in the prefrontal cortex (PFC) as significant and unique to clozapine, whereas serotonin receptors 5-HT2A in the PFC and 5-HT2C in the caudate nucleus were common significant sites of action for both drugs. Our results suggest that D4 and CHRM1 receptor activity in specific tissues may represent underappreciated drug targets to advance the pharmacologic treatment of schizophrenia. These findings may enhance our understanding of the organic basis of psychiatric disorders and help developing effective therapies.},
  annotation = {ZSCC: 0000004},
  file = {/home/harrisonpl/Documents/PDFs/Cardozo et al. - 2017 - Chemistry-based molecular signature underlying the atypia of clozapine.pdf},
  journal = {Translational Psychiatry},
  keywords = {Lawrence Hunter,Recommendation},
  number = {2}
}

@article{cardozoDataSourcesVivo2016,
  title = {Data Sources for in Vivo Molecular Profiling of Human Phenotypes},
  author = {Cardozo, Timothy and Gupta, Priyanka and Ni, Eric and Young, Lauren M. and Tivon, Doreen and Felsovalyi, Klara},
  year = {2016},
  volume = {8},
  abstract = {Molecular profiling of human diseases has been approached at the genetic (DNA), expression (RNA), and proteomic (protein) levels. An important goal of these efforts is to map observed molecular patterns to specific, mechanistic organic entities, such as loci in the genome, individual RNA molecules or defined proteins or protein assemblies. Importantly, such maps have been historically approached in the more intuitive context of a theoretical individual cell, but diseases are better described in reality using an in vivo framework, namely a library of several tissue-specific maps. In this article, we review the existing data atlases that can be used for this purpose and identify critical gaps that could move the field forward from cellular to in vivo dimensions. WIREs Syst Biol Med 2016, 8:472\textendash 484. doi: 10.1002/wsbm.1354. For further resources related to this article, please visit the WIREs website.},
  annotation = {ZSCC: 0000001},
  file = {/home/harrisonpl/Documents/PDFs/Cardozo et al. - 2016 - Data sources for in vivo molecular profiling of human phenotypes.pdf},
  keywords = {Lawrence Hunter,Recommendation}
}

@article{carlinFastFlexibleFramework2019,
  title = {A {{Fast}} and {{Flexible Framework}} for {{Network}}-{{Assisted Genomic Association}}},
  author = {Carlin, Daniel E. and Fong, Samson H. and Qin, Yue and Jia, Tongqiu and Huang, Justin K. and Bao, Bokan and Zhang, Chao and Ideker, Trey},
  year = {2019},
  month = jun,
  volume = {16},
  pages = {155--161},
  issn = {25890042},
  doi = {10.1016/j.isci.2019.05.025},
  abstract = {We present an accessible, fast, and customizable network propagation system for pathway boosting and interpretation of genome-wide association studies. This system\textemdash NAGA (Network Assisted Genomic Association)\textemdash taps the NDEx biological network resource to gain access to thousands of protein networks and select those most relevant and performative for a specific association study. The method works efficiently, completing genome-wide analysis in under 5 minutes on a modern laptop computer. We show that NAGA recovers many known disease genes from analysis of schizophrenia genetic data, and it substantially boosts associations with previously unappreciated genes such as amyloid beta precursor. On this and seven other gene-disease association tasks, NAGA outperforms conventional approaches in recovery of known disease genes and replicability of results. Protein interactions associated with disease are visualized and annotated in Cytoscape, which, in addition to standard programmatic interfaces, allows for downstream analysis. Biological Sciences; Genomics; Bioinformatics},
  annotation = {ZSCC: 0000002},
  file = {/home/harrisonpl/Documents/PDFs/Carlin et al. - 2019 - A Fast and Flexible Framework for Network-Assisted Genomic Association.pdf},
  journal = {iScience},
  keywords = {Bioinformatics,Biological Sciences,Genomics}
}

@article{cerconeWhatKnowledgeRepresentation1987,
  title = {What Is Knowledge Representation?},
  booktitle = {The Knowledge Frontier},
  author = {Cercone, Nick and McCalla, Gordon},
  year = {1987},
  pages = {1--43},
  doi = {10.1007/978-1-4612-4792-0_1},
  abstract = {Although knowledge representation is one of the central and, in some ways, most familiar concepts in AI, the most fundamental question about it \textendash{} What is it? \textendash{} has rarely been answered directly. Numerous papers have lobbied for one or another variety of representation, other papers have argued for various properties a representation should have, and still others have focused on properties that are important to the notion of representation in general. In this article, we go back to basics to address the question directly. We believe that the answer can best be understood in terms of five important and distinctly different roles that a representation plays, each of which places different and, at times, conflicting demands on the properties a representation should have. We argue that keeping in mind all five of these roles provides a usefully broad perspective that sheds light on some longstanding disputes and can invigorate both research and practice in the field.},
  file = {/home/harrisonpl/Documents/PDFs/Cercone, McCalla - 1987 - What is knowledge representation.pdf}
}

@article{ceruttiHowWeDesigned2019,
  title = {How We Designed Winning Algorithms for Abstract Argumentation and Which Insight We Attained},
  author = {Cerutti, Federico and Giacomin, Massimiliano and Vallati, M.},
  year = {2019},
  volume = {276},
  pages = {1--40},
  issn = {00043702},
  doi = {10.1016/j.artint.2019.08.001},
  abstract = {In this paper we illustrate the design choices that led to the development of ArgSemSAT, the winner of the preferred semantics track at the 2017 International Competition on Computational Models of Arguments (ICCMA 2017), a biennial contest on problems associated to the Dung's model of abstract argumentation frameworks, widely recognised as a fundamental reference in computational argumentation. The algorithms of ArgSemSAT are based on multiple calls to a SAT solver to compute complete labellings, and on encoding constraints to drive the search towards the solution of decision and enumeration problems. In this paper we focus on preferred semantics (and incidentally stable as well), one of the most popular and complex semantics for identifying acceptable arguments. We discuss our design methodology that includes a systematic exploration and empirical evaluation of labelling encodings, algorithmic variations and SAT solver choices. In designing the successful ArgSemSAT, we discover that: (1) there is a labelling encoding that appears to be universally better than other, logically equivalent ones; (2) composition of different techniques such as AllSAT and enumerating stable extensions when searching for preferred semantics brings advantages; (3) injecting domain specific knowledge in the algorithm design can lead to significant improvements.},
  annotation = {ZSCC: 0000001},
  file = {/home/harrisonpl/Documents/PDFs/Cerutti et al. - 2019 - How we designed winning algorithms for abstract argumentation and which insight.pdf},
  journal = {Artificial Intelligence},
  keywords = {Abstract argumentation,SAT-based algorithms,Stable and preferred semantics}
}

@article{ceustersFoundationsRealistOntology2010,
  title = {Foundations for a Realist Ontology of Mental Disease},
  author = {Ceusters, Werner and Smith, Barry},
  year = {2010},
  volume = {1},
  abstract = {While classifications of mental disorders have existed for over one hundred years, it still remains unspecified what terms such as 'mental disorder', 'disease' and 'illness' might actually denote. While ontologies have been called in aid to address this shortfall since the GALEN project of the early 1990s, most attempts thus far have sought to provide a formal description of the structure of some pre-existing terminology or classification, rather than of the corresponding structures and processes on the side of the patient.We here present a view of mental disease that is based on ontological realism and which follows the principles embodied in Basic Formal Ontology (BFO) and in the application of BFO in the Ontology of General Medical Science (OGMS). We analyzed statements about what counts as a mental disease provided (1) in the research agenda for the DSM-V, and (2) in Pies' model. The results were used to assess whether the representational units of BFO and OGMS were adequate as foundations for a formal representation of the entities in reality that these statements attempt to describe. We then analyzed the representational units specific to mental disease and provided corresponding definitions.Our key contributions lie in the identification of confusions and conflations in the existing terminology of mental disease and in providing what we believe is a framework for the sort of clear and unambiguous reference to entities on the side of the patient that is needed in order to avoid these confusions in the future. \textcopyright{} 2010 Ceusters and Smith; licensee BioMed Central Ltd.},
  annotation = {ZSCC: 0000081},
  file = {/home/harrisonpl/Documents/PDFs/Ceusters, Smith - 2010 - Foundations for a realist ontology of mental disease.pdf}
}

@article{chahardoliRelationExtractionSynthetic2019,
  title = {Relation Extraction with Synthetic Explanations and Neural Network By},
  author = {Chahardoli, Rozan},
  year = {2019},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/Chahardoli - 2019 - Relation extraction with synthetic explanations and neural network by.pdf}
}

@phdthesis{chahardoliRelationExtractionSynthetic2019a,
  ids = {Chahardoli2019},
  title = {Relation {{Extraction With Synthetic Explanations And Neural Network}}},
  author = {Chahardoli, Rozan},
  year = {2019},
  doi = {10.7939/R3-84JN-CR09},
  abstract = {Relation Extraction, which is defined as the detection of existing relations between a pair of entities in a sentence, has received a large interest lately, including more recent work on using neural methods. Since neural systems need a large number of annotated sentences to build effective models, Distant Supervision has been a preferred choice for collecting training labeled data. However, recent published work has shown that, training classifiers via a small number of annotated data and some explanation of why a sentence expresses a relation performs as accurate as distant supervision methods working with a large number of annotated sentences. In this thesis, we show that we can generate synthetic explanations, based on a small number of trigger words, for each relation in a way that the resulting explanations achieve comparable accuracy to human produced explanations by training a neural classifier. Our system is evaluated on five relation extraction tasks with different entity types (person-person, person-location, etc.) and the results show that synthetic explanations can work as precise as human generated explanations for the task of relation extraction. The proposed system also has the ability to classify noisy data coming from distant supervision methods with a reasonable accuracy.},
  file = {/home/harrisonpl/Documents/PDFs/Chahardoli - 2019 - Relation Extraction With Synthetic Explanations And Neural Network.pdf},
  keywords = {Distant Supervision,Named Entity Recognition,Neural Network,Relation Extraction,Sentence Embedding},
  type = {{{PhD Thesis}}}
}

@article{chauhanREflexFlexibleFramework2019,
  title = {{{REflex}}: {{Flexible Framework}} for {{Relation Extraction}} in {{Multiple Domains}}},
  author = {Chauhan, Geeticka and McDermott, Matthew B.A. and Szolovits, Peter},
  year = {2019},
  pages = {30--47},
  doi = {10.18653/v1/w19-5004},
  abstract = {Systematic comparison of methods for relation extraction (RE) is difficult because many experiments in the field are not described precisely enough to be completely reproducible and many papers fail to report ablation studies that would highlight the relative contributions of their various combined techniques. In this work, we build a unifying framework for RE, applying this on three highly used datasets (from the general, biomedical and clinical domains) with the ability to be extendable to new datasets. By performing a systematic exploration of modeling, pre-processing and training methodologies, we find that choices of pre-processing are a large contributor performance and that omission of such information can further hinder fair comparison. Other insights from our exploration allow us to provide recommendations for future research in this area.},
  annotation = {ZSCC: 0000001},
  file = {/home/harrisonpl/Documents/PDFs/Chauhan et al. - 2019 - REflex.pdf}
}

@article{cheBoostingDeepLearning2017,
  ids = {Che2017},
  title = {Boosting Deep Learning Risk Prediction with Generative Adversarial Networks for Electronic Health Records},
  author = {Che, Zhengping and Cheng, Yu and Zhai, Shuangfei and Sun, Zhaonan and Liu, Yan},
  year = {2017},
  volume = {2017-Novem},
  pages = {787--792},
  issn = {15504786},
  doi = {10.1109/ICDM.2017.93},
  abstract = {The rapid growth of Electronic Health Records (EHRs), as well as the accompanied opportunities in Data-Driven Healthcare (DDH), has been attracting widespread interests and attentions. Recent progress in the design and applications of deep learning methods has shown promising results and is forcing massive changes in healthcare academia and industry, but most of these methods rely on massive labeled data. In this work, we propose a general deep learning framework which is able to boost risk prediction performance with limited EHR data. Our model takes a modified generative adversarial network namely ehrGAN, which can provide plausible labeled EHR data by mimicking real patient records, to augment the training dataset in a semi-supervised learning manner. We use this generative model together with a convolutional neural network (CNN) based prediction model to improve the onset prediction performance. Experiments on two real healthcare datasets demonstrate that our proposed framework produces realistic data samples and achieves significant improvements on classification tasks with the generated data over several stat-of-the-art baselines.},
  annotation = {\_eprint: 1709.01648},
  archivePrefix = {arXiv},
  arxivid = {1709.01648},
  eprint = {1709.01648},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Che et al. - 2017 - Boosting deep learning risk prediction with generative adversarial networks for.pdf},
  isbn = {9781538638347},
  journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
  keywords = {Deep learning,Electronic health record,Generative adversarial network,Health care,hunter lab},
  mendeley-tags = {hunter lab}
}

@article{chenBioSentVecCreatingSentence2019,
  title = {{{BioSentVec}}: {{Creating}} Sentence Embeddings for Biomedical Texts},
  author = {Chen, Qingyu and Peng, Yifan and Lu, Zhiyong},
  year = {2019},
  pages = {0--4},
  doi = {10.1109/ICHI.2019.8904728},
  abstract = {Sentence embeddings have become an essential part of today's natural language processing (NLP) systems, especially together advanced deep learning methods. Although pre-trained sentence encoders are available in the general domain, none exists for biomedical texts to date. In this work, we introduce BioSentVec: the first open set of sentence embeddings trained with over 30 million documents from both scholarly articles in PubMed and clinical notes in the MIMICIII Clinical Database. We evaluate BioSentVec embeddings in two sentence pair similarity tasks in different biomedical text genres. Our benchmarking results demonstrate that the BioSentVec embeddings can better capture sentence semantics compared to the other competitive alternatives and achieve state-of-the-art performance in both tasks. We expect BioSentVec to facilitate the research and development in biomedical text mining and to complement the existing resources in biomedical word embeddings. The embeddings are publicly available at https://github.com/ncbi-nlp/BioSentVec.},
  annotation = {ZSCC: 0000025},
  file = {/home/harrisonpl/Documents/PDFs/Chen et al. - 2019 - BioSentVec.pdf},
  journal = {2019 IEEE International Conference on Healthcare Informatics, ICHI 2019},
  keywords = {Biomedical Text Mining,Sentence Embeddings}
}

@article{chenEvaluatingVectorspaceModels2017,
  ids = {Chen2017},
  title = {Evaluating Vector-Space Models of Analogy},
  author = {Chen, Dawn and Peterson, Joshua C. and Griffiths, Thomas L.},
  year = {2017},
  month = jun,
  abstract = {Vector-space representations provide geometric tools for reasoning about the similarity of a set of objects and their relationships. Recent machine learning methods for deriving vector-space embeddings of words (e.g., word2vec) have achieved considerable success in natural language processing. These vector spaces have also been shown to exhibit a surprising capacity to capture verbal analogies, with similar results for natural images, giving new life to a classic model of analogies as parallelograms that was first proposed by cognitive scientists. We evaluate the parallelogram model of analogy as applied to modern word embeddings, providing a detailed analysis of the extent to which this approach captures human relational similarity judgments in a large benchmark dataset. We find that that some semantic relationships are better captured than others. We then provide evidence for deeper limitations of the parallelogram model based on the intrinsic geometric constraints of vector spaces, paralleling classic results for first-order similarity.},
  annotation = {\_eprint: 1705.04416},
  archivePrefix = {arXiv},
  arxivid = {1705.04416},
  eprint = {1705.04416},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Chen et al. - 2017 - Evaluating vector-space models of analogy.pdf},
  journal = {arXiv:1705.04416 [cs]},
  keywords = {Computer Science - Computation and Language},
  mendeley-tags = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{chengStructurebasedMaximalAffinity2007,
  ids = {Cheng2007},
  title = {Structure-Based Maximal Affinity Model Predicts Small-Molecule Druggability},
  author = {Cheng, Alan C. and Coleman, Ryan G. and Smyth, Kathleen T. and Cao, Qing and Soulard, Patricia and Caffrey, Daniel R. and Salzberg, Anna C. and Huang, Enoch S.},
  year = {2007},
  volume = {25},
  pages = {71--75},
  issn = {10870156},
  doi = {10.1038/nbt1273},
  abstract = {Lead generation is a major hurdle in small-molecule drug discovery, with an estimated 60\% of projects failing from lack of lead matter or difficulty in optimizing leads for drug-like properties. It would be valuable to identify these less-druggable targets before incurring substantial expenditure and effort. Here we show that a model-based approach using basic biophysical principles yields good prediction of druggability based solely on the crystal structure of the target binding site. We quantitatively estimate the maximal affinity achievable by a drug-like molecule, and we show that these calculated values correlate with drug discovery outcomes. We experimentally test two predictions using high-throughput screening of a diverse compound collection. The collective results highlight the utility of our approach as well as strategies for tackling difficult targets. \textcopyright{} 2007 Nature Publishing Group.},
  file = {/home/harrisonpl/Documents/PDFs/Cheng et al. - 2007 - Structure-based maximal affinity model predicts small-molecule druggability.pdf},
  journal = {Nature Biotechnology},
  keywords = {Algorithms,Binding Sites,Chemical,Computer Simulation,Drug Design,Models,Molecular,Pharmaceutical Preparations,Protein Binding,Protein Interaction Mapping,Proteins},
  number = {1},
  pmid = {17211405}
}

@article{chenMiRDBOnlineDatabase2020,
  title = {{{miRDB}}: An Online Database for Prediction of Functional {{microRNA}} Targets},
  author = {Chen, Yuhao and Wang, Xiaowei},
  year = {2020},
  volume = {48},
  pages = {D127--D131},
  issn = {13624962},
  doi = {10.1093/nar/gkz757},
  abstract = {MicroRNAs (miRNAs) are small noncoding RNAs that act as master regulators in many biological processes. miRNAs function mainly by downregulating the expression of their gene targets. Thus, accurate prediction of miRNA targets is critical for characterization of miRNA functions. To this end, we have developed an online database, miRDB, for miRNA target prediction and functional annotations. Recently, we have performed major updates for miRDB. Specifically, by employing an improved algorithm for miRNA target prediction, we now present updated transcriptome-wide target prediction data in miRDB, including 3.5 million predicted targets regulated by 7000 miRNAs in five species. Further, we have implemented the new prediction algorithm into a web server, allowing custom target prediction with user-provided sequences. Another new database feature is the prediction of cell-specific miRNA targets. miRDB now hosts the expression profiles of over 1000 cell lines and presents target prediction data that are tailored for specific cell models. At last, a new web query interface has been added to miRDB for prediction of miRNA functions by integrative analysis of target prediction and Gene Ontology data. All data in miRDB are freely accessible at http://mirdb.org.},
  annotation = {ZSCC: 0000005},
  file = {/home/harrisonpl/Documents/PDFs/Chen, Wang - 2020 - miRDB.pdf},
  journal = {Nucleic acids research},
  keywords = {David Port,Recommendation},
  number = {D1}
}

@inproceedings{chenQuerybasedMedicalInformation2006,
  ids = {Chen2006},
  title = {A Query-Based Medical Information Summarization System Using Ontology Knowledge},
  booktitle = {Proceedings - {{IEEE Symposium}} on {{Computer}}-{{Based Medical Systems}}},
  author = {Chen, Ping and Verma, Rakesh},
  year = {2006},
  volume = {2006},
  pages = {37--42},
  issn = {10637125},
  doi = {10.1109/CBMS.2006.25},
  abstract = {As huge amounts of knowledge are created rapidly, effective information access becomes an important issue. Especially for critical domains, such as medical and financial areas, efficient retrieval of concise and relevant information is highly desired. In this paper we propose a new user query based text summarization technique that makes use of Unified Medical Language System, an ontology knowledge source from National Library of Medicine. We compare our method with keyword-only approach, and our ontology-based method performs clearly better. Our method also shows potential to be used in other information retrieval areas. \textcopyright{} 2006 IEEE.},
  file = {/home/harrisonpl/Documents/PDFs/Chen, Verma - 2006 - A query-based medical information summarization system using ontology knowledge.pdf},
  isbn = {0-7695-2517-2},
  keywords = {Biomedical ontology,hunter lab,Information retrieval,Text summarization,UMLS},
  mendeley-tags = {hunter lab}
}

@article{chenReadsGenesPathways2016,
  ids = {Chen2016,Chen2016a},
  title = {From Reads to Genes to Pathways: {{Differential}} Expression Analysis of {{RNA}}-{{Seq}} Experiments Using {{Rsubread}} and the {{edgeR}} Quasi-Likelihood Pipeline [Version 1; Referees: 5 Approved]},
  shorttitle = {From Reads to Genes to Pathways},
  author = {Chen, Yunshun and Lun, Aaron T.L. and Smyth, Gordon K.},
  year = {2016},
  month = aug,
  volume = {5},
  pages = {1--48},
  issn = {1759796X},
  doi = {10.12688/F1000RESEARCH.8987.1},
  abstract = {In recent years, RNA sequencing (RNA-seq) has become a very widely used technology for profiling gene expression. One of the most common aims of RNA-seq profiling is to identify genes or molecular pathways that are differentially expressed (DE) between two or more biological conditions. This article demonstrates a computational workflow for the detection of DE genes and pathways from RNA-seq data by providing a complete analysis of an RNA-seq experiment profiling epithelial cell subsets in the mouse mammary gland. The workflow uses R software packages from the open-source Bioconductor project and covers all steps of the analysis pipeline, including alignment of read sequences, data exploration, differential expression analysis, visualization and pathway analysis. Read alignment and count quantification is conducted using the Rsubread package and the statistical analyses are performed using the edgeR package. The differential expression analysis uses the quasi-likelihood functionality of edgeR.},
  file = {/home/harrisonpl/Documents/PDFs/Chen et al. - 2016 - From reads to genes to pathways.pdf},
  journal = {F1000Research},
  keywords = {HPL comprehensive exam},
  mendeley-tags = {HPL comprehensive exam}
}

@article{chenRobustlyExtractingMedical2020,
  title = {Robustly {{Extracting Medical Knowledge}} from {{EHRs}}: {{A Case Study}} of {{Learning}} a {{Health Knowledge Graph}}},
  author = {Chen, Irene Y. and Agrawal, Monica and Horng, Steven and Sontag, David},
  year = {2020},
  volume = {25},
  pages = {19--30},
  issn = {23356936},
  doi = {10.1142/9789811215636_0003},
  abstract = {Increasingly large electronic health records (EHRs) provide an opportunity to algorithmically learn medical knowledge. In one prominent example, a causal health knowledge graph could learn relationships between diseases and symptoms and then serve as a diagnostic tool to be refined with additional clinical input. Prior research has demonstrated the ability to construct such a graph from over 270,000 emergency department patient visits. In this work, we describe methods to evaluate a health knowledge graph for robustness. Moving beyond precision and recall, we analyze for which diseases and for which patients the graph is most accurate. We identify sample size and unmeasured confounders as major sources of error in the health knowledge graph. We introduce a method to leverage non-linear functions in building the causal graph to better understand existing model assumptions. Finally, to assess model generalizability, we extend to a larger set of complete patient visits within a hospital system. We conclude with a discussion on how to robustly extract medical knowledge from EHRs.},
  annotation = {ZSCC: 0000002},
  file = {/home/harrisonpl/Documents/PDFs/Chen et al. - 2020 - Robustly Extracting Medical Knowledge from EHRs.pdf},
  journal = {Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing},
  keywords = {electronic health records,healthcare,knowledge graph,machine learning}
}

@article{chenSemanticWebIntegrated2009,
  title = {Semantic Web for Integrated Network Analysis in Biomedicine},
  author = {Chen, Huajun and Ding, Li and Wu, Zhaohui and Yu, Tong and Dhanapalan, Lavanya and Chen, Jake Y.},
  year = {2009},
  volume = {10},
  pages = {177--192},
  issn = {14675463},
  doi = {10.1093/bib/bbp002},
  abstract = {The Semantic Web technology enables integration of heterogeneous data on the World Wide Web by making the semantics of data explicit through formal ontologies. In this article, we survey the feasibility and state of the art of utilizing the Semantic Web technology to represent, integrate and analyze the knowledge in various biomedical networks. We introduce a new conceptual framework, semantic graph mining, to enable researchers to integrate graph mining with ontology reasoning in network data analysis. Through four case studies, we demonstrate how semantic graph mining can be applied to the analysis of disease-causal genes, Gene Ontology category cross-talks, drug efficacy analysis and herb-drug interactions analysis. \textcopyright{} The Author 2009. Published by Oxford University Press.},
  annotation = {ZSCC: 0000061},
  file = {/home/harrisonpl/Documents/PDFs/Chen et al. - 2009 - Semantic web for integrated network analysis in biomedicine.pdf},
  journal = {Briefings in Bioinformatics},
  keywords = {Biomedical network analysis,Graph mining,hunter lab,Network biology,Network medicine,Semantic Web},
  number = {2}
}

@article{chenSurveySystematicAssessment2020,
  title = {A Survey and Systematic Assessment of Computational Methods for Drug Response Prediction},
  author = {Chen, Jinyu and Zhang, Louxin},
  year = {2020},
  pages = {1--10},
  issn = {1467-5463},
  doi = {10.1093/bib/bbz164},
  abstract = {Drug response prediction arises from both basic and clinical research of personalized therapy, as well as drug discovery for cancers. With gene expression profiles and other omics data being available for over 1000 cancer cell lines and tissues, different machine learning approaches have been applied to drug response prediction. These methods appear in a body of literature and have been evaluated on different datasets with only one or two accuracy metrics. We systematically assess 17 representative methods for drug response prediction, which have been developed in the past 5 years, on four large public datasets in nine metrics. This study provides insights and lessons for future research into drug response prediction.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/Chen, Zhang - 2020 - A survey and systematic assessment of computational methods for drug response.pdf},
  journal = {Briefings in Bioinformatics},
  keywords = {bayesian inference}
}

@article{chenUsingRandomForest2004,
  title = {Using Random Forest to Learn Imbalanced Data: {{Technical Report No}}. 666. {{University}} of {{California}}, {{Berkley}}},
  author = {Chen, Chao and Liaw, Andy and Brieman, L.},
  year = {2004},
  pages = {12},
  abstract = {In this paper we propose two ways to deal with the imbalanced data classification problem using random forest. One is based on cost sensitive learning, and the other is based on a sampling technique. Performance metrics such as precision and recall, false positive rate and false negative rate, F-measure and weighted accuracy are computed. Both methods are shown to improve the prediction accuracy of the minority class, and have favorable performance compared to the existing algorithms.},
  annotation = {ZSCC: NoCitationData[s0]},
  journal = {Using Random Forest to Learn Imbalanced Data},
  keywords = {\#nosource}
}

@inproceedings{chiEnhancingJointEntity2019,
  title = {Enhancing Joint Entity and Relation Extraction with Language Modeling and Hierarchical Attention},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Chi, Renjun and Wu, Bin and Hu, Linmei and Zhang, Yunlei},
  year = {2019},
  month = aug,
  volume = {11641 LNCS},
  pages = {314--328},
  publisher = {{Springer Verlag}},
  doi = {10.1007/978-3-030-26072-9_24},
  abstract = {Both entity recognition and relation extraction can benefit from being performed jointly, allowing them to enhance each other. However, existing methods suffer from the sparsity of relevant labels and strongly rely on external natural language processing tools, leading to error propagation. To tackle these problems, we propose an end-to-end joint framework for entity recognition and relation extraction with an auxiliary training objective on language modeling, i.e., learning to predict surrounding words for each word in sentences. Furthermore, we incorporate hierarchical multi-head attention mechanisms into the joint extraction model to capture vital semantic information from the available texts. Experiments show that the proposed approach consistently achieves significant improvements on joint extraction task of entities and relations as compared with strong baselines.},
  isbn = {978-3-030-26071-2},
  keywords = {\#nosource,Entity recognition,Hierarchical attention,Joint model,Language modeling objective,Relation extraction}
}

@incollection{chingOpportunitiesObstaclesDeep2018,
  title = {Opportunities and Obstacles for Deep Learning in Biology and Medicine},
  booktitle = {Opportunities and Obstacles for Deep Learning in Biology and Medicine},
  author = {Ching, Travers and Himmelstein, Daniel S. and {Beaulieu-Jones}, Brett K. and Kalinin, Alexandr A. and Do, Brian T. and Way, Gregory P. and Ferrero, Enrico and Agapow, Paul Michael and Zietz, Michael and Hoffman, Michael M. and Xie, Wei and Rosen, Gail L. and Lengerich, Benjamin J. and Israeli, Johnny and Lanchantin, Jack and Woloszynek, Stephen and Carpenter, Anne E. and Shrikumar, Avanti and Xu, Jinbo and Cofer, Evan M. and Lavender, Christopher A. and Turaga, Srinivas C. and Alexandari, Amr M. and Lu, Zhiyong and Harris, David J. and Decaprio, Dave and Qi, Yanjun and Kundaje, Anshul and Peng, Yifan and Wiley, Laura K. and Segler, Marwin H.S. and Boca, Simina M. and Swamidass, S. Joshua and Huang, Austin and Gitter, Anthony and Greene, Casey S.},
  year = {2018},
  month = apr,
  volume = {15},
  abstract = {Deep learning describes a class of machine learning algorithms that are capable of combining raw inputs into layers of intermediate features. These algorithms have recentlyshown impressive results across avarietyof domains. Biology and medicine are data-rich disciplines, but the data are complex and often ill-understood.Hence, deep learning techniques may be particularly well suited to solve problems of these fields. We examine applications of deep learning to a variety of biomedical problems-patient classification, fundamental biological processes and treatment of patients-And discuss whether deep learning will be able to transform these tasks or if the biomedical sphere poses unique challenges. Following from an extensive literature review, we find that deep learning has yet to revolutionize biomedicine or definitively resolve any of the most pressing challenges in the field, but promising advances have been made on the prior state of the art. Even though improvements over previous baselines have been modest in general, the recent progress indicates that deep learning methods will provide valuable means for speeding up or aiding human investigation. Though progress has been made linking a specific neural network's prediction to input features, understanding how users should interpret these models to make testable hypotheses about the system under study remains an open challenge. Furthermore, the limited amount of labelled data for training presents problems in some domains, as do legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning enabling changes at both bench and bedside with the potential to transform several areas of biology and medicine.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/Ching et al. - 2018 - Opportunities and obstacles for deep learning in biology and medicine.pdf},
  isbn = {0000000305396}
}

@article{choiLearningLowDimensionalRepresentations2016,
  title = {Learning {{Low}}-{{Dimensional Representations}} of {{Medical Concepts}}.},
  author = {Choi, Youngduck and Chiu, Chill Yi-I and Sontag, David},
  year = {2016},
  volume = {2016},
  pages = {41--50},
  issn = {2153-4063},
  abstract = {We show how to learn low-dimensional representations (embeddings) of a wide range of concepts in medicine, including diseases (e.g., ICD9 codes), medications, procedures, and laboratory tests. We expect that these embeddings will be useful across medical informatics for tasks such as cohort selection and patient summarization. These embeddings are learned using a technique called neural language modeling from the natural language processing community. However, rather than learning the embeddings solely from text, we show how to learn the embeddings from claims data, which is widely available both to providers and to payers. We also show that with a simple algorithmic adjustment, it is possible to learn medical concept embeddings in a privacy preserving manner from co-occurrence counts derived from clinical narratives. Finally, we establish a methodological framework, arising from standard medical ontologies such as UMLS, NDF-RT, and CCS, to further investigate the embeddings and precisely characterize their quantitative properties.},
  annotation = {ZSCC: 0000137},
  file = {/home/harrisonpl/Documents/PDFs/Choi et al. - 2016 - Learning Low-Dimensional Representations of Medical Concepts.pdf},
  journal = {AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science}
}

@article{cholletMeasureIntelligence2019,
  title = {On the {{Measure}} of {{Intelligence}}},
  author = {Chollet, Fran{\c c}ois},
  year = {2019},
  month = nov,
  abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
  archivePrefix = {arXiv},
  eprint = {1911.01547},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Chollet - 2019 - On the Measure of Intelligence.pdf;/home/harrisonpl/Zotero/storage/CYA6Y7CE/1911.html},
  journal = {arXiv:1911.01547 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  primaryClass = {cs}
}

@article{chuMiRgoIntegratingVarious2020,
  title = {{{miRgo}}: Integrating Various off-the-Shelf Tools for Identification of {{microRNA}}\textendash Target Interactions by Heterogeneous Features and a Novel Evaluation Indicator},
  author = {Chu, Yen Wei and Chang, Kai Po and Chen, Chi Wei and Liang, Yu Tai and Soh, Zhi Thong and Hsieh, Li Ching},
  year = {2020},
  volume = {10},
  pages = {1--11},
  issn = {20452322},
  doi = {10.1038/s41598-020-58336-5},
  abstract = {MicroRNAs (miRNAs) are short non-coding RNAs that regulate gene expression and biological processes through binding to messenger RNAs. Predicting the relationship between miRNAs and their targets is crucial for research and clinical applications. Many tools have been developed to predict miRNA\textendash target interactions, but variable results among the different prediction tools have caused confusion for users. To solve this problem, we developed miRgo, an application that integrates many of these tools. To train the prediction model, extreme values and median values from four different data combinations, which were obtained via an energy distribution function, were used to find the most representative dataset. Support vector machines were used to integrate 11 prediction tools, and numerous feature types used in these tools were classified into six categories\textemdash binding energy, scoring function, evolution evidence, binding type, sequence property, and structure\textemdash to simplify feature selection. In addition, a novel evaluation indicator, the Chu-Hsieh-Liang (CHL) index, was developed to improve the prediction power in positive data for feature selection. miRgo achieved better results than all other prediction tools in evaluation by an independent testing set and by its subset of functionally important genes. The tool is available at http://predictor.nchu.edu.tw/miRgo.},
  annotation = {ZSCC: 0000000},
  file = {/home/harrisonpl/Documents/PDFs/Chu et al. - 2020 - miRgo.pdf},
  journal = {Scientific Reports},
  keywords = {David Port,Recommendation},
  number = {1}
}

@article{ciccareseOpenSemanticAnnotation2012,
  title = {Open Semantic Annotation of Scientific Publications Using {{DOMEO}}},
  author = {Ciccarese, Paolo and Ocana, Marco and Clark, Tim},
  year = {2012},
  volume = {3},
  pages = {S1},
  issn = {20411480},
  doi = {10.1186/2041-1480-3-S1-S1},
  abstract = {Background: Our group has developed a useful shared software framework for performing, versioning, sharing and viewing Web annotations of a number of kinds, using an open representation model. Methods: The Domeo Annotation Tool was developed in tandem with this open model, the Annotation Ontology (AO). Development of both the Annotation Framework and the open model was driven by requirements of several different types of alpha users, including bench scientists and biomedical curators from university research labs, online scientific communities, publishing and pharmaceutical companies. Several use cases were incrementally implemented by the toolkit. These use cases in biomedical communications include personal note-taking, group document annotation, semantic tagging, claim-evidence-context extraction, reagent tagging, and curation of textmining results from entity extraction algorithms. Results: We report on the Domeo user interface here. Domeo has been deployed in beta release as part of the NIH Neuroscience Information Framework (NIF, http://www.neuinfo.org ) and is scheduled for production deployment in the NIF's next full release. Future papers will describe other aspects of this work in detail, including Annotation Framework Services and components for integrating with external textmining services, such as the NCBO Annotator web service, and with other textmining applications using the Apache UIMA framework.},
  annotation = {ZSCC: 0000060},
  file = {/home/harrisonpl/Documents/PDFs/Ciccarese et al. - 2012 - Open semantic annotation of scientific publications using DOMEO.pdf},
  journal = {Journal of Biomedical Semantics},
  keywords = {hunter lab,Natural language processing},
  number = {1}
}

@article{cohenGettingStartedText2008,
  title = {Getting Started in Text Mining},
  author = {Cohen, K. Bretonnel and Hunter, Lawrence},
  year = {2008},
  volume = {4},
  pages = {0001--0003},
  issn = {1553734X},
  doi = {10.1371/journal.pcbi.0040020},
  annotation = {ZSCC: 0000256},
  file = {/home/harrisonpl/Documents/PDFs/Cohen, Hunter - 2008 - Getting started in text mining.pdf},
  journal = {PLoS Computational Biology},
  keywords = {Biologists,Breast cancer,Database searching,Fats,Genetic causes of cancer,hunter lab,Scientists,Software engineering,Text mining},
  number = {1}
}

@article{constuctionBrianConciseCommon,
  title = {Brian's {{Concise Common Lisp Reference Sheet Control Structures}} , {{Loops}} , {{Code Blocks}}},
  author = {Constuction, List and Alteration, List and Access, List},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/Constuction et al. - Brian's Concise Common Lisp Reference Sheet Control Structures , Loops , Code.pdf}
}

@article{covertSimultaneousCrossEvaluationHeterogeneous2019,
  title = {Simultaneous {{Cross}}-{{Evaluation}} of {{Heterogeneous E}}. Coli {{Datasets}} via {{Mechanistic Simulation}}},
  author = {Covert, Markus},
  year = {2019},
  month = feb,
  volume = {116},
  pages = {451a},
  issn = {00063495},
  doi = {10.1016/j.bpj.2018.11.2429},
  file = {/home/harrisonpl/Documents/PDFs/Covert - 2019 - Simultaneous Cross-Evaluation of Heterogeneous E.pdf},
  journal = {Biophysical Journal},
  language = {en},
  number = {3}
}

@book{craverSearchMechanismsDiscoveries2014,
  title = {In Search of Mechanisms: Discoveries across the Life Sciences},
  author = {Craver, Carl F. and Darden, Lindley},
  year = {2014},
  volume = {51},
  publisher = {{University of Chicago Press}},
  address = {{Chicago and London}},
  abstract = {With In Search of Mechanisms, Carl F. Craver and Lindley Darden offer both a descriptive and an instructional account of how biologists discover mechanisms. Drawing on examples from across the life sciences and through the centuries, Craver and Darden compile an impressive toolbox of strategies that biologists have used and will use again to reveal the mechanisms that produce, underlie, or maintain the phenomena characteristic of living things. They discuss the questions that figure in the search for mechanisms, characterizing the experimental, observational, and conceptual considerations used to answer them, all the while providing examples from the history of biology to highlight the kinds of evidence and reasoning strategies employed to assess mechanisms. At a deeper level, Craver and Darden pose a systematic view of what biology is, of how biology makes progress, of how biological discoveries are and might be made, and of why knowledge of biological mechanisms is important for the future of the human species. \textendash},
  annotation = {ZSCC: NoCitationData[s0]},
  isbn = {978-0-226-03979-4},
  keywords = {\#nosource,biological mechanism,biology,mechanism}
}

@article{csermelyLearningSignalingNetworks2020,
  title = {Learning of {{Signaling Networks}}: {{Molecular Mechanisms}}},
  author = {Csermely, P{\'e}ter and Kunsic, Nina and Mendik, P{\'e}ter and Kerest{\'e}ly, M{\'a}rk and Farag{\'o}, Teod{\'o}ra and Veres, D{\'a}niel V. and Tompa, P{\'e}ter},
  year = {2020},
  volume = {xx},
  pages = {1--11},
  issn = {13624326},
  doi = {10.1016/j.tibs.2019.12.005},
  abstract = {Molecular processes of neuronal learning have been well described. However, learning mechanisms of non-neuronal cells are not yet fully understood at the molecular level. Here, we discuss molecular mechanisms of cellular learning, including conformational memory of intrinsically disordered proteins (IDPs) and prions, signaling cascades, protein translocation, RNAs [miRNA and long noncoding RNA (lncRNA)], and chromatin memory. We hypothesize that these processes constitute the learning of signaling networks and correspond to a generalized Hebbian learning process of single, non-neuronal cells, and we discuss how cellular learning may open novel directions in drug design and inspire new artificial intelligence methods.},
  annotation = {ZSCC: 0000000},
  file = {/home/harrisonpl/Documents/PDFs/Csermely et al. - 2020 - Learning of Signaling Networks.pdf},
  journal = {Trends in Biochemical Sciences},
  keywords = {epigenetic memory,epithelial–mesenchymal transition,histone modification,molecular memory,signaling pathways,system-level memory},
  number = {xx}
}

@misc{cubeRubixCubeSolve,
  title = {Rubix {{Cube Solve Guide}}},
  author = {Cube, Rubix},
  file = {/home/harrisonpl/Documents/PDFs/Cube - Rubix Cube Solve Guide.pdf}
}

@article{czakonHighPrecisionDifferentialPredictions2016,
  title = {High-{{Precision Differential Predictions}} for {{Top}}-{{Quark Pairs}} at the {{LHC}}},
  author = {Czakon, Michal and Heymes, David and Mitov, Alexander},
  year = {2016},
  volume = {116},
  issn = {10797114},
  doi = {10.1103/PhysRevLett.116.082003},
  abstract = {We present the first complete next-to-next-to-leading order (NNLO) QCD predictions for differential distributions in the top-quark pair production process at the LHC. Our results are derived from a fully differential partonic Monte Carlo calculation with stable top quarks which involves no approximations beyond the fixed-order truncation of the perturbation series. The NNLO corrections improve the agreement between existing LHC measurements [V. Khachatryan et al. (CMS Collaboration), Eur. Phys. J. C 75, 542 (2015)] and standard model predictions for the top-quark transverse momentum distribution, thus helping alleviate one long-standing discrepancy. The shape of the top-quark pair invariant mass distribution turns out to be stable with respect to radiative corrections beyond NLO which increases the value of this observable as a place to search for physics beyond the standard model. The results presented here provide essential input for parton distribution function fits, implementation of higher-order effects in Monte Carlo generators, as well as top-quark mass and strong coupling determination.},
  annotation = {ZSCC: 0000326},
  file = {/home/harrisonpl/Documents/PDFs/Czakon et al. - 2016 - High-Precision Differential Predictions for Top-Quark Pairs at the LHC.pdf},
  journal = {Physical Review Letters},
  number = {8}
}

@article{dardenForthcomingAnthologyBuilding2016,
  title = {For Forthcoming Anthology , " {{Building Theories}} ," {{Edited}} by {{Emiliano Ippoliti}} and {{David Danks}} , to Be Published by {{Springer}} , Based on Workshop " {{Building Theories}} "},
  author = {Darden, Lindley and Pal, Lipika R and Kundu, Kunal and Moult, John},
  year = {2016},
  pages = {1--33},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/Darden et al. - 2016 - For forthcoming anthology , Building Theories , Edited by Emiliano Ippoliti.pdf},
  keywords = {diagrams,discovery,genetic disease,mechanism}
}

@article{dardenGeneralizationsBiology1996,
  title = {Generalizations in {{Biology}}},
  author = {Darden, Lindley},
  year = {1996},
  month = sep,
  volume = {27},
  pages = {409--419},
  issn = {0039-3681},
  doi = {10.1016/0039-3681(95)00050-X},
  file = {/home/harrisonpl/Documents/PDFs/Darden - 1996 - Generalizations in Biology.pdf;/home/harrisonpl/Zotero/storage/CDNL4X4H/003936819500050X.html},
  journal = {Studies in History and Philosophy of Science Part A},
  language = {en},
  number = {3}
}

@article{dardenHarnessingFormalConcepts2018,
  title = {Harnessing Formal Concepts of Biological Mechanism to Analyze Human Disease},
  author = {Darden, Lindley and Kundu, Kunal and Pal, Lipika R. and Moult, John},
  year = {2018},
  volume = {14},
  pages = {1--10},
  issn = {15537358},
  doi = {10.1371/journal.pcbi.1006540},
  abstract = {Mechanism is a widely used concept in biology. In 2017, more than 10\% of PubMed abstracts used the term. Therefore, searching for and reasoning about mechanisms is fundamental to much of biomedical research, but until now there has been almost no computational infrastructure for this purpose. Recent work in the philosophy of science has explored the central role that the search for mechanistic accounts of biological phenomena plays in biomedical research, providing a conceptual basis for representing and analyzing biological mechanism. The foundational categories for components of mechanisms\textemdash entities and activities\textemdash guide the development of general, abstract types of biological mechanism parts. Building on that analysis, we have developed a formal framework for describing and representing biological mechanism, MecCog, and applied it to describing mechanisms underlying human genetic disease. Mechanisms are depicted using a graphical notation. Key features are assignment of mechanism components to stages of biological organization and classes; visual representation of uncertainty, ignorance, and ambiguity; and tight integration with literature sources. The MecCog framework facilitates analysis of many aspects of disease mechanism, including the prioritization of future experiments, probing of gene-drug and gene-environment interactions, identification of possible new drug targets, personalized drug choice, analysis of nonlinear interactions between relevant genetic loci, and classification of diseases based on mechanism.},
  annotation = {ZSCC: 0000003},
  file = {/home/harrisonpl/Documents/PDFs/Darden et al. - 2018 - Harnessing formal concepts of biological mechanism to analyze human disease.pdf},
  journal = {PLoS Computational Biology},
  keywords = {biology,Complex traits,Crohn's disease,Genetic loci,Genetics of disease,Genome-wide association studies,Graphs,mechanism,Molecular genetics,ontology,Phenotypes},
  number = {12}
}

@article{dardenProductGuidesProcess2018,
  title = {The Product Guides the Process: {{Discovering}} Disease Mechanisms},
  shorttitle = {The Product Guides the Process},
  author = {Darden, Lindley and Pal, Lipika R. and Kundu, Kunal and Moult, John},
  year = {2018},
  volume = {41},
  pages = {101--117},
  issn = {21926263},
  doi = {10.1007/978-3-319-72787-5_6},
  abstract = {The nature of the product to be discovered guides the reasoning to discover it. Biologists and medical researchers often search for mechanisms. The ``new mechanistic philosophy of science'' provides resources about the nature of biological mechanisms that aid the discovery of mechanisms. Here, we apply these resources to the discovery of mechanisms in medicine. A new diagrammatic representation of a disease mechanism chain indicates both what is known and, most significantly, what is not known at a given time, thereby guiding the researcher and collaborators in discovery. Mechanisms of genetic diseases provide the examples.},
  annotation = {ZSCC: 0000006},
  keywords = {\#nosource,Diagrams,Discovery,Genetic disease,Mechanism}
}

@article{dardenStrategiesDiscoveringMechanisms2002,
  title = {Strategies for {{Discovering Mechanisms}}: {{Schema Instantiation}}, {{Modular Subassembly}}, {{Forward}}/{{Backward Chaining}}},
  shorttitle = {Strategies for {{Discovering Mechanisms}}},
  author = {Darden, Lindley},
  year = {2002},
  month = sep,
  volume = {69},
  pages = {S354-S365},
  publisher = {{The University of Chicago Press}},
  issn = {0031-8248},
  doi = {10.1086/341858},
  abstract = {Discovery proceeds in stages of construction, evaluation, and revision. Each of these stages is constrained by what is known or conjectured about what is being discovered. A new characterization of mechanism aids in specifying what is to be discovered when a mechanism is sought. Guidance in discovering mechanisms may be provided by the reasoning strategies of schema instantiation, modular subassembly, and forward/backward chaining. Examples are found in mechanisms in molecular biology, biochemistry, immunology, and evolutionary biology.},
  file = {/home/harrisonpl/Documents/PDFs/Darden - 2002 - Strategies for Discovering Mechanisms.pdf;/home/harrisonpl/Zotero/storage/UDERPQNJ/Darden - 2002 - Strategies for Discovering Mechanisms Schema Inst.pdf;/home/harrisonpl/Zotero/storage/E4MFALWM/341858.html},
  journal = {Philosophy of Science},
  number = {S3}
}

@article{DataSourcesVivo,
  title = {Data Sources for in Vivo Molecular Profiling of Human Phenotypes - {{Google Search}}},
  keywords = {\#nosource}
}

@article{deepaRelationExtractionUsing2019,
  title = {Relation {{Extraction Using Deep Learning Methods}}-a {{Survey}}},
  author = {Deepa, C A and Reghuraj, P C and Ramanujan, Ajeesh},
  year = {2019},
  volume = {6956},
  pages = {3},
  issn = {2229-6956},
  doi = {10.21917/ijsc.2019.0263},
  abstract = {Relation extraction has an important role in extracting structured information from unstructured raw text. This task is a crucial ingredient in numerous information extraction systems seeking to mine structured facts from text. Nowadays, neural networks play an important role in the task of relation extraction. The traditional non deep learning models require feature engineering. Deep Learning models such as Convolutional Neural Networks and Long Short Term Memory networks require less feature engineering than non-deep learning models. Relation Extraction has the potential of employing deep learning models with the creation of huge datasets using distant supervision. This paper surveys the current trend in Relation Extraction using Deep Learning models.},
  annotation = {ZSCC: 0000000},
  file = {/home/harrisonpl/Documents/PDFs/Deepa et al. - 2019 - Relation Extraction Using Deep Learning Methods-a Survey.pdf},
  journal = {Ictact Journal on Soft Computing},
  keywords = {CNN,Deep Learning,LSTM,Relation Extraction,word Embeddings},
  number = {April}
}

@article{DeepreviewCollaborativelyWritten,
  title = {Deep-Review: {{A}} Collaboratively Written Review Paper on Deep Learning, Genomics, and Precision Medicine},
  shorttitle = {Deep-Review},
  keywords = {\#nosource,Lawrence Hunter,Recommendation}
}

@article{dejongModelingSimulationGenetic2002,
  title = {Modeling and Simulation of Genetic Regulatory Systems: {{A}} Literature Review},
  shorttitle = {Modeling and {{Simulation}} of {{Genetic Regulatory Syst}}},
  author = {De Jong, Hidde},
  year = {2002},
  month = jan,
  volume = {9},
  pages = {67--103},
  issn = {10665277},
  doi = {10.1089/10665270252833208},
  abstract = {In order to understand the functioning of organisms on the molecular level, we need to know which genes are expressed, when and where in the organism, and to which extent. The regulation of gene expression is achieved through genetic regulatory systems structured by networks of interactions between DNA, RNA, proteins, and small molecules. As most genetic regulatory networks of interest involve many components connected through interlocking positive and negative feedback loops, an intuitive understanding of their dynamics is hard to obtain. As a consequence, formal methods and computer tools for the modeling and simulation of genetic regulatory networks will be indispensable. This paper reviews formalisms that have been employed in mathematical biology and bioinformatics to describe genetic regulatory systems, in particular directed graphs, Bayesian networks, Boolean networks and their generalizations, ordinary and partial differential equations, qualitative differential equations, stochastic equations, and rule-based formalisms. In addition, the paper discusses how these formalisms have been used in the simulation of the behavior of actual regulatory systems.},
  annotation = {ZSCC: 0003077},
  file = {/home/harrisonpl/Documents/PDFs/De Jong - 2002 - Modeling and simulation of genetic regulatory systems.pdf},
  journal = {Journal of Computational Biology},
  keywords = {\#nosource,Computational biology,Genetic regulatory networks,HPL comprehensive exam,Mathematical modeling,Simulation},
  number = {1}
}

@article{delauEpidemiologyParkinsonDisease2006,
  title = {Epidemiology of {{Parkinson}}'s Disease},
  author = {{de Lau}, Lonneke ML and Breteler, Monique MB},
  year = {2006},
  volume = {5},
  pages = {525--535},
  issn = {14744422},
  doi = {10.1016/S1474-4422(06)70471-9},
  abstract = {The causes of Parkinson's disease (PD), the second most common neurodegenerative disorder, are still largely unknown. Current thinking is that major gene mutations cause only a small proportion of all cases and that in most cases, non-genetic factors play a part, probably in interaction with susceptibility genes. Numerous epidemiological studies have been done to identify such non-genetic risk factors, but most were small and methodologically limited. Larger, well-designed prospective cohort studies have only recently reached a stage at which they have enough incident patients and person-years of follow-up to investigate possible risk factors and their interactions. In this article, we review what is known about the prevalence, incidence, risk factors, and prognosis of PD from epidemiological studies. \textcopyright{} 2006 Elsevier Ltd. All rights reserved.},
  annotation = {ZSCC: 0003543},
  journal = {Lancet Neurology},
  keywords = {\#nosource,Age Factors,Humans,Parkinson Disease,Prognosis,Risk Factors},
  number = {6}
}

@article{delGNUEmacs211997,
  title = {{{GNU Emacs}} 21 {{Reference Card GNU Emacs Reference Card Buffers}}},
  author = {Del, C-x and Tab, C-x},
  year = {1997},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/Del, Tab - 1997 - GNU Emacs 21 Reference Card GNU Emacs Reference Card Buffers.pdf},
  journal = {Info}
}

@inproceedings{demner-fushmanProceedings18thBioNLP2019,
  title = {Proceedings of the 18th {{BioNLP Workshop}} and {{Shared Task}}},
  author = {{Demner-Fushman}, Dina and Cohen, K Bretonnel and Ananiadou, Sophia and Tsujii, Jun'ichi},
  year = {2019},
  month = aug,
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  annotation = {ZSCC: 0000000},
  file = {/home/harrisonpl/Documents/PDFs/Demner-Fushman et al. - 2019 - Proceedings of the 18th BioNLP Workshop and Shared Task.pdf},
  keywords = {HPL comprehensive exam}
}

@article{dingFATREFasterDependencyfree2020,
  title = {{{FAT}}-{{RE}}: {{A}} Faster Dependency-Free Model for Relation Extraction},
  shorttitle = {{{FAT}}-{{RE}}},
  author = {Ding, Lifang and Lei, Zeyang and Xun, Guangxu and Yang, Yujiu},
  year = {2020},
  month = aug,
  pages = {100598},
  issn = {1570-8268},
  doi = {10.1016/j.websem.2020.100598},
  abstract = {Recent years have seen the dependency tree as effective information for relation extraction. Two problems still exist in previous methods: (1) dependency tree relies on external tools and needs to be carefully integrated with a trade-off between pruning noisy words and keeping semantic integrity; (2) dependency-based methods still have to encode sequential context as a supplement, which needs extra time. To tackle the two problems, we propose a faster dependency-free model in this paper: regarding the sentence as a fully-connected graph, we customize the vanilla transformer architecture to remove the irrelevant information via filtering mechanism and further aggregate the sentence information through the enhanced query. Our model yields comparable results on the SemEval2010 Task8 dataset and better results on the TACRED dataset, without requiring external information from the dependency tree but with improved time efficiency.},
  file = {/home/harrisonpl/Documents/PDFs/Ding et al. - 2020 - FAT-RE.pdf;/home/harrisonpl/Zotero/storage/X6QWN7HV/S1570826820300366.html},
  journal = {Journal of Web Semantics},
  keywords = {Aggregation,Filtering,Relation extraction,Transformer},
  language = {en}
}

@article{diRelationExtractionDomainaware2019,
  title = {Relation Extraction via Domain-Aware Transfer Learning},
  author = {Di, Shimin and Shen, Yanyan and Chen, Lei},
  year = {2019},
  pages = {1348--1357},
  doi = {10.1145/3292500.3330890},
  abstract = {Relation extraction in knowledge base construction has been researched for the last decades due to its applicability to many problems. Most classical works, such as supervised information extraction [2] and distant supervision [23], focus on how to construct the knowledge base (KB) by utilizing the large number of labels or certain related KBs. However, in many real-world scenarios, the existing methods may not perform well when a new knowledge base is required but only scarce labels or few related KBs available. In this paper, we propose a novel approach called, Relation Extraction via Domain-aware Transfer Learning (ReTrans), to extract relation mentions from a given text corpus by exploring the experience from a large amount of existing KBs which may not be closely related to the target relation. We first propose to initialize the representation of relation mentions from the massive text corpus and update those representations according to existing KBs. Based on the representations of relation mentions, we investigate the contribution of each KB to the target task and propose to select useful KBs for boosting the effectiveness of the proposed approach. Based on selected KBs, we develop a novel domain-aware transfer learning framework to transfer knowledge from source domains to the target domain, aiming to infer the true relation mentions in the unstructured text corpus. Most importantly, we give the stability and generalization bound of ReTrans. Experimental results on the real world datasets well demonstrate that the effectiveness of our approach, which outperforms all the state-of-the-art baselines.},
  annotation = {ZSCC: 0000000},
  file = {/home/harrisonpl/Documents/PDFs/Di et al. - 2019 - Relation extraction via domain-aware transfer learning.pdf},
  journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  keywords = {Relation extraction,Transfer learning}
}

@article{dongChemDesIntegratedWebbased2015,
  title = {{{ChemDes}}: {{An}} Integrated Web-Based Platform for Molecular Descriptor and Fingerprint Computation},
  shorttitle = {{{ChemDes}}},
  author = {Dong, Jie and Cao, Dong Sheng and Miao, Hong Yu and Liu, Shao and Deng, Bai Chuan and Yun, Yong Huan and Wang, Ning Ning and Lu, Ai Ping and Zeng, Wen Bin and Chen, Alex F.},
  year = {2015},
  volume = {7},
  pages = {60},
  issn = {17582946},
  doi = {10.1186/s13321-015-0109-z},
  abstract = {Background: Molecular descriptors and fingerprints have been routinely used in QSAR/SAR analysis, virtual drug screening, compound search/ranking, drug ADME/T prediction and other drug discovery processes. Since the calculation of such quantitative representations of molecules may require substantial computational skills and efforts, several tools have been previously developed to make an attempt to ease the process. However, there are still several hurdles for users to overcome to fully harness the power of these tools. First, most of the tools are distributed as standalone software or packages that require necessary configuration or programming efforts of users. Second, many of the tools can only calculate a subset of molecular descriptors, and the results from multiple tools need to be manually merged to generate a comprehensive set of descriptors. Third, some packages only provide application programming interfaces and are implemented in different computer languages, which pose additional challenges to the integration of these tools. Results: A freely available web-based platform, named ChemDes, is developed in this study. It integrates multiple state-of-the-art packages (i.e., Pybel, CDK, RDKit, BlueDesc, Chemopy, PaDEL and jCompoundMapper) for computing molecular descriptors and fingerprints. ChemDes not only provides friendly web interfaces to relieve users from burdensome programming work, but also offers three useful and convenient auxiliary tools for format converting, MOPAC optimization and fingerprint similarity calculation. Currently, ChemDes has the capability of computing 3679 molecular descriptors and 59 types of molecular fingerprints. Conclusion: ChemDes provides users an integrated and friendly tool to calculate various molecular descriptors and fingerprints. It is freely available at http://www.scbdd.com/chemdes. The source code of the project is also available as a supplementary file.},
  annotation = {ZSCC: 0000082},
  file = {/home/harrisonpl/Documents/PDFs/Dong et al. - 2015 - ChemDes.pdf},
  journal = {Journal of Cheminformatics},
  keywords = {Chemoinformatics,Molecular descriptors,Molecular fingerprints,Molecular representation,Online descriptor calculation,QSAR/QSPR},
  number = {1}
}

@incollection{douglashofstadterCopycatProjectModel,
  title = {The {{Copycat Project}}: {{A Model}} of {{Mental Pluidity}} and {{Analogy}}-Making},
  author = {{Douglas Hofstadter} and {Melanie Mitchell}}
}

@inproceedings{duanHybridGraphModel2019,
  title = {A Hybrid Graph Model for Distant Supervision Relation Extraction},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Duan, Shangfu and Gao, Huan and Liu, Bing and Qi, Guilin},
  year = {2019},
  month = jun,
  volume = {11503 LNCS},
  pages = {36--51},
  publisher = {{Springer Verlag}},
  doi = {10.1007/978-3-030-21348-0_3},
  abstract = {Distant supervision has advantages of generating training data automatically for relation extraction by aligning triples in Knowledge Graphs with large-scale corpora. Some recent methods attempt to incorporate extra information to enhance the performance of relation extraction. However, there still exist two major limitations. Firstly, these methods are tailored for a specific type of information which is not enough to cover most of the cases. Secondly, the introduced extra information may contain noise. To address these issues, we propose a novel hybrid graph model, which can incorporate heterogeneous background information in a unified framework, such as entity types and human-constructed triples. These various kinds of knowledge can be integrated efficiently even with several missing cases. In addition, we further employ an attention mechanism to identify the most confident information which can alleviate the side effect of noise. Experimental results demonstrate that our model outperforms the state-of-the-art methods significantly in various evaluation metrics.},
  annotation = {ZSCC: 0000000},
  isbn = {978-3-030-21347-3},
  keywords = {\#nosource,Distant supervision,Heterogeneous information,Hybrid graph,Relation extraction}
}

@inproceedings{dunnCloakingMalwareTrusted2011,
  title = {Cloaking Malware with the Trusted Platform Module},
  author = {Dunn, Alan M. and Hofmann, Owen S. and Waters, Brent and Witchel, Emmett},
  year = {2011},
  abstract = {The Trusted Platform Module (TPM) is commonly thought of as hardware that can increase platform security. However, it can also be used for malicious purposes. The TPM, along with other hardware, can implement a cloaked computation, whose memory state cannot be observed by any other software, including the operating system and hypervisor. We show that malware can use cloaked computations to hide essential secrets (like the target of an attack) from a malware analyst. We describe and implement a protocol that establishes an encryption key under control of the TPM that can only be used by a specific infection program. An infected host then proves the legitimacy of this key to a remote malware distribution platform, and receives and executes an encrypted payload in a way that prevents software visibility of the decrypted payload. We detail how malware can benefit from cloaked computations and discuss defenses against our protocol. Hardening legitimate uses of the TPM against attack improves the resilience of our malware, creating a Catch-22 for secure computing technology.},
  keywords = {\#nosource}
}

@article{edwardsBeginnerGuideComparative2013,
  title = {Beginner's Guide to Comparative Bacterial Genome Analysis Using next-Generation Sequence Data},
  author = {Edwards, David J and Holt, Kathryn E},
  year = {2013},
  volume = {3},
  pages = {2},
  issn = {2042-5783},
  doi = {10.1186/2042-5783-3-2},
  abstract = {High throughput sequencing is now fast and cheap enough to be considered part of the toolbox for investigating bacteria, and there are thousands of bacterial genome sequences available for comparison in the public domain. Bacterial genome analysis is increasingly being performed by diverse groups in research, clinical and public health labs alike, who are interested in a wide array of topics related to bacterial genetics and evolution. Examples include outbreak analysis and the study of pathogenicity and antimicrobial resistance. In this beginner's guide, we aim to provide an entry point for individuals with a biology background who want to perform their own bioinformatics analysis of bacterial genome data, to enable them to answer their own research questions. We assume readers will be familiar with genetics and the basic nature of sequence data, but do not assume any computer programming skills. The main topics covered are assembly, ordering of contigs, annotation, genome comparison and extracting common typing information. Each section includes worked examples using publicly available E. coli data and free software tools, all which can be performed on a desktop computer.},
  annotation = {ZSCC: 0000114},
  file = {/home/harrisonpl/Documents/PDFs/Edwards, Holt - 2013 - Beginner's guide to comparative bacterial genome analysis using next-generation.pdf},
  journal = {Microbial Informatics and Experimentation},
  keywords = {Analysis,Bacterial,Comparative,Genomics,Methods,Microbial,Next generation sequencing},
  number = {1}
}

@article{edwardsComparativeBacterialGenome2009,
  title = {Comparative Bacterial Genome Analysis},
  author = {Edwards, David J and Holt, Kathryn E},
  year = {2009},
  volume = {25},
  pages = {1422--1423},
  issn = {13674803},
  doi = {10.1093/bioinformatics/btp163},
  abstract = {The Biopython Project is an international association of developers of freely available Python (http://www.python.org) tools for computational molecular biology. The web site http://www.biopython.org provides an online resource for modules, scripts, and web links for developers of Python-based software for life science research.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/Edwards, Holt - 2009 - Comparative bacterial genome analysis.pdf},
  journal = {Bioinformatics},
  keywords = {Algorithm Analysis and Problem Complexity,Bioinformatics,Microbial Genetics and Genomics,Microbiology},
  number = {11}
}

@article{eilbeckSequenceOntologyTool2005,
  title = {The {{Sequence Ontology}}: A Tool for the Unification of Genome Annotations.},
  shorttitle = {The {{Sequence Ontology}}},
  author = {Eilbeck, Karen and Lewis, Suzanna E. and Mungall, Christopher J. and Yandell, Mark and Stein, Lincoln and Durbin, Richard and Ashburner, Michael},
  year = {2005},
  volume = {6},
  pages = {R44},
  issn = {14656914},
  doi = {10.1186/gb-2005-6-5-r44},
  abstract = {The Sequence Ontology (SO) is a structured controlled vocabulary for the parts of a genomic annotation. SO provides a common set of terms and definitions that will facilitate the exchange, analysis and management of genomic data. Because SO treats part-whole relationships rigorously, data described with it can become substrates for automated reasoning, and instances of sequence features described by the SO can be subjected to a group of logical operations termed extensional mereology operators.},
  annotation = {ZSCC: 0000693},
  file = {/home/harrisonpl/Documents/PDFs/Eilbeck et al. - 2005 - The Sequence Ontology.pdf},
  journal = {Genome biology},
  keywords = {ontology},
  number = {5}
}

@article{EmacsCheatsheet,
  ids = {EmacsCheatsheeta},
  title = {Emacs {{Cheatsheet}}},
  file = {/home/harrisonpl/Documents/PDFs/Emacs Cheatsheet.pdf},
  keywords = {Cheatsheet,emacs},
  mendeley-tags = {Cheatsheet,emacs}
}

@article{erhardtStatusTextminingTechniques2006,
  title = {Status of Text-Mining Techniques Applied to Biomedical Text},
  author = {Erhardt, Ram{\'o}n A.A. and Schneider, Reinhard and Blaschke, Christian},
  year = {2006},
  volume = {11},
  pages = {315--325},
  issn = {13596446},
  doi = {10.1016/j.drudis.2006.02.011},
  abstract = {Scientific progress is increasingly based on knowledge and information. Knowledge is now recognized as the driver of productivity and economic growth, leading to a new focus on the role of information in the decision-making process. Most scientific knowledge is registered in publications and other unstructured representations that make it difficult to use and to integrate the information with other sources (e.g. biological databases). Making a computer understand human language has proven to be a complex achievement, but there are techniques capable of detecting, distinguishing and extracting a limited number of different classes of facts. In the biomedical field, extracting information has specific problems: complex and ever-changing nomenclature (especially genes and proteins) and the limited representation of domain knowledge. \textcopyright{} 2005 Elsevier Ltd. All rights reserved.},
  journal = {Drug Discovery Today},
  keywords = {\#nosource,Natural language processing},
  number = {7-8}
}

@article{evangelouMetaanalysisMethodsGenomewide2013,
  title = {Meta-Analysis Methods for Genome-Wide Association Studies and Beyond},
  author = {Evangelou, Evangelos and Ioannidis, John P.A.},
  year = {2013},
  volume = {14},
  abstract = {Meta-analysis of genome-wide association studies (GWASs) has become a popular method for discovering genetic risk variants. Here, we overview both widely applied and newer statistical methods for GWAS meta-analysis, including issues of interpretation and assessment of sources of heterogeneity. We also discuss extensions of these meta-analysis methods to complex data. Where possible, we provide guidelines for researchers who are planning to use these methods. Furthermore, we address special issues that may arise for meta-analysis of sequencing data and rare variants. Finally, we discuss challenges and solutions surrounding the goals of making meta-analysis data publicly available and building powerful consortia. \textcopyright{} 2013 Macmillan Publishers Limited.},
  annotation = {ZSCC: 0000427},
  file = {/home/harrisonpl/Documents/PDFs/Evangelou, Ioannidis - 2013 - Meta-analysis methods for genome-wide association studies and beyond.pdf},
  keywords = {Disease genetics}
}

@article{fabregatReactomePathwayKnowledgebase2018,
  title = {The {{Reactome Pathway Knowledgebase}}},
  author = {Fabregat, Antonio and Jupe, Steven and Matthews, Lisa and Sidiropoulos, Konstantinos and Gillespie, Marc and Garapati, Phani and Haw, Robin and Jassal, Bijay and Korninger, Florian and May, Bruce and Milacic, Marija and Roca, Corina Duenas and Rothfels, Karen and Sevilla, Cristoffer and Shamovsky, Veronica and Shorser, Solomon and Varusai, Thawfeek and Viteri, Guilherme and Weiser, Joel and Wu, Guanming and Stein, Lincoln and Hermjakob, Henning and D'Eustachio, Peter},
  year = {2018},
  volume = {46},
  pages = {D649--D655},
  issn = {13624962},
  doi = {10.1093/nar/gkx1132},
  abstract = {The Reactome Knowledgebase (https://reactome.org) provides molecular details of signal transduction, transport, DNA replication, metabolism, and other cellular processes as an ordered network of molecular transformations - an extended version of a classic metabolic map, in a single consistent data model. Reactome functions both as an archive of biological processes and as a tool for discovering unexpected functional relationships in data such as gene expression profiles or somatic mutation catalogues from tumor cells. To support the continued brisk growth in the size and complexity of Reactome, we have implemented a graph database, improved performance of data analysis tools, and designed new data structures and strategies to boost diagram viewer performance. To make our website more accessible to human users, we have improved pathway display and navigation by implementing interactive Enhanced High Level Diagrams (EHLDs) with an associated icon library, and subpathway highlighting and zooming, in a simplified and reorganized web site with adaptive design. To encourage re-use of our content, we have enabled export of pathway diagrams as 'PowerPoint' files.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/Fabregat et al. - 2018 - The Reactome Pathway Knowledgebase.pdf},
  journal = {Nucleic Acids Research},
  keywords = {Database,HPL comprehensive exam,Knowledgebase,Pathway},
  number = {D1}
}

@article{falkenhainer1986StructureMappingEngine,
  title = {1986 - {{The Structure}}-{{Mapping Engine}}},
  author = {Falkenhainer, Brian and Forbus, Kenneth D and Gentner, Dedre},
  pages = {6},
  file = {/home/harrisonpl/Zotero/storage/HPENS34G/AAAI86-045.pdf},
  language = {en}
}

@article{farahmandCausalInferenceEngine2019,
  ids = {Farahmand2019},
  title = {Causal {{Inference Engine}}: A Platform for Directional Gene Set Enrichment Analysis and Inference of Active Transcriptional Regulators},
  shorttitle = {Causal {{Inference Engine}}},
  author = {Farahmand, Saman and O'Connor, Corey and Macoska, Jill A. and Zarringhalam, Kourosh},
  year = {2019},
  month = dec,
  volume = {47},
  pages = {11563--11573},
  issn = {13624962},
  doi = {10.1093/nar/gkz1046},
  abstract = {Inference of active regulatory mechanisms underlying specific molecular and environmental perturbations is essential for understanding cellular response. The success of inference algorithms relies on the quality and coverage of the underlying network of regulator-gene interactions. Several commercial platforms provide large and manually curated regulatory networks and functionality to perform inference on these networks. Adaptation of such platforms for open-source academic applications has been hindered by the lack of availability of accurate, high-coverage networks of regulatory interactions and integration of efficient causal inference algorithms. In this work, we present CIE, an integrated platform for causal inference of active regulatory mechanisms form differential gene expression data. Using a regularized Gaussian Graphical Model, we construct a transcriptional regulatory network by integrating publicly available ChIP-seq experiments with gene-expression data from tissue-specific RNA-seq experiments. Our GGM approach identifies high confidence transcription factor (TF)-gene interactions and annotates the interactions with information on mode of regulation (activation vs. repression). Benchmarks against manually curated databases of TF-gene interactions show that our method can accurately detect mode of regulation. We demonstrate the ability of our platform to identify active transcriptional regulators by using controlled in vitro overexpression and stem-cell differentiation studies and utilize our method to investigate transcriptional mechanisms of fibroblast phenotypic plasticity.},
  file = {/home/harrisonpl/Documents/PDFs/Farahmand et al. - 2019 - Causal Inference Engine a platform for directional gene set enrichment analysis and inference of active transc.pdf},
  journal = {Nucleic acids research},
  keywords = {Causal inference,Favorite,Gene expression,GSEA,HPL comprehensive exam,Pathway enrichment,Reactome},
  language = {en},
  mendeley-tags = {Causal inference,GSEA,Gene expression,HPL comprehensive exam,Pathway enrichment,Reactome},
  number = {22}
}

@article{fernandesInsightsSurveybasedAnalysis2019,
  ids = {Fernandes2019},
  title = {Insights from a Survey-Based Analysis of the Academic Job Market},
  author = {Fernandes, Jason D and Sarabipour, Sarvenaz and Smith, Christopher T and Niemi, Natalie M and Jadavji, Nafisa M and Kozik, Ariangela J and Holehouse, Alex S and Pejaver, Vikas and Symmons, Orsolya and Filho, Alexandre W Bisson and Haage, Amanda},
  year = {2019},
  pages = {796466},
  doi = {10.1101/796466},
  abstract = {Many postdoctoral fellows in the STEM fields enter the academic job market with little knowledge of the process and expectations, and without any means to assess their qualifications relative to the general applicant pool. Demystifying this process is critical, as there is little information publicly available. In this work, we provide insight into the process of academic job searches by gathering data to establish background metrics for typical faculty job applicants, and further correlate these metrics with job search outcomes. We analyzed 317 responses to an anonymous survey for faculty job applicants from the May 2018 - May 2019 market cycle. Responses were about evenly split by gender, largely North American-centric and life science focused, and highly successful with 58\% of applicants receiving at least one offer. Traditional metrics (funding, publications, etc.) of a positive research track record above a certain threshold of qualifications were unable to completely differentiate applicants that did and did not receive a job offer. Our findings suggest that there is no single clear path to a faculty job offer and that perhaps criteria not captured by our survey may also influence landing a faculty position above a certain threshold of qualification. Furthermore, our survey did capture applicants perception of the faculty job application process as unnecessarily stressful, time-consuming, and largely lacking in feedback, irrespective of a successful outcome. We hope that this study will provide an avenue for better data-driven decision making by the applicants and search committees, better evidence-based mentorship practices by principal investigators, and improved hiring practices by institutions.},
  file = {/home/harrisonpl/Documents/PDFs/Fernandes et al. - 2019 - Insights from a survey-based analysis of the academic job market.pdf},
  journal = {bioRxiv}
}

@article{FiehnLabDescriptors,
  title = {Fiehn {{Lab}} - {{Descriptors}}},
  keywords = {\#nosource}
}

@article{filippatosEvacetrapibCardiovascularOutcomes2017,
  ids = {Filippatos2017},
  title = {Evacetrapib and Cardiovascular Outcomes: {{Reasons}} for Lack of Efficacy},
  shorttitle = {Evacetrapib and Cardiovascular Outcomes},
  booktitle = {Journal of Thoracic Disease},
  author = {Filippatos, Theodosios D. and Elisaf, Moses S.},
  year = {2017},
  volume = {9},
  issn = {20776624},
  doi = {10.21037/jtd.2017.07.75},
  abstract = {Cholesteryl ester transfer protein (CETP) inhibitors are a drug class targeting the enzyme CETP (1). The first CETP inhibitor, torcetrapib, was discontinued because of increased cardiovascular events that were attributed to off-target adverse effects (increased aldosterone and blood pressure) (2).},
  file = {/home/harrisonpl/Documents/PDFs/Filippatos, Elisaf - 2017 - Evacetrapib and cardiovascular outcomes.pdf},
  journal = {Journal of Thoracic Disease},
  keywords = {cardiovascular disease,mechanism},
  mendeley-tags = {cardiovascular disease,mechanism}
}

@article{foggiaGraphMatchingLearning2014,
  title = {Graph Matching and Learning in Pattern Recognition in the Last 10 Years},
  author = {Foggia, Pasquale and Percannella, Gennaro and Vento, Mario},
  year = {2014},
  month = apr,
  volume = {28},
  issn = {02180014},
  doi = {10.1142/S0218001414500013},
  abstract = {In this paper, we examine the main advances registered in the last ten years in Pattern Recognition methodologies based on graph matching and related techniques, analyzing more than 180 papers; the aim is to provide a systematic framework presenting the recent history and the current developments. This is made by introducing a categorization of graph-based techniques and reporting, for each class, the main contributions and the most outstanding research results. \textcopyright{} 2014 World Scientific Publishing Company.},
  journal = {International Journal of Pattern Recognition and Artificial Intelligence},
  keywords = {\#nosource,graph and tree search strategies,graph clustering,graph embeddings,graph kernels,graph learning,graph matching,Structural pattern recognition},
  number = {1}
}

@article{fraserValueCompleteMicrobial2002,
  ids = {Fraser2002},
  title = {The Value of Complete Microbial Genome Sequencing (You Get What You Pay For)},
  author = {Fraser, Claire M. and Eisen, Jonathan A. and Nelson, Karen E. and Paulsen, Ian T. and Salzberg, Steven L.},
  year = {2002},
  volume = {184},
  pages = {6403--6405},
  issn = {00219193},
  doi = {10.1128/JB.184.23.6403-6405.2002},
  file = {/home/harrisonpl/Documents/PDFs/Fraser et al. - 2002 - The value of complete microbial genome sequencing (you get what you pay for).pdf},
  journal = {Journal of Bacteriology},
  number = {23},
  pmid = {12426324}
}

@article{frostVarianceadjustedMahalanobisVAM2020,
  ids = {Frost2020},
  title = {Variance-Adjusted {{Mahalanobis}} ({{VAM}}): A Fast and Accurate Method for Cell-Specific Gene Set Scoring},
  author = {Frost, H. Robert},
  year = {2020},
  file = {/home/harrisonpl/Documents/PDFs/Frost - 2020 - Variance-adjusted Mahalanobis (VAM).pdf},
  journal = {bioRxiv}
}

@article{fuGraphRelModelingText2019,
  ids = {Fu2019},
  title = {{{GraphRel}}: {{Modeling Text}} as {{Relational Graphs}} for {{Joint Entity}} and {{Relation Extraction}}},
  author = {Fu, Tsu-Jui and Li, Peng-Hsuan and Ma, Wei-Yun},
  year = {2019},
  pages = {1409--1418},
  doi = {10.18653/v1/p19-1136},
  abstract = {In this paper, we present GraphRel, an end-to-end relation extraction model which uses graph convolutional networks (GCNs) to jointly learn named entities and relations. In contrast to previous baselines, we consider the interaction between named entities and relations via a relation-weighted GCN to better extract relations. Linear and dependency structures are both used to extract both sequential and regional features of the text, and a complete word graph is further utilized to extract implicit features among all word pairs of the text. With the graph-based approach, the prediction for overlapping relations is substantially improved over previous sequential approaches. We evaluate GraphRel on two public datasets: NYT and WebNLG. Results show that GraphRel maintains high precision while increasing recall substantially. Also, GraphRel outperforms previous work by 3.2\% and 5.8\% (F1 score), achieving a new state-of-the-art for relation extraction.},
  file = {/home/harrisonpl/Documents/PDFs/Fu et al. - 2019 - GraphRel.pdf}
}

@article{furrerOGERHybridMultitype2019,
  title = {{{OGER}}++: Hybrid Multi-Type Entity Recognition},
  shorttitle = {{{OGER}}++},
  author = {Furrer, Lenz and Jancso, Anna and Colic, Nicola and Rinaldi, Fabio},
  year = {2019},
  month = jan,
  volume = {11},
  pages = {7},
  issn = {1758-2946},
  doi = {10.1186/s13321-018-0326-3},
  abstract = {We present a text-mining tool for recognizing biomedical entities in scientific literature. OGER++ is a hybrid system for named entity recognition and concept recognition (linking), which combines a dictionary-based annotator with a corpus-based disambiguation component. The annotator uses an efficient look-up strategy combined with a normalization method for matching spelling variants. The disambiguation classifier is implemented as a feed-forward neural network which acts as a postfilter to the previous step.},
  file = {/home/harrisonpl/Documents/PDFs/Furrer et al. - 2019 - OGER++.pdf},
  journal = {Journal of Cheminformatics},
  language = {en},
  number = {1}
}

@article{gaborSemEval2018TaskSemantic2018,
  ids = {Gabor2018},
  title = {{{SemEval}}-2018 {{Task}} 7: {{Semantic Relation Extraction}} and {{Classification}} in {{Scientific Papers}}},
  author = {G{\'a}bor, Kata and Buscaldi, Davide and Schumann, Anne-Kathrin and QasemiZadeh, Behrang and Zargayouna, Haifa and Charnois, Thierry},
  year = {2018},
  pages = {679--688},
  doi = {10.18653/v1/s18-1111},
  abstract = {This paper describes the first task on semantic relation extraction and classification in scientific paper abstracts at SemEval 2018. The challenge focuses on domain-specific semantic relations and includes three different sub-tasks. The subtasks were designed so as to compare and quantify the effect of different pre-processing steps on the relation classification results. We expect the task to be relevant for a broad range of researchers working on extracting specialized knowledge from domain corpora, for example but not limited to scientific or bio-medical information extraction. The task attracted a total of 32 participants, with 158 submissions across different scenarios .},
  file = {/home/harrisonpl/Documents/PDFs/Gábor et al. - 2018 - SemEval-2018 Task 7.pdf},
  keywords = {Competition,Natural language processing,Relation extraction},
  mendeley-tags = {Competition,Natural language processing,Relation extraction}
}

@article{gacitaEnhancerPromoterUsage2020,
  title = {Enhancer and {{Promoter Usage}} in the {{Normal}} and {{Failed Human Heart}}},
  author = {Gacita, Anthony M. and {Dellefave-Castillo}, Lisa and Page, Patrick G. T. and Barefield, David Y. and Wasserstrom, Andrew and Puckelwartz, Megan J. and Nobrega, Marcelo A. and McNally, Elizabeth M.},
  year = {2020},
  month = mar,
  pages = {2020.03.17.988790},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.03.17.988790},
  abstract = {{$<$}p{$>$}The failed heart is characterized by re-expression of a fetal gene program, which contributes to adaptation and maladaptation in heart failure. To define genomewide enhancer and promoter use in heart failure, Cap Analysis of Gene Expression (CAGE-seq) was applied to healthy and failed human left ventricles to define short RNAs associated with both promoters and enhancers. Integration of CAGE-seq data with RNA sequencing identified a combined \textasciitilde 17,000 promoters and \textasciitilde 1,500 enhancers active in healthy and failed human left ventricles. Comparing promoter usage between healthy and failed hearts highlighted promoter shifts which altered amino-terminal protein sequences. Comparing enhancer usage between healthy and failed hearts revealed a majority of differentially utilized heart failure enhancers were intronic and primarily localized within the first intron, identifying this position as a common feature associated with tissue-specific gene expression changes in the heart. This dataset defines the dynamic genomic regulatory landscape underlying heart failure and serves as an important resource for understanding genetic contributions to cardiac dysfunction.{$<$}/p{$>$}},
  chapter = {New Results; http://web.archive.org/web/20200321013506/https://www.biorxiv.org/content/10.1101/2020.03.17.988790v1},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  file = {/home/harrisonpl/Documents/PDFs/Gacita et al. - 2020 - Enhancer and Promoter Usage in the Normal and Failed Human Heart.pdf},
  journal = {bioRxiv},
  keywords = {David Port,Recommended},
  language = {en}
}

@article{gagnon-bartschUsingControlGenes2012,
  ids = {Gagnon-Bartsch2012},
  title = {Using Control Genes to Correct for Unwanted Variation in Microarray Data},
  author = {{Gagnon-Bartsch}, Johann A. and Speed, Terence P.},
  year = {2012},
  volume = {13},
  pages = {539--552},
  issn = {14654644},
  doi = {10.1093/biostatistics/kxr034},
  abstract = {Microarray expression studies suffer from the problem of batch effects and other unwanted variation. Many methods have been proposed to adjust microarray data to mitigate the problems of unwanted variation. Several of these methods rely on factor analysis to infer the unwanted variation from the data. A central problem with this approach is the difficulty in discerning the unwanted variation from the biological variation that is of interest to the researcher. We present a new method, intended for use in differential expression studies, that attempts to overcome this problem by restricting the factor analysis to negative control genes. Negative control genes are genes known a priori not to be differentially expressed with respect to the biological factor of interest. Variation in the expression levels of these genes can therefore be assumed to be unwanted variation. We name this method "Remove Unwanted Variation, 2-step" (RUV-2). We discuss various techniques for assessing the performance of an adjustment method and compare the performance of RUV-2 with that of other commonly used adjustment methods such as Combat and Surrogate Variable Analysis (SVA). We present several example studies, each concerning genes differentially expressed with respect to gender in the brain and find that RUV-2 performs as well or better than other methods. Finally, we discuss the possibility of adapting RUV-2 for use in studies not concerned with differential expression and conclude that there may be promise but substantial challenges remain. The Author 2011. Published by Oxford University Press. All rights reserved.},
  file = {/home/harrisonpl/Documents/PDFs/Gagnon-Bartsch, Speed - 2012 - Using control genes to correct for unwanted variation in microarray data.pdf},
  journal = {Biostatistics},
  keywords = {Batch effect,Control gene,Differential expression,Factor analysis,SVA,Unwanted variation},
  number = {3},
  pmid = {22101192}
}

@article{garnickHormonalTherapyManagement1997,
  title = {Hormonal Therapy in the Management of Prostate Cancer: {{From Huggins}} to the Present},
  shorttitle = {Hormonal Therapy in the Management of Prostate Cancer},
  author = {Garnick, Marc B.},
  year = {1997},
  month = mar,
  volume = {49},
  pages = {5--15},
  issn = {0090-4295},
  doi = {10.1016/S0090-4295(97)00163-5},
  abstract = {Hormonal therapy has been the mainstay of treatment for advanced forms of prostate cancer. Although early clinical studies suggested that major improvements and even cure could occur, later randomized prospective investigations showed that hormonal treatments were palliative rather than curative. Subsequent studies have suggested, but not conclusively proven, that earlier initiation of hormonal therapy for patients with early forms of metastatic disease may prolong disease-free and overall survival. The choice of hormonal agents, either alone as monotherapy or in combination with agents of differing mechanisms of action, suggests that further improvements in survival can be achieved with combinations of hormonal agents. Many studies have suggested that the use of hormonal therapy with localized forms of prostate cancer may provide pathologic benefit in decreasing the amount of tumor found at the time of radical prostatectomy or treated during radiation therapy. Although short-term outcomes looking at tumor ``downstaging'' have been positive, longer follow-up will be necessary before definitive conclusions can be made regarding the ultimate utility of preoperative or preradiation therapy hormonal treatments (neoadjuvant hormonal therapy).},
  file = {/home/harrisonpl/Documents/PDFs/Garnick - 1997 - Hormonal therapy in the management of prostate cancer.pdf;/home/harrisonpl/Zotero/storage/K6QB8YNJ/S0090429597001635.html},
  journal = {Urology},
  language = {en},
  number = {3, Supplement 1}
}

@article{garrabrantLogicalInduction2016,
  ids = {Garrabrant2016},
  title = {Logical {{Induction}}},
  author = {Garrabrant, Scott and {Benson-Tilsen}, Tsvi and Critch, Andrew and Soares, Nate and Taylor, Jessica},
  year = {2016},
  abstract = {We present a computable algorithm that assigns probabilities to every logical statement in a given formal language, and refines those probabilities over time. For instance, if the language is Peano arithmetic, it assigns probabilities to all arithmetical statements, including claims about the twin prime conjecture, the outputs of long-running computations, and its own probabilities. We show that our algorithm, an instance of what we call a logical inductor, satisfies a number of intuitive desiderata, including: (1) it learns to predict patterns of truth and falsehood in logical statements, often long before having the resources to evaluate the statements, so long as the patterns can be written down in polynomial time; (2) it learns to use appropriate statistical summaries to predict sequences of statements whose truth values appear pseudorandom; and (3) it learns to have accurate beliefs about its own current beliefs, in a manner that avoids the standard paradoxes of self-reference. For example, if a given computer program only ever produces outputs in a certain range, a logical inductor learns this fact in a timely manner; and if late digits in the decimal expansion of \$\textbackslash backslashpi\$ are difficult to predict, then a logical inductor learns to assign \$\textbackslash backslashapprox 10\textbackslash backslash\%\$ probability to "the \$n\$th digit of \$\textbackslash backslashpi\$ is a 7" for large \$n\$. Logical inductors also learn to trust their future beliefs more than their current beliefs, and their beliefs are coherent in the limit (whenever \$\textbackslash backslashphi \textbackslash backslashimplies \textbackslash backslashpsi\$, \$\textbackslash backslashmathbb\{P\}\_\textbackslash backslashinfty(\textbackslash backslashphi) \textbackslash backslashle \textbackslash backslashmathbb\{P\}\_\textbackslash backslashinfty(\textbackslash backslashpsi)\$, and so on); and logical inductors strictly dominate the universal semimeasure in the limit. These properties and many others all follow from a single logical induction criterion, which is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence \$\textbackslash backslashphi\$ is associated with a stock that is worth \$\textbackslash backslash\$\$1 per share if [...]},
  annotation = {\_eprint: 1609.03543},
  archivePrefix = {arXiv},
  arxivid = {1609.03543},
  eprint = {1609.03543},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Garrabrant et al. - 2016 - Logical Induction.pdf},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science,Mathematics - Logic,Mathematics - Probability}
}

@article{garrettHomeostasisInflammationIntestine2010,
  ids = {Garrett2010},
  title = {Homeostasis and {{Inflammation}} in the {{Intestine}}},
  author = {Garrett, Wendy S. and Gordon, Jeffrey I. and Glimcher, Laurie H.},
  year = {2010},
  volume = {140},
  pages = {859--870},
  issn = {00928674},
  doi = {10.1016/j.cell.2010.01.023},
  abstract = {The gut is home to our largest collection of microbes. The ability of the immune system to coevolve with the microbiota during postnatal life allows the host and microbiota to coexist in a mutually beneficial relationship. Failure to achieve or maintain equilibrium between a host and its microbiota has negative consequences for both intestinal and systemic health. In this Review, we consider the many cellular and molecular methods by which inflammatory responses are regulated to maintain intestinal homeostasis and the disease states that can ensue when this balance is lost. \textcopyright{} 2010 Elsevier Inc.},
  file = {/home/harrisonpl/Documents/PDFs/Garrett et al. - 2010 - Homeostasis and Inflammation in the Intestine.pdf},
  journal = {Cell},
  number = {6},
  pmid = {20303876}
}

@article{gasperinSemisupervisedAnaphoraResolution2006,
  ids = {Gasperin2006},
  title = {Semi-Supervised Anaphora Resolution in Biomedical Texts},
  author = {Gasperin, Caroline},
  year = {2006},
  pages = {96--103},
  doi = {10.3115/1654415.1654436},
  abstract = {Resolving anaphora is an important step in the identification of named entities such as genes and proteins in biomedical scientific articles. The goal of this work is to resolve associative and coreferential anaphoric expressions making use of the rich domain resources (such as databases and ontologies) available for the biomedical area, instead of annotated training data. The results are comparable to extant state-of-the-art supervised methods in the same domain. The system is integrated into an interactive tool designed to assist FlyBase curators by aiding the identification of the salient entities in a given paper as a first step in the aggregation of information about them.},
  file = {/home/harrisonpl/Documents/PDFs/Gasperin - 2006 - Semi-supervised anaphora resolution in biomedical texts.pdf},
  journal = {HLT-NAACL 2006 - BioNLP 2006: Linking Natural Language Processing and Biology: Towards Deeper Biological Literature Analysis, Proceedings of the Workshop},
  number = {June}
}

@article{gasthausImprovementsSequenceMemoizer2010,
  title = {Improvements to the Sequence Memoizer},
  author = {Gasthaus, Jan and Teh, Yee Whye},
  year = {2010},
  pages = {9},
  abstract = {The sequence memoizer is a model for sequence data with state-of-the-art performance on language modeling and compression. We propose a number of improvements to the model and inference algorithm, including an enlarged range of hyperparameters, a memory-efficient representation, and inference algorithms operating on the new representation. Our derivations are based on precise definitions of the various processes that will also allow us to provide an elementary proof of the "mysterious" coagulation and fragmentation properties used in the original paper on the sequence memoizer by Wood et al. (2009). We present some experimental results supporting our improvements.},
  journal = {Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010, NIPS 2010},
  keywords = {\#nosource}
}

@article{gentnerComputationalModelsAnalogy2011,
  ids = {Gentner2011},
  title = {Computational Models of Analogy},
  booktitle = {Wiley Interdisciplinary Reviews: {{Cognitive}} Science},
  author = {Gentner, Dedre and Forbus, Kenneth D.},
  year = {2011},
  month = may,
  volume = {2},
  issn = {19395078},
  doi = {10.1002/wcs.105},
  abstract = {Analogical mapping is a core process in human cognition. A number of valuable computational models of analogy have been created, capturing aspects of how people compare representations, retrieve potential analogs from memory, and learn from the results. In the past 25 years, this area has progressed rapidly, fueled by strong collaboration between psychologists and Artificial Intelligence (AI) scientists, with contributions from linguists and philosophers as well. There is now considerable consensus regarding the constraints governing the mapping process. However, computational models still differ in their focus, with some aimed at capturing the range of analogical phenomena at the cognitive level and others aimed at modeling how analogical processes might be implemented in neural systems. Some recent work has focused on modeling interactions between analogy and other processes, and on modeling analogy as a part of larger cognitive systems. \textcopyright{} 2010 John Wiley \& Sons, Ltd.},
  file = {/home/harrisonpl/Documents/PDFs/Gentner, Forbus - 2011 - Computational models of analogy.pdf},
  journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
  keywords = {Analogy,Favorite,HPL comprehensive exam,Review},
  mendeley-tags = {Analogy,HPL comprehensive exam,Review}
}

@article{ghisleniCorrelationFineneedleAspiration2006,
  title = {Correlation between Fine-Needle Aspiration Cytology and Histopathology in the Evaluation of Cutaneous and Subcutaneous Masses from Dogs and Cats},
  author = {Ghisleni, G. and Roccabianca, P. and Ceruti, R. and Stefanello, D. and Bertazzolo, W. and Bonfanti, U. and Caniatti, M.},
  year = {2006},
  volume = {35},
  pages = {24--30},
  issn = {1939-165X},
  doi = {10.1111/j.1939-165X.2006.tb00084.x},
  abstract = {Background: Fine-needle aspiration cytology (FNAC) is commonly used as a diagnostic procedure to evaluate superficial and deep masses in animals. However, few studies have addressed the accuracy of FNAC in the evaluation of cutaneous and subcutaneous masses in a clinical setting. Objective: The purpose of this study was to compare the accuracy of FNAC as compared with histopathology in the diagnosis of cutaneous and subcutaneous masses from dogs and cats. Methods: Cytologic and histopathologic specimens obtained between 1999 and 2003 from 292 palpable cutaneous and subcutaneous masses obtained from 242 dogs and 50 cats were retrospectively evaluated. Cytologic samples were obtained by FNA and histopathologic samples were collected by surgical biopsy or at necropsy. Concordance was determined and the accuracy of FNAC for the diagnosis of neoplasia was determined using histopathology as the gold standard. Results: Of 292 specimens, 49 (from 44 dogs and 5 cats) were excluded due to poor cellularity of the cytologic specimen (retrieval rate 83.2\%, n = 243). A cytologic diagnosis of neoplasia was obtained in 176 cases (175 true positives and 1 false positive compared with histopathology). Sixty-seven cytology samples were classified as non-neoplastic (46 true negatives, 21 false negatives compared with histopathology). Overall, the cytologic diagnosis was in agreement with the histopathologic diagnosis in 90.9\% (221/243) of cases. For diagnosing neoplasia, cytology had a sensitivity of 89.3\%, a specificity of 97.9\%, a positive predictive value of 99.4\%, and a negative predictive value of 68.7\%. Conclusions: The results of this study confirmed FNAC as a reliable and useful diagnostic procedure for the evaluation of palpable cutaneous and subcutaneous lesions in small animal practice.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1939-165X.2006.tb00084.x},
  file = {/home/harrisonpl/Documents/PDFs/Ghisleni et al. - 2006 - Correlation between fine-needle aspiration cytology and histopathology in the.pdf},
  journal = {Veterinary Clinical Pathology},
  keywords = {Cat,cytology,dog,fine-needle aspiration,skin,tumor},
  language = {en},
  number = {1}
}

@article{gianfrancescoPotentialBiasesMachine2018,
  ids = {Gianfrancesco2018},
  title = {Potential {{Biases}} in {{Machine Learning Algorithms Using Electronic Health Record Data}}},
  author = {Gianfrancesco, Milena A. and Tamang, Suzanne and Yazdany, Jinoos and Schmajuk, Gabriela},
  year = {2018},
  volume = {178},
  pages = {1544--1547},
  issn = {21686106},
  doi = {10.1001/jamainternmed.2018.3763},
  abstract = {A promise of machine learning in health care is the avoidance of biases in diagnosis and treatment; a computer algorithm could objectively synthesize and interpret the data in the medical record. Integration of machine learning with clinical decision support tools, such as computerized alerts or diagnostic support, may offer physicians and others who provide health care targeted and timely information that can improve clinical decisions. Machine learning algorithms, however, may also be subject to biases. The biases include those related to missing data and patients not identified by algorithms, sample size and underestimation, and misclassification and measurement error. There is concern that biases and deficiencies in the data used by machine learning algorithms may contribute to socioeconomic disparities in health care. This Special Communication outlines the potential biases that may be introduced into machine learning-based clinical decision support tools that use electronic health record data and proposes potential solutions to the problems of overreliance on automation, algorithms based on biased data, and algorithms that do not provide information that is clinically meaningful. Existing health care disparities should not be amplified by thoughtless or excessive reliance on machines..},
  file = {/home/harrisonpl/Documents/PDFs/Gianfrancesco et al. - 2018 - Potential Biases in Machine Learning Algorithms Using Electronic Health Record.pdf},
  journal = {JAMA Internal Medicine},
  number = {11}
}

@article{gil20YearCommunityRoadmap2019,
  title = {A 20-{{Year Community Roadmap}} for {{Artificial Intelligence Research}} in the {{US}}},
  author = {Gil, Yolanda and Selman, Bart},
  year = {2019},
  abstract = {Decades of research in artificial intelligence (AI) have produced formidable technologies that are providing immense benefit to industry, government, and society. AI systems can now translate across multiple languages, identify objects in images and video, streamline manufacturing processes, and control cars. The deployment of AI systems has not only created a trillion-dollar industry that is projected to quadruple in three years, but has also exposed the need to make AI systems fair, explainable, trustworthy, and secure. Future AI systems will rightfully be expected to reason effectively about the world in which they (and people) operate, handling complex tasks and responsibilities effectively and ethically, engaging in meaningful communication, and improving their awareness through experience. Achieving the full potential of AI technologies poses research challenges that require a radical transformation of the AI research enterprise, facilitated by significant and sustained investment. These are the major recommendations of a recent community effort coordinated by the Computing Community Consortium and the Association for the Advancement of Artificial Intelligence to formulate a Roadmap for AI research and development over the next two decades.},
  file = {/home/harrisonpl/Documents/PDFs/Gil, Selman - 2019 - A 20-Year Community Roadmap for Artificial Intelligence Research in the US.pdf}
}

@article{gilAutomatedHypothesisGeneration2014,
  ids = {Gil2014,Gil2014a,Gil2014b,Spangler2014a,gilAmplifyScientificDiscovery2014},
  title = {Automated Hypothesis Generation Based on Mining Scientific Literature},
  author = {Gil, Yolanda and Greaves, Mark and Hendler, James and Hirsh, Haym and Spangler, Scott and Wilkins, Angela D. and Bachman, Benjamin J. and Nagarajan, Meena and Dayaram, Tajhal and Haas, Peter and Regenbogen, Sam and Pickering, Curtis R. and Comer, Austin and Myers, Jeffrey N. and Stanoi, Ioana and Kato, Linda and Lelescu, Ana and Labrie, Jacques J. and Parikh, Neha and Lisewski, Andreas Martin and Donehower, Lawrence and Chen, Ying and Lichtarge, Olivier},
  year = {2014},
  month = aug,
  volume = {346},
  pages = {1877--1886},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, New York, USA}},
  issn = {10959203},
  doi = {10.1126/science.1259439},
  abstract = {Keeping up with the ever-expanding flow of data and publications is untenable and poses a fundamental bottleneck to scientific progress. Current search technologies typically find many relevant documents, but they do not extract and organize the information content of these documents or suggest new scientific hypotheses based on this organized content. We present an initial case study on KnIT, a prototype system that mines the information contained in the scientific literature, represents it explicitly in a queriable network, and then further reasons upon these data to generate novel and experimentally testable hypotheses. KnIT combines entity detection with neighbor-text feature analysis and with graph-based diffusion of information to identify potential new properties of entities that are strongly implied by existing relationships. We discuss a successful application of our approach that mines the published literature to identify new protein kinases that phosphorylate the protein tumor suppressor p53. Retrospective analysis demonstrates the accuracy of this approach and ongoing laboratory experiments suggest that kinases identified by our system may indeed phosphorylate p53. These results establish proof of principle for automated hypothesis generation and discovery based on text mining of the scientific literature. \textcopyright{} 2014 ACM.},
  file = {/home/harrisonpl/Documents/PDFs/Gil et al. - 2014 - Automated hypothesis generation based on mining scientific literature.pdf},
  isbn = {9781450329569},
  journal = {Science},
  keywords = {HPL comprehensive exam,hypothesis generation,scientific discovery,text mining},
  language = {en},
  mendeley-tags = {HPL comprehensive exam,hypothesis generation,scientific discovery,text mining},
  number = {6206},
  series = {{{KDD}} '14}
}

@article{gisteraaImmunologyAtherosclerosis2017,
  ids = {Gistera2017},
  title = {The Immunology of Atherosclerosis},
  booktitle = {Nature Reviews Nephrology},
  author = {Gister{\textbackslash}aa, Anton and Hansson, G{\"o}ran K.},
  year = {2017},
  volume = {13},
  issn = {1759507X},
  doi = {10.1038/nrneph.2017.51},
  abstract = {Cardiovascular disease is the leading cause of death worldwide, both in the general population and among patients with chronic kidney disease (CKD). In most cases, the underlying cause of the cardiovascular event is atherosclerosis-a chronic inflammatory disease. CKD accelerates atherosclerosis via augmentation of inflammation, perturbation of lipid metabolism, and other mechanisms. In the artery wall, subendothelial retention of plasma lipoproteins triggers monocyte-derived macrophages and T helper type 1 (T H 1) cells to form atherosclerotic plaques. Inflammation is initiated by innate immune reactions to modified lipoproteins and is perpetuated by T H 1 cells that react to autoantigens from the apolipoprotein B100 protein of LDL. Other T cells are also active in atherosclerotic lesions; regulatory T cells inhibit pathological inflammation, whereas T H 17 cells can promote plaque fibrosis. The slow build-up of atherosclerotic plaques is asymptomatic, but plaque rupture or endothelial erosion can induce thrombus formation, leading to myocardial infarction or ischaemic stroke. Targeting risk factors for atherosclerosis has reduced mortality, but a need exists for novel therapies to stabilize plaques and to treat arterial inflammation. Patients with CKD would likely benefit from such preventive measures.},
  file = {/home/harrisonpl/Documents/PDFs/Gisteraa, Hansson - 2017 - The immunology of atherosclerosis.pdf},
  journal = {Nature Reviews Nephrology},
  keywords = {cardiovascular disease},
  mendeley-tags = {cardiovascular disease}
}

@article{gkoutosOntologybasedCrossspeciesIntegration2012,
  ids = {Gkoutos2012},
  title = {Ontology-Based Cross-Species Integration and Analysis of {{Saccharomyces}} Cerevisiae Phenotypes},
  author = {Gkoutos, Georgios V. and Hoehndorf, Robert},
  year = {2012},
  volume = {3},
  pages = {S6},
  issn = {20411480},
  doi = {10.1186/2041-1480-3-S2-S6},
  abstract = {Ontologies are widely used in the biomedical community for annotation and integration of databases. Formal definitions can relate classes from different ontologies and thereby integrate data across different levels of granularity, domains and species. We have applied this methodology to the Ascomycete Phenotype Ontology (APO), enabling the reuse of various orthogonal ontologies and we have converted the phenotype associated data found in the SGD following our proposed patterns. We have integrated the resulting data in the cross-species phenotype network PhenomeNET, and we make both the cross-species integration of yeast phenotypes and a similarity-based comparison of yeast phenotypes across species available in the PhenomeBrowser. Furthermore, we utilize our definitions and the yeast phenotype annotations to suggest novel functional annotations of gene products in yeast.},
  file = {/home/harrisonpl/Documents/PDFs/Gkoutos, Hoehndorf - 2012 - Ontology-based cross-species integration and analysis of Saccharomyces.pdf},
  journal = {Journal of Biomedical Semantics},
  keywords = {ontology},
  mendeley-tags = {ontology},
  number = {2},
  pmid = {23046642}
}

@article{glimmHermiTOWLReasoner2014,
  title = {{{HermiT}}: {{An OWL}} 2 {{Reasoner}}},
  shorttitle = {{{HermiT}}},
  author = {Glimm, Birte and Horrocks, Ian and Motik, Boris and Stoilos, Giorgos and Wang, Zhe},
  year = {2014},
  month = oct,
  volume = {53},
  pages = {245--269},
  issn = {0168-7433, 1573-0670},
  doi = {10.1007/s10817-014-9305-1},
  abstract = {This system description paper introduces the OWL 2 reasoner HermiT. The reasoner is fully compliant with the OWL 2 Direct Semantics as standardised by the World Wide Web Consortium (W3C). HermiT is based on the hypertableau calculus, and it supports a wide range of standard and novel optimisations that improve the performance of reasoning on real-world ontologies. Apart from the standard OWL 2 reasoning task of entailment checking, HermiT supports several specialised reasoning services such as class and property classification, as well as a range of features outside the OWL 2 standard such as DL-safe rules, SPARQL queries, and description graphs. We discuss the system's architecture, and we present an overview of the techniques used to support the mentioned reasoning tasks. We further compare the performance of reasoning in HermiT with that of FaCT++ and Pellet\textemdash two other popular and widely used OWL 2 reasoners.},
  file = {/home/harrisonpl/Zotero/storage/693EFFFG/Glimm2014_Article_HermiTAnOWL2Reasoner.pdf},
  journal = {Journal of Automated Reasoning},
  language = {en},
  number = {3}
}

@article{gnerreHighqualityDraftAssemblies2011,
  ids = {Gnerre,Gnerre2011,Gnerre2011a},
  title = {High-Quality Draft Assemblies of Mammalian Genomes from Massively Parallel Sequence Data},
  author = {Gnerre, Sante and MacCallum, Iain and Przybylski, Dariusz and Ribeiro, Filipe J. and Burton, Joshua N. and Walker, Bruce J. and Sharpe, Ted and Hall, Giles and Shea, Terrance P. and Sykes, Sean and Berlin, Aaron M. and Aird, Daniel and Costello, Maura and Daza, Riza and Williams, Louise and Nicol, Robert and Gnirke, Andreas and Nusbaum, Chad and Lander, Eric S. and Jaffe, David B.},
  year = {2011},
  volume = {108},
  pages = {1513--1518},
  issn = {00278424},
  doi = {10.1073/pnas.1017351108},
  abstract = {Massively parallel DNA sequencing technologies are revolutionizing genomics by making it possible to generate billions of relatively short ({$\sim$}100-base) sequence reads at very low cost. Whereas such data can be readily used for a wide range of biomedical applications, it has proven difficult to use them to generate high-quality de novo genome assemblies of large, repeat-rich vertebrate genomes. To date, the genome assemblies generated from such data have fallen far short of those obtained with the older (but much more expensive) capillary-based sequencing approach. Here, we report the development of an algorithm for genome assembly, ALLPATHS-LG, and its application to massively parallel DNA sequence data from the human and mouse genomes, generated on the Illumina platform. The resulting draft genome assemblies have good accuracy, short-range contiguity, long-range connectivity, and coverage of the genome. In particular, the base accuracy is high ({$\geq$}99.95\%) and the scaffold sizes (N50 size = 11.5 Mb for human and 7.2 Mb for mouse) approach those obtained with capillary-based sequencing. The combination of improved sequencing technology and improved computational methods should now make it possible to increase dramatically the de novo sequencing of large genomes. The ALLPATHS-LG program is available at http://www.broadinstitute.org/science/programs/genome-biology/crd.},
  file = {/home/harrisonpl/Documents/PDFs/Gnerre et al. - 2011 - High-quality draft assemblies of mammalian genomes from massively parallel.pdf},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  number = {4},
  pmid = {21187386}
}

@article{goetzStateArtReview2003,
  title = {State of the Art Review the Unified Parkinson ' s Disease Rating Scale ( {{UPDRS}} ): {{Status}} and Recommendations},
  author = {Goetz, Dr. Christopher G.},
  year = {2003},
  volume = {18},
  pages = {738--750},
  journal = {Movement Disorders},
  keywords = {\#nosource,clinimetrics,disorder society,has the authority and,mds,of the movement,parkinson,rating scales,responsi-,s disease,s disease rating scale,the international executive council,unified parkinson,updrs},
  number = {7}
}

@article{goetzUnifiedParkinsonDisease2003,
  ids = {Goetz2003},
  title = {The {{Unified Parkinson}}'s {{Disease Rating Scale}} ({{UPDRS}}): {{Status}} and Recommendations},
  shorttitle = {The {{Unified Parkinson}}'s {{Disease Rating Scale}} ({{UPDR}}},
  author = {Goetz, Christopher C.},
  year = {2003},
  month = jul,
  volume = {18},
  pages = {738--750},
  issn = {08853185},
  doi = {10.1002/mds.10473},
  abstract = {The Movement Disorder Society Task Force for Rating Scales for Parkinson's Disease prepared a critique of the Unified Parkinson's Disease Rating Scale (UPDRS). Strengths of the UPDRS include its wide utilization, its application across the clinical spectrum of PD, its nearly comprehensive coverage of motor symptoms, and its clinimetric properties, including reliability and validity. Weaknesses include several ambiguities in the written text, inadequate instructions for raters, some metric flaws, and the absence of screening questions on several important non-motor aspects of PD. The Task Force recommends that the MDS sponsor the development of a new version of the UPDRS and encourage efforts to establish its clinimetric properties, especially addressing the need to define a Minimal Clinically Relevant Difference and a Minimal Clinically Relevant Incremental Difference, as well as testing its correlation with the current UPDRS. If developed, the new scale should be culturally unbiased and be tested in different racial, gender, and age-groups. Future goals should include the definition of UPDRS scores with confidence intervals that correlate with clinically pertinent designations, "minimal," "mild," "moderate," and "severe" PD. Whereas the presence of non-motor components of PD can be identified with screening questions, a new version of the UPDRS should include an official appendix that includes other, more detailed, and optionally used scales to determine severity of these impairments. \textcopyright{} 2003 Movement Disorder Society.},
  file = {/home/harrisonpl/Documents/PDFs/Goetz - 2003 - The Unified Parkinson's Disease Rating Scale (UPDRS).pdf},
  journal = {Movement Disorders},
  keywords = {Bias,Clinimetrics,Cross-Cultural Comparison,Humans,Neurologic Examination,Parkinson Disease,Parkinson's disease,Rating scales,Reproducibility of Results,Unified Parkinson's Disease Rating Scale (UPDRS)},
  language = {eng},
  mendeley-tags = {Bias,Cross-Cultural Comparison,Folder - BIOS 6611,Humans,Neurologic Examination,Parkinson Disease,Reproducibility of Results},
  number = {7},
  pmid = {12815652}
}

@article{gonenPredictingDrugtargetInteractions2012,
  ids = {Gonen2012},
  title = {Predicting Drug-Target Interactions from Chemical and Genomic Kernels Using {{Bayesian}} Matrix Factorization},
  author = {G{\"o}nen, Mehmet},
  year = {2012},
  volume = {28},
  pages = {2304--2310},
  issn = {13674803},
  doi = {10.1093/bioinformatics/bts360},
  abstract = {Motivation: Identifying interactions between drug compounds and target proteins has a great practical importance in the drug discovery process for known diseases. Existing databases contain very few experimentally validated drug-target interactions and formulating successful computational methods for predicting interactions remains challenging. Results: In this study, we consider four different drug-target interaction networks from humans involving enzymes, ion channels, G-protein-coupled receptors and nuclear receptors. We then propose a novel Bayesian formulation that combines dimensionality reduction, matrix factorization and binary classification for predicting drug-target interaction networks using only chemical similarity between drug compounds and genomic similarity between target proteins. The novelty of our approach comes from the joint Bayesian formulation of projecting drug compounds and target proteins into a unified subspace using the similarities and estimating the interaction network in that subspace. We propose using a variational approximation in order to obtain an efficient inference scheme and give its detailed derivations. Finally, we demonstrate the performance of our proposed method in three different scenarios: (i) exploratory data analysis using low-dimensional projections, (ii) predicting interactions for the out-of-sample drug compounds and (iii) predicting unknown interactions of the given network. \textcopyright{} The Author 2012. Published by Oxford University Press. All rights reserved.},
  file = {/home/harrisonpl/Documents/PDFs/Gönen - 2012 - Predicting drug-target interactions from chemical and genomic kernels using.pdf},
  journal = {Bioinformatics},
  number = {18}
}

@article{gorijalaFunctionalBiologicalPaths2020,
  title = {Functional Biological Paths Altered in {{Alzheimer}} ' s Disease : From Genes to Bile Acids {{Introduction}} :},
  author = {Gorijala, Priyanka and Nho, Kwangsik and Risacher, Shannon L and {Kaddurah-daouk}, Rima and Andrew, J},
  year = {2020},
  keywords = {\#nosource}
}

@article{gorlatovaProteinCharacterizationCandidate2011,
  ids = {Gorlatova2011},
  title = {Protein Characterization of a Candidate Mechanism {{SNP}} for {{Crohn}}'s Disease: {{The}} Macrophage Stimulating Protein {{R689C}} Substitution},
  author = {Gorlatova, Natalia and Chao, Kinlin and Pal, Lipika R. and Araj, Rawan Hanna and Galkin, Andrey and Turko, Illarion and Moult, John and Herzberg, Osnat},
  year = {2011},
  volume = {6},
  issn = {19326203},
  doi = {10.1371/journal.pone.0027269},
  abstract = {High throughput genome wide associations studies (GWAS) are now identifying a large number of genome loci related to risk of common human disease. Each such locus presents a challenge in identifying the relevant underlying mechanism. Here we report the experimental characterization of a proposed causal single nucleotide polymorphism (SNP) in a locus related to risk of Crohn's disease and ulcerative colitis. The SNP lies in the MST1 gene encoding Macrophage Stimulating Protein (MSP), and results in an R689C amino acid substitution within the {$\beta$}-chain of MSP (MSP{$\beta$}). MSP binding to the RON receptor tyrosine kinase activates signaling pathways involved in the inflammatory response. We have purified wild-type and mutant MSP{$\beta$} proteins and compared biochemical and biophysical properties that might impact the MSP/RON signaling pathway. Surface plasmon resonance (SPR) binding studies showed that MSP{$\beta$} R689C affinity to RON is approximately 10-fold lower than that of the wild-type MSP{$\beta$} and differential scanning fluorimetry (DSF) showed that the thermal stability of the mutant MSP{$\beta$} was slightly lower than that of wild-type MSP{$\beta$}, by 1.6 K. The substitution was found not to impair the specific Arg483-Val484 peptide bond cleavage by matriptase-1, required for MSP activation, and mass spectrometry of tryptic fragments of the mutated protein showed that the free thiol introduced by the R689C mutation did not form an aberrant disulfide bond. Together, the studies indicate that the missense SNP impairs MSP function by reducing its affinity to RON and perhaps through a secondary effect on in vivo concentration arising from reduced thermodynamic stability, resulting in down-regulation of the MSP/RON signaling pathway. \textcopyright{} 2011 Gorlatova et al.},
  file = {/home/harrisonpl/Documents/PDFs/Gorlatova et al. - 2011 - Protein characterization of a candidate mechanism SNP for Crohn's disease.pdf},
  journal = {PLoS ONE},
  keywords = {Lawrence Hunter,Recommendation},
  mendeley-tags = {Lawrence Hunter,Recommendation},
  number = {11}
}

@article{gouldCholesterolReductionYields1998,
  ids = {Gould1998a},
  title = {Cholesterol Reduction Yields Clinical Benefit: {{Impact}} of Statin Trials},
  shorttitle = {Cholesterol {{Reduction Yields Clinical Benefit}}},
  author = {Gould, A. Lawrence and Rossouw, Jacques E. and Santanello, Nancy C. and Heyse, Joseph F. and Furberg, Curt D.},
  year = {1998},
  volume = {97},
  pages = {946--952},
  issn = {00097322},
  doi = {10.1161/01.CIR.97.10.946},
  abstract = {Background - We determined the effect of incorporating the results of eight recently published trials of Hmg CoA reductase inhibitors ('statins') on the conclusions from our previously published meta-analysis regarding the clinical benefit of cholesterol lowering. Methods and Results - We used the same analytic approach as in our previous investigation, separating the specific effects of cholesterol lowering from the effects attributable to the different types of intervention studied. The reductions in coronary heart disease (CHD) and total mortality risk observed for the statins fell near the predictions from our earlier meta-analysis. Including the statin trial findings into the calculations led to a prediction that for every 10 percentage points of cholesterol lowering, CHD mortality risk would be reduced by 15\% (P\textbackslash textless.001), and total mortality risk would be reduced by 11\% (P\textbackslash textless.001), as opposed to the values of 13\% and 10\%, respectively, reported previously. Cholesterol lowering in general and by the statins in particular does not increase non-CHD mortality risk. Conclusions - Adding the results from the statin trials confirmed our original conclusion that lowering cholesterol is clinically beneficial. The relationships (slope) between cholesterol lowering and reduction in CHD and total mortality risk became stronger, and the standard error of the estimated slopes decreased by about half. Use of statins does not increase non-CHD mortality risk. The effect of the statins on CHD and total mortality risk can be explained by their lipid- lowering ability and appears to be directly proportional to the degree to which they lower lipids.},
  file = {/home/harrisonpl/Documents/PDFs/Gould et al. - 1998 - Cholesterol reduction yields clinical benefit.pdf},
  journal = {Circulation},
  keywords = {Cholesterol,Meta-analysis,Mortality},
  number = {10},
  pmid = {9529261}
}

@article{gouldImpactStatinTrials1998,
  ids = {Gould1998,Gould1998b},
  title = {Impact of {{Statin Trials}}},
  author = {Gould, A L and Rossouw, J E and Santanello, N C and Heyse, J F and Furberg, C D},
  year = {1998},
  volume = {97},
  pages = {946--952},
  file = {/home/harrisonpl/Documents/PDFs/Gould et al. - 1998 - Impact of Statin Trials.pdf},
  journal = {Circulation},
  keywords = {mortality,reductase inhibitors,statins,trials of hmg coa}
}

@inproceedings{gracaRobustSimulationsTuring2005,
  ids = {Graca2005},
  title = {Robust Simulations of {{Turing}} Machines with Analytic Maps and Flows},
  booktitle = {Lecture {{Notes}} in {{Computer Science}}},
  author = {Gra{\c c}a, Daniel S. and Campagnolo, Manuel L. and Buescu, Jorge},
  year = {2005},
  volume = {3526},
  pages = {169--179},
  publisher = {{Springer Verlag}},
  issn = {03029743},
  doi = {10.1007/11494645_21},
  abstract = {In this paper, we show that closed-form analytic maps and flows can simulate Turing machines in an error-robust manner. The maps and ODEs defining the flows are explicitly obtained and the simulation is performed in real time. \textcopyright{} Springer-Verlag Berlin Heidelberg 2005.},
  file = {/home/harrisonpl/Documents/PDFs/Graça et al. - 2005 - Robust simulations of Turing machines with analytic maps and flows.pdf}
}

@article{greenburyHyperTraPSInferringProbabilistic2020,
  ids = {Greenbury2020,Greenbury2020a,Greenbury2020b},
  title = {{{HyperTraPS}}: {{Inferring Probabilistic Patterns}} of {{Trait Acquisition}} in {{Evolutionary}} and {{Disease Progression Pathways}}},
  shorttitle = {{{HyperTraPS}}},
  author = {Greenbury, Sam F. and Barahona, Mauricio and Johnston, Iain G.},
  year = {2020},
  month = jan,
  volume = {10},
  pages = {39--51.e10},
  publisher = {{Elsevier Inc.}},
  issn = {24054720},
  doi = {10.1016/j.cels.2019.10.009},
  abstract = {The explosion of data throughout the biomedical sciences provides unprecedented opportunities to learn about the dynamics of evolution and disease progression, but harnessing these large and diverse datasets remains challenging. Here, we describe a highly generalizable statistical platform to infer the dynamic pathways by which many, potentially interacting, traits are acquired or lost over time. We use HyperTraPS (hypercubic transition path sampling) to efficiently learn progression pathways from cross-sectional, longitudinal, or phylogenetically linked data, readily distinguishing multiple competing pathways, and identifying the most parsimonious mechanisms underlying given observations. This Bayesian approach allows inclusion of prior knowledge, quantifies uncertainty in pathway structure, and allows predictions, such as which symptom a patient will acquire next. We provide visualization tools for intuitive assessment of multiple, variable pathways. We apply the method to ovarian cancer progression and the evolution of multidrug resistance in tuberculosis, demonstrating its power to reveal previously undetected dynamic pathways. In the era of big data, computational approaches to learn how diseases develop and organisms evolve are central to advance biological and medical understanding. Greenbury et al. present HyperTraPS, an approach to harness large, diverse datasets to learn how features like evolutionary traits or disease symptoms change and interact through time. They reveal diverse pathways of cancer progression and routes to multidrug resistance in tuberculosis and show that the approach can be applied to a wide variety of biomedical questions.},
  annotation = {\_eprint: 1912.00762},
  archivePrefix = {arXiv},
  arxivid = {1912.00762},
  eprint = {1912.00762},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Greenbury et al. - 2020 - HyperTraPS.pdf},
  journal = {Cell Systems},
  keywords = {Bayesian inference,cancer progression models,comp-exam,disease pathways,drug resistance,HPL comprehensive exam,HyperTraPS,phylogenetic character mapping,precision healthcare,trait evolution},
  language = {en},
  mendeley-tags = {Bayesian inference,HyperTraPS,cancer progression models,comp-exam,disease pathways,drug resistance,phylogenetic character mapping,precision healthcare,trait evolution},
  number = {1}
}

@article{gremseBRENDATissueOntology2011,
  ids = {Gremse2011},
  title = {The {{BRENDA Tissue Ontology}} ({{BTO}}): {{The}} First All-Integrating Ontology of All Organisms for Enzyme Sources},
  shorttitle = {The {{BRENDA Tissue Ontology}} ({{BTO}})},
  author = {Gremse, Marion and Chang, Antje and Schomburg, Ida and Grote, Andreas and Scheer, Maurice and Ebeling, Christian and Schomburg, Dietmar},
  year = {2011},
  volume = {39},
  pages = {D507--D513},
  issn = {03051048},
  doi = {10.1093/nar/gkq968},
  abstract = {BTO, the BRENDA Tissue Ontology (http://www.BTO.brenda-enzymes.org) represents a comprehensive structured encyclopedia of tissue terms. The project started in 2003 to create a connection between the enzyme data collection of the BRENDA enzyme database and a structured network of source tissues and cell types. Currently, BTO contains more than 4600 different anatomical structures, tissues, cell types and cell lines, classified under generic categories corresponding to the rules and formats of the Gene Ontology Consortium and organized as a directed acyclic graph (DAG). Most of the terms are endowed with comments on their derivation or definitions. The content of the ontology is constantly curated with{\~ }1000 new terms each year. Four different types of relationships between the terms are implemented. A versatile web interface with several search and navigation functionalities allows convenient online access to the BTO and to the enzymes isolated from the tissues. Important areas of applications of the BTO terms are the detection of enzymes in tissues and the provision of a solid basis for text-mining approaches in this field. It is widely used by lab scientists, curators of genomic and biochemical databases and bioinformaticians. The BTO is freely available at http://www.obofoundry.org. \textcopyright{} The Author(s) 2010.},
  file = {/home/harrisonpl/Documents/PDFs/Gremse et al. - 2011 - The BRENDA Tissue Ontology (BTO).pdf},
  journal = {Nucleic Acids Research},
  keywords = {ontology},
  mendeley-tags = {ontology},
  number = {SUPPL. 1},
  pmid = {21030441}
}

@article{guoLUBMBenchmarkOWL2005,
  ids = {Guo2005},
  title = {{{LUBM}}: {{A}} Benchmark for {{OWL}} Knowledge Base Systems},
  shorttitle = {{{LUBM}}},
  author = {Guo, Yuanbo and Pan, Zhengxiang and Heflin, Jeff},
  year = {2005},
  volume = {3},
  pages = {158--182},
  issn = {15708268},
  doi = {10.1016/j.websem.2005.06.005},
  abstract = {We describe our method for benchmarking Semantic Web knowledge base systems with respect to use in large OWL applications. We present the Lehigh University Benchmark (LUBM) as an example of how to design such benchmarks. The LUBM features an ontology for the university domain, synthetic OWL data scalable to an arbitrary size, 14 extensional queries representing a variety of properties, and several performance metrics. The LUBM can be used to evaluate systems with different reasoning capabilities and storage mechanisms. We demonstrate this with an evaluation of two memory-based systems and two systems with persistent storage. \textcopyright{} 2005 Elsevier B.V. All rights reserved.},
  file = {/home/harrisonpl/Documents/PDFs/Guo et al. - 2005 - LUBM.pdf},
  journal = {Web Semantics},
  keywords = {Evaluation,Knowledge base system,Lehigh University Benchmark,ontology,Semantic Web},
  mendeley-tags = {ontology},
  number = {2-3},
  series = {Selcted {{Papers}} from the {{International Semantic Web Conference}}, 2004}
}

@article{guthrieBriefPrimerGenomic2017,
  ids = {Guthrie2017},
  title = {A Brief Primer on Genomic Epidemiology: Lessons Learned from {{Mycobacterium}} Tuberculosis},
  shorttitle = {A Brief Primer on Genomic Epidemiology},
  author = {Guthrie, Jennifer L. and Gardy, Jennifer L.},
  year = {2017},
  volume = {1388},
  pages = {59--77},
  issn = {17496632},
  doi = {10.1111/nyas.13273},
  abstract = {Genomics is now firmly established as a technique for the investigation and reconstruction of communicable disease outbreaks, with many genomic epidemiology studies focusing on revealing transmission routes of Mycobacterium tuberculosis. In this primer, we introduce the basic techniques underlying transmission inference from genomic data, using illustrative examples from M. tuberculosis and other pathogens routinely sequenced by public health agencies. We describe the laboratory and epidemiological scenarios under which genomics may or may not be used, provide an introduction to sequencing technologies and bioinformatics approaches to identifying transmission-informative variation and resistance-associated mutations, and discuss how variation must be considered in the light of available clinical and epidemiological information to infer transmission.},
  file = {/home/harrisonpl/Documents/PDFs/Guthrie, Gardy - 2017 - A brief primer on genomic epidemiology.pdf},
  journal = {Annals of the New York Academy of Sciences},
  keywords = {genomics,resistance,transmission,tuberculosis},
  number = {1}
}

@article{haarslevHighPerformanceReasoning2001,
  title = {High Performance Reasoning with Very Large Knowledge Bases: {{A}} Practical Case Study},
  author = {Haarslev, Volker and M{\"o}ller, Ralf},
  year = {2001},
  pages = {161--166},
  issn = {10450823},
  abstract = {We present an empirical analysis of optimization techniques devised to speed up the so-called TBox classification supported by description logic systems which have to deal with very large knowledge bases (e.g. containing more than 100,000 concept introduction axioms). These techniques are integrated into the RACE architecture which implements a TBox and ABox reasoner for the description logic ALCNHR+. The described techniques consist of adaptions of previously known as well as new optimization techniques for efficiently coping with these kinds of very large knowledge bases. The empirical results presented in this paper are based on experiences with an ontology for the Unified Medical Language System and demonstrate a considerable runtime improvement. They also indicate that appropriate description logic systems based on sound and complete algorithms can be particularly useful for very large knowledge bases.},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  keywords = {\#nosource,High Performance,ontology},
  number = {Dl}
}

@article{haarslevRacerProKnowledgeRepresentation2012,
  title = {The {{RacerPro}} Knowledge Representation and Reasoning System},
  author = {Haarslev, Volker and Hidde, Kay and M{\"o}ller, Ralf and Wessel, Michael},
  year = {2012},
  volume = {3},
  pages = {267--277},
  issn = {15700844},
  doi = {10.3233/SW-2011-0032},
  abstract = {RacerPro is a software system for building applications based on ontologies. The backbone of RacerPro is a description logic reasoner. It provides inference services for terminological knowledge as well as for representations of knowledge about individuals. Based on new optimization techniques and techniques that have been developed in the research field of description logics throughout the years, a mature architecture for typical-case reasoning tasks is provided. The system has been used in hundreds of research projects and industrial contexts throughout the last twelve years. W3C standards as well as detailed feedback reports from numerous users have influenced the design of the system architecture in general, and have also shaped the RacerPro knowledge representation and interface languages. With its query and rule languages, RacerPro goes well beyond standard inference services provided by other OWL reasoners. \textcopyright{} 2010 - IOS Press and the authors. All rights reserved.},
  journal = {Semantic Web},
  keywords = {\#nosource,Abox Abduction,Deduction over Tboxes and Aboxes,Description Logic Reasoning Systems,Expressive Ontology-based Query Answering,ontology,Ontology Reasoning Systems},
  number = {3}
}

@article{haendelBornReusableBest2019,
  title = {Born Reusable : {{Best}} for Translational Impact},
  author = {Haendel, Melissa},
  year = {2019},
  file = {/home/harrisonpl/Documents/PDFs/Haendel - 2019 - Born reusable.pdf}
}

@article{hanahanHallmarksCancerNext2011,
  title = {Hallmarks of {{Cancer}}: {{The Next Generation}}},
  shorttitle = {Hallmarks of {{Cancer}}},
  author = {Hanahan, Douglas and Weinberg, Robert A.},
  year = {2011},
  month = mar,
  volume = {144},
  pages = {646--674},
  issn = {0092-8674},
  doi = {10.1016/j.cell.2011.02.013},
  abstract = {The hallmarks of cancer comprise six biological capabilities acquired during the multistep development of human tumors. The hallmarks constitute an organizing principle for rationalizing the complexities of neoplastic disease. They include sustaining proliferative signaling, evading growth suppressors, resisting cell death, enabling replicative immortality, inducing angiogenesis, and activating invasion and metastasis. Underlying these hallmarks are genome instability, which generates the genetic diversity that expedites their acquisition, and inflammation, which fosters multiple hallmark functions. Conceptual progress in the last decade has added two emerging hallmarks of potential generality to this list\textemdash reprogramming of energy metabolism and evading immune destruction. In addition to cancer cells, tumors exhibit another dimension of complexity: they contain a repertoire of recruited, ostensibly normal cells that contribute to the acquisition of hallmark traits by creating the ``tumor microenvironment.'' Recognition of the widespread applicability of these concepts will increasingly affect the development of new means to treat human cancer.},
  file = {/home/harrisonpl/Documents/PDFs/Hanahan, Weinberg - 2011 - Hallmarks of Cancer.pdf;/home/harrisonpl/Zotero/storage/74GKLC4C/S0092867411001279.html},
  journal = {Cell},
  language = {en},
  number = {5}
}

@article{handClassifierTechnologyIllusion2006,
  ids = {Hand2006},
  title = {Classifier Technology and the Illusion of Progress},
  author = {Hand, David J.},
  year = {2006},
  volume = {21},
  pages = {1--14},
  issn = {08834237},
  doi = {10.1214/088342306000000060},
  abstract = {A great many tools have been developed for supervised classification, ranging from early methods such as linear discriminant analysis through to modern developments such as neural networks and support vector machines. A large number of comparative studies have been conducted in attempts to establish the relative superiority of these methods. This paper argues that these comparisons often fail to take into account important aspects of real problems, so that the apparent superiority of more sophisticated methods may be something of an illusion. In particular, simple methods typically yield performance almost as good as more sophisticated methods, to the extent that the difference in performance may be swamped by other sources of uncertainty that generally are not considered in the classical supervised classification paradigm. \textcopyright{} Institute of Mathematical Statistics, 2006.},
  file = {/home/harrisonpl/Documents/PDFs/Hand - 2006 - Classifier technology and the illusion of progress.pdf},
  journal = {Statistical Science},
  keywords = {Empirical comparisons,Error rate,Flat maximum effect,Misclassification rate,Population drift,Principle of parsimony,Problem uncertainty,Selectivity bias,Simplicity,Supervised classification},
  number = {1}
}

@article{handyEpigeneticModificationsBasic2011,
  ids = {DianeE.HandyRitaCastroJosephLoscalzo2011,Handy2011},
  title = {Epigenetic Modifications: {{Basic}} Mechanisms and Role in Cardiovascular Disease},
  author = {Handy, Diane E. and Castro, Rita and Loscalzo, Joseph},
  year = {2011},
  volume = {123},
  pages = {2145--2156},
  issn = {00097322},
  doi = {10.1161/CIRCULATIONAHA.110.956839},
  abstract = {With the effectiveness of therapeutic agents ever decreasing and the increased incidence of multi-drug resistant pathogens, there is a clear need for administration of more potent, potentially more toxic, drugs. Alternatively, biopharmaceuticals may hold potential but require specialised protection from premature in vivo degradation. Thus, a paralleled need for specialised drug delivery systems has arisen. Although cell-mediated drug delivery is not a completely novel concept, the few applications described to date are not yet ready for in vivo application, for various reasons such as drug-induced carrier cell death, limited control over the site and timing of drug release and/or drug degradation by the host immune system. Here, we present our hypothesis for a new drug delivery system, which aims to negate these limitations. We propose transport of nanoparticle-encapsulated drugs inside autologous macrophages polarised to M1 phenotype for high mobility and treated to induce transient phagosome maturation arrest. In addition, we propose a significant shift of existing paradigms in the study of host-microbe interactions, in order to study microbial host immune evasion and dissemination patterns for their therapeutic utilisation in the context of drug delivery. We describe a system in which microbial strategies may be adopted to facilitate absolute control over drug delivery, and without sacrificing the host carrier cells. We provide a comprehensive summary of the lessons we can learn from microbes in the context of drug delivery and discuss their feasibility for in vivo therapeutic application. We then describe our proposed ``synthetic microbe drug delivery system'' in detail. In our opinion, this multidisciplinary approach may hold the solution to effective, controlled drug delivery.},
  annotation = {\_eprint: NIHMS150003},
  arxiv = {NIHMS150003},
  arxivid = {NIHMS150003},
  file = {/home/harrisonpl/Documents/PDFs/Handy et al. - 2011 - Epigenetic modifications.pdf},
  isbn = {6176321972},
  journal = {Circulation},
  keywords = {cardiovascular diseases,epiblast,genes,gfp fusion,histone h2b-,icm,lineage specification,live imaging,metabolism,mouse blastocyst,pdgfr α,primitive endoderm,risk factors},
  number = {19},
  pmid = {1000000221}
}

@incollection{hawPerformPathwayEnrichment2020,
  title = {Perform Pathway Enrichment Analysis Using {{reactomeFIViz}}},
  booktitle = {Methods in {{Molecular Biology}}},
  author = {Haw, Robin and Loney, Fred and Ong, Edison and He, Yongqun and Wu, Guanming},
  year = {2020},
  volume = {2074},
  pages = {165--179},
  publisher = {{Humana Press Inc.}},
  abstract = {Modern large-scale biological data analysis often generates a set of significant genes, frequently associated with scores. Pathway-based approaches are routinely performed to understand the functional contexts of these genes. Reactome is the most comprehensive open-access biological pathway knowledge base, widely used in the research community, providing a solid foundation for pathway-based data analysis. ReactomeFIViz is a Cytoscape app built upon Reactome pathways to help users perform pathway- and network-based data analysis and visualization. In this chapter we describe procedures on how to perform pathway enrichment analysis using ReactomeFIViz for a gene score file. We describe two types of analysis: pathway enrichment based on a set of significant genes and GSEA analysis using gene scores without cutoff. We also describe a feature to overlay gene scores onto pathway diagrams, enabling users to understand the underlying mechanisms for up- or down- regulated pathways collected from pathway analysis.},
  keywords = {\#nosource,Biological pathway,Cytoscape,Gene score,GSEA,Pathway enrichment analysis,Reactome,ReactomeFIViz}
}

@article{haywoodTranscriptomeSignatureVentricular2020a,
  ids = {haywoodTranscriptomeSignatureVentricular2020},
  title = {Transcriptome Signature of Ventricular Arrhythmia in Dilated Cardiomyopathy Reveals Increased Fibrosis and Activated {{TP53}}},
  author = {Haywood, Mary E. and Cocciolo, Andrea and Porter, Kadijah F. and Dobrinskikh, Evgenia and Slavov, Dobromir and Graw, Sharon L. and Reece, T. Brett and Ambardekar, Amrut V. and Bristow, Michael R. and Mestroni, Luisa and Taylor, Matthew R. G.},
  year = {2020},
  month = feb,
  volume = {139},
  pages = {124--134},
  publisher = {{Elsevier}},
  issn = {0022-2828, 1095-8584},
  doi = {10.1016/j.yjmcc.2019.12.010},
  abstract = {{$<$}h2{$>$}Abstract{$<$}/h2{$><$}h3{$>$}Aims{$<$}/h3{$><$}p{$>$}One-third of DCM patients experience ventricular tachycardia (VT), but a clear biological basis for this has not been established. The purpose of this study was to identify transcriptome signatures and enriched pathways in the hearts of dilated cardiomyopathy (DCM) patients with VT.{$<$}/p{$><$}h3{$>$}Methods and results{$<$}/h3{$><$}p{$>$}We used RNA-sequencing in explanted heart tissue from 49 samples: 19 DCM patients with VT, 16 DCM patients without VT, and 14 non-failing controls. We compared each DCM cohort to the controls and identified the genes that were differentially expressed in DCM patients with VT but not without VT. Differentially expressed genes were evaluated using pathway analysis, and pathways of interest were investigated by qRT-PCR validation, Western blot, and microscopy. There were 590 genes differentially expressed in DCM patients with VT that are not differentially expressed in patients without VT. These genes were enriched for genes in the TGF\ss 1 and TP53 signaling pathways. Increased fibrosis and activated TP53 signaling was demonstrated in heart tissue of DCM patients with VT.{$<$}/p{$><$}h3{$>$}Conclusions{$<$}/h3{$><$}p{$>$}Our study supports that distinct biological mechanisms distinguish ventricular arrhythmia in DCM patients.{$<$}/p{$>$}},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/Haywood et al. - 2020 - Transcriptome signature of ventricular arrhythmia in dilated cardiomyopathy.pdf},
  journal = {Journal of Molecular and Cellular Cardiology},
  keywords = {\#nosource},
  language = {English},
  pmid = {31958463}
}

@article{hendrycksGaussianErrorLinear2016,
  ids = {Hendrycks2016},
  title = {Gaussian {{Error Linear Units}} ({{GELUs}})},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  year = {2016},
  pages = {1--6},
  abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. The GELU nonlinearity weights inputs by their magnitude, rather than gates inputs by their sign as in ReLUs. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
  annotation = {\_eprint: 1606.08415},
  archivePrefix = {arXiv},
  arxivid = {1606.08415},
  eprint = {1606.08415},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Hendrycks, Gimpel - 2016 - Gaussian Error Linear Units (GELUs).pdf},
  journal = {arXiv},
  keywords = {Non-linear,Regularization,RNN}
}

@inproceedings{hennigOntologybasedApproachText2008,
  ids = {Hennig2008},
  title = {An Ontology-Based Approach to Text Summarization},
  booktitle = {Proceedings - 2008 {{IEEE}}/{{WIC}}/{{ACM International Conference}} on {{Web Intelligence}} and {{Intelligent Agent Technology}} - {{Workshops}}, {{WI}}-{{IAT Workshops}} 2008},
  author = {Hennig, Leonhard and Umbrath, Winfried and Wetzker, Robert},
  year = {2008},
  pages = {291--294},
  publisher = {{IEEE Computer Society}},
  doi = {10.1109/WIIAT.2008.175},
  abstract = {Extractive text summarization aims to create a condensed version of one or more source documents by selecting the most informative sentences. Research in text summarization has therefore often focused on measures of the usefulness of sentences for a summary. We present an approach to sentence extraction that maps sentences to nodes of a hierarchical ontology. By considering ontology attributes we are able to improve the semantic representation of a sentence's information content. The classifier that maps sentences to the taxonomy is trained using search engines and is therefore very flexible and not bound to a specific domain. In our experiments, we train an SVM classifier to identify summary sentences using ontology-based sentence features. Our experimental results show that the ontology-based extraction of sentences outperforms baseline classifiers, leading to higher Rouge scores of summary extracts. \textcopyright{} 2008 IEEE.},
  file = {/home/harrisonpl/Documents/PDFs/Hennig et al. - 2008 - An ontology-based approach to text summarization.pdf},
  isbn = {978-0-7695-3496-1},
  keywords = {hierarchical classification,hunter lab,Natural language processing,ontology,sentence extraction,summarization},
  mendeley-tags = {Natural language processing,hunter lab},
  series = {{{WI}}-{{IAT}} '08}
}

@article{henryBiPOmRulebasedOntology2020a,
  title = {{{BiPOm}}: A Rule-Based Ontology to Represent and Infer Molecule Knowledge from a Biological Process-Centered Viewpoint},
  shorttitle = {{{BiPOm}}},
  author = {Henry, Vincent and Sa{\"i}s, Fatiha and Inizan, Olivier and Marchadier, Elodie and Dibie, Juliette and Goelzer, Anne and Fromion, Vincent},
  year = {2020},
  month = jul,
  volume = {21},
  pages = {327},
  issn = {1471-2105},
  doi = {10.1186/s12859-020-03637-9},
  abstract = {Managing and organizing biological knowledge remains a major challenge, due to the complexity of living systems. Recently, systemic representations have been promising in tackling such a challenge at the whole-cell scale. In such representations, the cell is considered as a system composed of interlocked subsystems. The need is now to define a relevant formalization of the systemic description of cellular processes.},
  file = {/home/harrisonpl/Documents/PDFs/Henry et al. - 2020 - BiPOm.pdf;/home/harrisonpl/Zotero/storage/UWZJ754R/s12859-020-03637-9.html},
  journal = {BMC Bioinformatics},
  number = {1}
}

@article{hernaezComparisonSingleModulebased2020,
  ids = {Hernaez},
  title = {Comparison of Single and Module-Based Methods for Modeling Gene Regulatory Networks},
  author = {Hernaez, Mikel and Blatti, Charles and Gevaert, Olivier},
  year = {2020},
  volume = {36},
  pages = {558--567},
  issn = {13674811},
  doi = {10.1093/bioinformatics/btz549},
  abstract = {MOTIVATION: Gene regulatory networks describe the regulatory relationships among genes, and developing methods for reverse engineering these networks is an ongoing challenge in computational biology. The majority of the initially proposed methods for gene regulatory network discovery create a network of genes and then mine it in order to uncover previously unknown regulatory processes. More recent approaches have focused on inferring modules of co-regulated genes, linking these modules with regulatory genes and then mining them to discover new molecular biology. RESULTS: In this work we analyze module-based network approaches to build gene regulatory networks, and compare their performance to single gene network approaches. In the process, we propose a novel approach to estimate gene regulatory networks drawing from the module-based methods. We show that generating modules of co-expressed genes which are predicted by a sparse set of regulators using a variational Bayes method, and then building a bipartite graph on the generated modules using sparse regression, yields more informative networks than previous single and module-based network approaches as measured by: (i) the rate of enriched gene sets, (ii) a network topology assessment, (iii) ChIP-Seq evidence and (iv) the KnowEnG Knowledge Network collection of previously characterized gene-gene interactions. AVAILABILITY AND IMPLEMENTATION: The code is written in R and can be downloaded from https://github.com/mikelhernaez/linker. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
  file = {/home/harrisonpl/Documents/PDFs/Hernaez et al. - 2020 - Comparison of single and module-based methods for modeling gene regulatory.pdf},
  journal = {Bioinformatics (Oxford, England)},
  number = {2}
}

@article{hoehndorfRoleOntologiesBiological2015,
  ids = {Hoehndorf2015},
  title = {The Role of Ontologies in Biological and Biomedical Research: {{A}} Functional Perspective},
  author = {Hoehndorf, Robert and Schofield, Paul N. and Gkoutos, Georgios V.},
  year = {2015},
  volume = {16},
  pages = {1069--1080},
  issn = {14774054},
  doi = {10.1093/bib/bbv011},
  abstract = {Ontologies are widely used in biological and biomedical research. Their success lies in their combination of fourmain features present in almost all ontologies: provision of standard identifiers for classes and relations that represent the phenomena within a domain; provision of a vocabulary for a domain; provision ofmetadata that describes the intended meaning of the classes and relations in ontologies; and the provision ofmachine-readable axioms and definitions that enable computational access to some aspects of themeaning of classes and relations. While each of these features enables applications that facilitate data integration, data access and analysis, a great potential lies in the possibility of combining these four features to support integrative analysis and interpretation ofmultimodal data. Here, we provide a functional perspective on ontologies in biology and biomedicine, focusing on what ontologies can do and describing how they can be used in support of integrative research. We also outline perspectives for using ontologies in data-driven science, in particular their application in structured datamining andmachine learning applications.},
  file = {/home/harrisonpl/Documents/PDFs/Hoehndorf et al. - 2015 - The role of ontologies in biological and biomedical research.pdf},
  journal = {Briefings in Bioinformatics},
  keywords = {Data integration,Datamining,Ontology,SemanticWeb},
  number = {6}
}

@article{hofstadterdouglasr.CopycatProjectModel2016,
  title = {The Copycat Project: {{A}} Model of Mental Fluidity and Analogy-Making},
  author = {{Hofstadter, Douglas R.} and Mitchell, Melanie},
  year = {2016},
  month = mar,
  volume = {8},
  pages = {4--4},
  issn = {1931-8499, 1931-8499},
  doi = {10.1145/2907674.2907678},
  file = {/home/harrisonpl/Documents/PDFs/Douglas Hofstadter, Melanie Mitchell - The Copycat Project.pdf;/home/harrisonpl/Documents/PDFs/Hofstadter, Douglas R., Mitchell - 2016 - The copycat project.pdf},
  journal = {ACM SIGEVOlution},
  language = {en},
  number = {2}
}

@article{hooperInteractionsMicrobiotaImmune2012,
  ids = {Hooper2012},
  title = {Interactions between the Microbiota and the Immune System},
  booktitle = {Science},
  author = {Hooper, Lora V. and Littman, Dan R. and Macpherson, Andrew J.},
  year = {2012},
  volume = {336},
  issn = {10959203},
  doi = {10.1126/science.1223490},
  abstract = {The large numbers of microorganisms that inhabit mammalian body surfaces have a highly coevolved relationship with the immune system. Although many of these microbes carry out functions that are critical for host physiology, they nevertheless pose the threat of breach with ensuing pathologies. The mammalian immune system plays an essential role in maintaining homeostasis with resident microbial communities, thus ensuring that the mutualistic nature of the host-microbial relationship is maintained. At the same time, resident bacteria profoundly shape mammalian immunity. Here, we review advances in our understanding of the interactions between resident microbes and the immune system and the implications of these findings for human health.},
  file = {/home/harrisonpl/Documents/PDFs/Hooper et al. - 2012 - Interactions between the microbiota and the immune system.pdf},
  journal = {Science},
  pmid = {22674334}
}

@article{horrocksEvenMoreIrresistible2006,
  title = {The Even More Irresistible {{SROIQ}}},
  author = {Horrocks, Ian and Kutz, Oliver and Sattler, Ulrike},
  year = {2006},
  pages = {57--67},
  abstract = {We describe an extension of the description logic underlying OWL-DL, SHOIN, with a number of expressive means that we believe will make it more useful in practice. Roughly speaking, we extend SHOIN with all expressive means that were suggested to us by ontology developers as useful additions to OWL-DL, and which, additionally, do not affect its decidability and practicability. We consider complex role inclusion axioms of the form R S v R or S R v R to express propagation of one property along another one, which have proven useful in medical terminologies. Furthermore, we extend SHOIN with reflexive, antisymmetric, and irreflexive roles, disjoint roles, a universal role, and constructs 9R.Self, allowing, for instance, the definition of concepts such as a "narcist". Finally, we consider negated role assertions in Aboxes and qualified number restrictions. The resulting logic is called SROIQ. We present a rather elegant tableau-based reasoning algorithm: it combines the use of automata to keep track of universal value restrictions with the techniques developed for SHOIQ. The logic SROIQ has been adopted as the logical basis for the next iteration of OWL, OWL 1.1. Copyright \textcopyright{} 2006, American Association for Artificial Intelligence.},
  journal = {Proceedings of the International Workshop on Temporal Representation and Reasoning},
  keywords = {\#nosource,ontology,Semantic Web}
}

@article{huangMaximumCommonSubgraph2006,
  ids = {Huang2006},
  title = {Maximum Common Subgraph: {{Some}} Upper Bound and Lower Bound Results},
  author = {Huang, Xiuzhen and Lai, Jing and Jennings, Steven F.},
  year = {2006},
  month = dec,
  volume = {7},
  pages = {S6},
  publisher = {{BioMed Central}},
  issn = {14712105},
  doi = {10.1186/1471-2105-7-S4-S6},
  abstract = {Background: Structure matching plays an important part in understanding the functional role of biological structures. Bioinformatics assists in this effort by reformulating this process into a problem of finding a maximum common subgraph between graphical representations of these structures. Among the many different variants of the maximum common subgraph problem, the maximum common induced subgraph of two graphs is of special interest. Results: Based on current research in the area of parameterized computation, we derive a new lower bound for the exact algorithms of the maximum common induced subgraph of two graphs which is the best currently known. Then we investigate the upper bound and design techniques for approaching this problem, specifically, reducing it to one of finding a maximum clique in the product graph of the two given graphs. Considering the upper bound result, the derived lower bound result is asymptotically tight. Conclusion: Parameterized computation is a viable approach with great potential for investigating many applications within bioinformatics, such as the maximum common subgraph problem studied in this paper. With an improved hardness result and the proposed approaches in this paper, future research can be focused on further exploration of efficient approaches for different variants of this problem within the constraints imposed by real applications. \textcopyright{} 2006 Huang et al; licensee BioMed Central Ltd.},
  file = {/home/harrisonpl/Documents/PDFs/Huang et al. - 2006 - Maximum common subgraph.pdf},
  journal = {BMC Bioinformatics},
  number = {SUPPL.4}
}

@article{hughesMSPrepSummarizationNormalizationDiagnostics2014,
  ids = {Hughe,Hughes2014a},
  title = {{{MSPrep}}-{{Summarization}}, Normalization and Diagnostics for Processing of Mass Spectrometry-Based Metabolomic Data},
  author = {Hughes, Grant and {Cruickshank-Quinn}, Charmion and Reisdorph, Richard and Lutz, Sharon and Petrache, Irina and Reisdorph, Nichole and Bowler, Russell and Kechris, Katerina},
  year = {2014},
  volume = {30},
  pages = {133--134},
  issn = {13674803},
  doi = {10.1093/bioinformatics/btt589},
  abstract = {Motivation: Although R packages exist for the pre-processing of metabolomic data, they currently do not incorporate additional analysis steps of summarization, filtering and normalization of aligned data. We developed the MSPrep R package to complement other packages by providing these additional steps, implementing a selection of popular normalization algorithms and generating diagnostics to help guide investigators in their analyses. Availability: http://www.sourceforge.net/projects/msprepContact: Supplementary Information: Supplementary materials are available at Bioinformatics online. \textcopyright{} 2013 The Author .},
  file = {/home/harrisonpl/Documents/PDFs/Hughes et al. - 2014 - MSPrep-Summarization, normalization and diagnostics for processing of mass.pdf},
  journal = {Bioinformatics},
  number = {1},
  pmid = {24174567}
}

@misc{humanNIHHumanMicrobiome2013,
  title = {{{NIH Human Microbiome Project Defines Normal Bacterial Makeup}}},
  author = {Human, N I H and Project, Microbiome and Normal, Defines and Makeup, Bacterial},
  year = {2013},
  abstract = {Genome sequencing creates first reference data for microbes living with healthy adults.},
  howpublished = {https://www.nih.gov/news-events/news-releases/nih-human-microbiome-project-defines-normal-bacterial-makeup-body},
  keywords = {\#nosource}
}

@article{hummelSymbolicconnectionistTheoryRelational2003,
  title = {A Symbolic-Connectionist Theory of Relational Inference and Generalization},
  booktitle = {Psychological Review},
  author = {Hummel, John E. and Holyoak, Keith J.},
  year = {2003},
  volume = {110},
  pages = {220--264},
  issn = {0033295X},
  doi = {10.1037/0033-295X.110.2.220},
  abstract = {The authors present a theory of how relational inference and generalization can be accomplished within a cognitive architecture that is psychologically and neurally realistic. Their proposal is a form of symbolic connectionism: a connectionist system based on distributed representations of concept meanings, using temporal synchrony to bind fillers and roles into relational structures. The authors present a specific instantiation of their theory in the form of a computer simulation model, Learning and Inference with Schemas and Analogies (LISA). By using a kind of self-supervised learning, LISA can make specific inferences and form new relational generalizations and can hence acquire new schemas by induction from examples. The authors demonstrate the sufficiency of the model by using it to simulate a body of empirical phenomena concerning analogical inference and relational generalization.},
  file = {/home/harrisonpl/Documents/PDFs/Hummel, Holyoak - 2003 - A symbolic-connectionist theory of relational inference and generalization.pdf},
  keywords = {HPL comprehensive exam,hunter lab},
  language = {English},
  mendeley-tags = {HPL comprehensive exam,hunter lab},
  number = {2}
}

@article{hunterOpenDMAPOpenSource2008,
  ids = {Hunter2008},
  title = {{{OpenDMAP}}: {{An}} Open Source, Ontology-Driven Concept Analysis Engine, with Applications to Capturing Knowledge Regarding Protein Transport, Protein Interactions and Cell-Type-Specific Gene Expression},
  shorttitle = {{{OpenDMAP}}},
  author = {Hunter, Lawrence and Lu, Zhiyong and Firby, James and Baumgartner, William A. and Johnson, Helen L. and Ogren, Philip V. and Cohen, K. Bretonnel},
  year = {2008},
  volume = {9},
  pages = {78},
  issn = {14712105},
  doi = {10.1186/1471-2105-9-78},
  abstract = {Background: Information extraction (IE) efforts are widely acknowledged to be important in harnessing the rapid advance of biomedical knowledge, particularly in areas where important factual information is published in a diverse literature. Here we report on the design, implementation and several evaluations of OpenDMAP, an ontology-driven, integrated concept analysis system. It significantly advances the state of the art in information extraction by leveraging knowledge in ontological resources, integrating diverse text processing applications, and using an expanded pattern language that allows the mixing of syntactic and semantic elements and variable ordering. Results: OpenDMAP information extraction systems were produced for extracting protein transport assertions (transport), protein-protein interaction assertions (interaction) and assertions that a gene is expressed in a cell type (expression). Evaluations were performed on each system, resulting in F-scores ranging from .26 - .72 (precision .39 - .85, recall .16 - .85). Additionally, each of these systems was run over all abstracts in MEDLINE, producing a total of 72,460 transport instances, 265,795 interaction instances and 176,153 expression instances. Conclusion: OpenDMAP advances the performance standards for extracting protein-protein interaction predications from the full texts of biomedical research articles. Furthermore, this level of performance appears to generalize to other information extraction tasks, including extracting information about predicates of more than two arguments. The output of the information extraction system is always constructed from elements of an ontology, ensuring that the knowledge representation is grounded with respect to a carefully constructed model of reality. The results of these efforts can be used to increase the efficiency of manual curation efforts and to provide additional features in systems that integrate multiple sources for information extraction. The open source OpenDMAP code library is freely available at http://bionlp.sourceforge.net/. \textcopyright{} 2008 Hunter et al; licensee BioMed Central Ltd.},
  file = {/home/harrisonpl/Documents/PDFs/Hunter et al. - 2008 - OpenDMAP.pdf},
  journal = {BMC Bioinformatics},
  keywords = {ontology},
  mendeley-tags = {ontology},
  number = {1}
}

@article{huttenhowerInflammatoryBowelDisease2014,
  ids = {Huttenhower2014},
  title = {Inflammatory Bowel Disease as a Model for Translating the Microbiome},
  author = {Huttenhower, Curtis and Kostic, Aleksandar D. and Xavier, Ramnik J.},
  year = {2014},
  volume = {40},
  pages = {843--854},
  issn = {10974180},
  doi = {10.1016/j.immuni.2014.05.013},
  abstract = {The inflammatory bowel diseases (IBDs) are among the most closely studied chronic inflammatory disorders that involve environmental, host genetic, and commensal microbial factors. This combination of features has made IBD both an appropriate and a high-priority platform for translatable research in host-microbiome interactions. Decades of epidemiology have identified environmental risk factors, although most mechanisms of action remain unexplained. The genetic architecture of IBD has been carefully dissected in multiple large populations, identifying several responsible host epithelial and immune pathways but without yet a complete systems-level explanation. Most recently, the commensal gut microbiota have been found to be both ecologically and functionally perturbed during the disease, but with as-yet-unexplained heterogeneity among IBD subtypes and individual patients. IBD thus represents perhaps the most comprehensive current model for understanding the human microbiome's role in complex inflammatory disease. Here, we review the influences of the microbiota on IBD and its potential for translational medicine. Inflammatory bowel diseases (IBDs) involve environmental, host genetic, and commensal microbial factors, thereby allowing interrogation of interactions between the microbiome and immune system. Here Huttenhower etal. review the role of microbiota in IBD and its potential for translational medicine. \textcopyright{} 2014 Elsevier Inc.},
  file = {/home/harrisonpl/Documents/PDFs/Huttenhower et al. - 2014 - Inflammatory bowel disease as a model for translating the microbiome.pdf},
  journal = {Immunity},
  number = {6}
}

@article{huttlinArchitectureHumanInteractome2017,
  title = {Architecture of the Human Interactome Defines Protein Communities and Disease Networks},
  booktitle = {Nature},
  author = {Huttlin, Edward L. and Bruckner, Raphael J. and Paulo, Joao A. and Cannon, Joe R. and Ting, Lily and Baltier, Kurt and Colby, Greg and Gebreab, Fana and Gygi, Melanie P. and Parzen, Hannah and Szpyt, John and Tam, Stanley and Zarraga, Gabriela and {Pontano-Vaites}, Laura and Swarup, Sharan and White, Anne E. and Schweppe, Devin K. and Rad, Ramin and Erickson, Brian K. and Obar, Robert A. and Guruharsha, K. G. and Li, Kejie and {Artavanis-Tsakonas}, Spyros and Gygi, Steven P. and Wade Harper, J.},
  year = {2017},
  volume = {545},
  pages = {505--509},
  publisher = {{Nature Publishing Group}},
  issn = {14764687},
  doi = {10.1038/nature22366},
  abstract = {The physiology of a cell can be viewed as the product of thousands of proteins acting in concert to shape the cellular response. Coordination is achieved in part through networks of protein-protein interactions that assemble functionally related proteins into complexes, organelles, and signal transduction pathways. Understanding the architecture of the human proteome has the potential to inform cellular, structural, and evolutionary mechanisms and is critical to elucidating how genome variation contributes to disease. Here we present BioPlex 2.0 (Biophysical Interactions of ORFeome-derived complexes), which uses robust affinity purification-mass spectrometry methodology to elucidate protein interaction networks and co-complexes nucleated by more than 25\% of protein-coding genes from the human genome, and constitutes, to our knowledge, the largest such network so far. With more than 56,000 candidate interactions, BioPlex 2.0 contains more than 29,000 previously unknown co-associations and provides functional insights into hundreds of poorly characterized proteins while enhancing network-based analyses of domain associations, subcellular localization, and co-complex formation. Unsupervised Markov clustering of interacting proteins identified more than 1,300 protein communities representing diverse cellular activities. Genes essential for cell fitness are enriched within 53 communities representing central cellular functions. Moreover, we identified 442 communities associated with more than 2,000 disease annotations, placing numerous candidate disease genes into a cellular framework. BioPlex 2.0 exceeds previous experimentally derived interaction networks in depth and breadth, and will be a valuable resource for exploring the biology of incompletely characterized proteins and for elucidating larger-scale patterns of proteome organization.},
  file = {/home/harrisonpl/Documents/PDFs/Huttlin et al. - 2017 - Architecture of the human interactome defines protein communities and disease2.pdf},
  journal = {Nature},
  keywords = {Lawrence Hunter,Recommendation},
  mendeley-tags = {Lawrence Hunter,Recommendation},
  number = {7655},
  pmid = {28514442}
}

@article{ICEPIndianaCheminformatics,
  title = {{{ICEP}}: {{Indiana Cheminformatics Education Portal}} - {{Characterizing 2D}} Structures with Descriptors and Fingerprints},
  keywords = {\#nosource}
}

@article{IntelliJCheatsheet,
  ids = {IntelliJCheatsheeta},
  title = {{{IntelliJ Cheatsheet}}},
  file = {/home/harrisonpl/Documents/PDFs/IntelliJ Cheatsheet.pdf}
}

@techreport{IntuitiveOverviewTaylor,
  ids = {IntuitiveOverviewTaylora},
  title = {An {{Intuitive Overview}} of {{Taylor Series}} \guillemotright{} Mixedmath},
  file = {/home/harrisonpl/Documents/PDFs/An Intuitive Overview of Taylor Series » mixedmath.pdf}
}

@article{islamajdoganBioCreativeVIPrecision2017,
  ids = {IslamajDogan,IslamajDogan2017,IslamajDogan2017a},
  title = {{{BioCreative VI Precision Medicine Track}}: Creating a Training Corpus for Mining Protein-Protein Interactions Affected by Mutations},
  shorttitle = {{{BioCreative VI Precision Medicine Track}}},
  author = {Islamaj Dogan, Rezarta and {Chatr-aryamontri}, Andrew and Kim, Sun and Wei, Chih-Hsuan and Peng, Yifan and Comeau, Donald and Lu, Zhiyong},
  year = {2017},
  pages = {171--175},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/w17-2321},
  abstract = {The Precision Medicine Track in BioCrea-tive VI aims to bring together the BioNLP community for a novel challenge focused on mining the biomedical literature in search of mutations and protein-protein interactions (PPI). In order to support this track with an effective training dataset with limited curator time, the track organizers carefully reviewed PubMed articles from two different sources: curated public PPI databases, and the results of state-of-the-art public text mining tools. We detail here the data collection, manual review and annotation process and describe this training corpus characteristics. We also describe a corpus performance baseline. This analysis will provide useful information to developers and researchers for comparing and developing innovative text mining approaches for the BioCreative VI challenge and other Precision Medicine related applications .},
  file = {/home/harrisonpl/Documents/PDFs/Islamaj Dogan et al. - 2017 - BioCreative VI Precision Medicine Track.pdf},
  journal = {BioNLP 2017}
}

@article{jacksonROBOTToolAutomating2019,
  ids = {Jackson2019},
  title = {{{ROBOT}}: {{A Tool}} for {{Automating Ontology Workflows}}},
  author = {Jackson, Rebecca C. and Balhoff, James P. and Douglass, Eric and Harris, Nomi L. and Mungall, Christopher J. and Overton, James A.},
  year = {2019},
  volume = {20},
  pages = {1--10},
  publisher = {{BMC Bioinformatics}},
  issn = {14712105},
  doi = {10.1186/s12859-019-3002-3},
  abstract = {Background: Ontologies are invaluable in the life sciences, but building and maintaining ontologies often requires a challenging number of distinct tasks such as running automated reasoners and quality control checks, extracting dependencies and application-specific subsets, generating standard reports, and generating release files in multiple formats. Similar to more general software development, automation is the key to executing and managing these tasks effectively and to releasing more robust products in standard forms. For ontologies using the Web Ontology Language (OWL), the OWL API Java library is the foundation for a range of software tools, including the Prot\'eg\'e ontology editor. In the Open Biological and Biomedical Ontologies (OBO) community, we recognized the need to package a wide range of low-level OWL API functionality into a library of common higher-level operations and to make those operations available as a command-line tool. Results: ROBOT (a recursive acronym for "ROBOT is an OBO Tool") is an open source library and command-line tool for automating ontology development tasks. The library can be called from any programming language that runs on the Java Virtual Machine (JVM). Most usage is through the command-line tool, which runs on macOS, Linux, and Windows. ROBOT provides ontology processing commands for a variety of tasks, including commands for converting formats, running a reasoner, creating import modules, running reports, and various other tasks. These commands can be combined into larger workflows using a separate task execution system such as GNU Make, and workflows can be automatically executed within continuous integration systems. Conclusions: ROBOT supports automation of a wide range of ontology development tasks, focusing on OBO conventions. It packages common high-level ontology development functionality into a convenient library, and makes it easy to configure, combine, and execute individual tasks in comprehensive, automated workflows. This helps ontology developers to efficiently create, maintain, and release high-quality ontologies, so that they can spend more time focusing on development tasks. It also helps guarantee that released ontologies are free of certain types of logical errors and conform to standard quality control checks, increasing the overall robustness and efficiency of the ontology development lifecycle.},
  file = {/home/harrisonpl/Documents/PDFs/Jackson et al. - 2019 - ROBOT.pdf},
  journal = {BMC Bioinformatics},
  keywords = {Automation,Import management,Ontology development,Ontology release,Quality control,Reasoning,Workflows},
  number = {1}
}

@article{jaiswalPlantOntologyPO2005,
  ids = {Jaiswal,Jaiswal2005a},
  title = {Plant {{Ontology}} ({{PO}}): {{A}} Controlled Vocabulary of Plant Structures and Growth Stages},
  shorttitle = {Plant {{Ontology}} ({{PO}})},
  author = {Jaiswal, Pankaj and Avraham, Shulamit and Ilic, Katica and Kellogg, Elizabeth A. and McCouch, Susan and Pujar, Anuradha and Reiser, Leonore and Rhee, Seung Y. and Sachs, Martin M. and Schaeffer, Mary and Stein, Lincoln and Stevens, Peter and Vincent, Leszek and Ware, Doreen and Zapata, Felipe},
  year = {2005},
  volume = {6},
  pages = {388--397},
  issn = {15316912},
  doi = {10.1002/cfg.496},
  abstract = {The Plant Ontology Consortium (POC) (www.plantontology.org) is a collaborative effort among several plant databases and experts in plant systematics, botany and genomics. A primary goal of the POC is to develop simple yet robust and extensible controlled vocabularies that accurately reflect the biology of plant structures and developmental stages. These provide a network of vocabularies linked by relationships (ontology) to facilitate queries that cut across datasets within a database or between multiple databases. The current version of the ontology integrates diverse vocabularies used to describe Arabidopsis, maize and rice (Oryza sp.) anatomy, morphology and growth stages. Using the ontology browser, over 3500 gene annotations from three species-specific databases, The Arabidopsis Information Resource (TAIR) for Arabidopsis, Gramene for rice and MaizeGDB for maize, can now be queried and retrieved. Copyright \textcopyright{} 2006 John Wiley \& Sons, Ltd.},
  file = {/home/harrisonpl/Documents/PDFs/Jaiswal et al. - 2005 - Plant Ontology (PO).pdf},
  journal = {Comparative and Functional Genomics},
  keywords = {Controlled vocabulary,Gene expression,ontology,Phenotype,Plant anatomy,Plant development,Plant growth stage,Plant morphology,Plant ontology},
  mendeley-tags = {ontology},
  number = {7-8},
  pmid = {18629207}
}

@article{janssonNetworkExplanationsExplanatory2020,
  title = {Network Explanations and Explanatory Directionality},
  author = {Jansson, Lina},
  year = {2020},
  volume = {375},
  pages = {20190318},
  issn = {0962-8436},
  doi = {10.1098/rstb.2019.0318},
  abstract = {Network explanations raise foundational questions about the nature of scientific explanation. The challenge discussed in this article comes from the fact that network explanations are often thought...},
  file = {/home/harrisonpl/Documents/PDFs/Jansson - 2020 - Network explanations and explanatory directionality.pdf},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  keywords = {\#nosource,directionality,explanation,non-causal models},
  number = {1796}
}

@article{jauregiunanueRecurrentNeuralNetworks2017,
  ids = {JauregiUnanue2017,JauregiUnanue2017b},
  title = {Recurrent Neural Networks with Specialized Word Embeddings for Health-Domain Named-Entity Recognition},
  author = {Jauregi Unanue, I{\~n}igo and Zare Borzeshi, Ehsan and Piccardi, Massimo},
  year = {2017},
  month = dec,
  volume = {76},
  pages = {102--109},
  publisher = {{Academic Press Inc.}},
  issn = {15320464},
  doi = {10.1016/j.jbi.2017.11.007},
  abstract = {Background Previous state-of-the-art systems on Drug Name Recognition (DNR) and Clinical Concept Extraction (CCE) have focused on a combination of text ``feature engineering'' and conventional machine learning algorithms such as conditional random fields and support vector machines. However, developing good features is inherently heavily time-consuming. Conversely, more modern machine learning approaches such as recurrent neural networks (RNNs) have proved capable of automatically learning effective features from either random assignments or automated word ``embeddings''. Objectives (i) To create a highly accurate DNR and CCE system that avoids conventional, time-consuming feature engineering. (ii) To create richer, more specialized word embeddings by using health domain datasets such as MIMIC-III. (iii) To evaluate our systems over three contemporary datasets. Methods Two deep learning methods, namely the Bidirectional LSTM and the Bidirectional LSTM-CRF, are evaluated. A CRF model is set as the baseline to compare the deep learning systems to a traditional machine learning approach. The same features are used for all the models. Results We have obtained the best results with the Bidirectional LSTM-CRF model, which has outperformed all previously proposed systems. The specialized embeddings have helped to cover unusual words in DrugBank and MedLine, but not in the i2b2/VA dataset. Conclusions We present a state-of-the-art system for DNR and CCE. Automated word embeddings has allowed us to avoid costly feature engineering and achieve higher accuracy. Nevertheless, the embeddings need to be retrained over datasets that are adequate for the domain, in order to adequately cover the domain-specific vocabulary.},
  annotation = {\_eprint: 1706.09569},
  archivePrefix = {arXiv},
  arxivid = {1706.09569},
  eprint = {1706.09569},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Jauregi Unanue et al. - 2017 - Recurrent neural networks with specialized word embeddings for health-domain.pdf},
  journal = {Journal of Biomedical Informatics},
  keywords = {Artificial intelligence [MeSH],Clinical concept extraction,Drug name recognition,Machine learning [MeSH],Neural networks (computer) [MeSH]},
  number = {September}
}

@article{jetkaInformationtheoreticAnalysisMultivariate2019,
  ids = {Jetka2019,Jetka2019a},
  title = {Information-Theoretic Analysis of Multivariate Single-Cell Signaling Responses},
  author = {Jetka, Tomasz and Niena{\textbackslash}ltowski, Karol and Winarski, Tomasz and B{\textbackslash}lo{\'n}ski, S{\textbackslash}lawomir and Komorowski, Micha{\textbackslash}l},
  year = {2019},
  volume = {15},
  pages = {e1007132},
  issn = {15537358},
  doi = {10.1371/journal.pcbi.1007132},
  abstract = {Mathematical methods of information theory appear to provide a useful language to describe how stimuli are encoded in activities of signaling effectors. Exploring the information-theoretic perspective, however, remains conceptually, experimentally and computationally challenging. Specifically, existing computational tools enable efficient analysis of relatively simple systems, usually with one input and output only. Moreover, their robust and readily applicable implementations are missing. Here, we propose a novel algorithm, SLEMI\textemdash statistical learning based estimation of mutual information, to analyze signaling systems with high-dimensional outputs and a large number of input values. Our approach is efficient in terms of computational time as well as sample size needed for accurate estimation. Analysis of the NF-{$\kappa$}B single\textemdash cell signaling responses to TNF-{$\alpha$} reveals that NF-{$\kappa$}B signaling dynamics improves discrimination of high concentrations of TNF-{$\alpha$} with a relatively modest impact on discrimination of low concentrations. Provided R-package allows the approach to be used by computational biologists with only elementary knowledge of information theory.},
  annotation = {\_eprint: 1808.05581},
  archivePrefix = {arXiv},
  arxivid = {1808.05581},
  eprint = {1808.05581},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Jetka et al. - 2019 - Information-theoretic analysis of multivariate single-cell signaling responses.pdf},
  isbn = {1111111111},
  journal = {PLoS Computational Biology},
  number = {7}
}

@article{jiaoStatininducedGGPPDepletion2020,
  title = {Statin-Induced {{GGPP}} Depletion Blocks Macropinocytosis and Starves Cells with Oncogenic Defects},
  author = {Jiao, Zhihua and Cai, Huaqing and Long, Yu and Sirka, Orit Katarina and Padmanaban, Veena and Ewald, Andrew J. and Devreotes, Peter N.},
  year = {2020},
  month = feb,
  volume = {117},
  pages = {4158--4168},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1917938117},
  abstract = {Cancer cells display novel characteristics which can be exploited for therapeutic advantage. Isolated studies have shown that 1) the mevalonate pathway and 2) increased macropinocytosis are important in tumorigenesis, but a connection between these two observations has not been envisioned. A library screen for compounds that selectively killed Dictyostelium pten- cells identified pitavastatin. Pitavastatin also killed human breast epithelial MCF10A cells lacking PTEN or expressing K-RasG12V, as well as mouse tumor organoids. The selective killing of cells with oncogenic defects was traced to GGPP (geranylgeranyl diphosphate) depletion. Disruption of GGPP synthase in Dictyostelium revealed that GGPP is needed for pseudopod extension and macropinocytosis. Fluid-phase uptake through macropinocytosis is lower in PTEN-deleted cells and, as reported previously, higher in cells expressing activated Ras. Nevertheless, uptake was more sensitive to pitavastatin in cells with either of these oncogenic mutations than in wild-type cells. Loading the residual macropinosomes after pitavastatin with high concentrations of protein mitigated the cell death, indicating that defective macropinocytosis leads to amino acid starvation. Our studies suggest that the dependence of cancer cells on the mevalonate pathway is due to the role of GGPP in macropinocytosis and the reliance of these cells on macropinocytosis for nutrient uptake. Thus, inhibition of the networks mediating these processes is likely to be effective in cancer intervention.},
  annotation = {ZSCC: 0000000},
  copyright = {\textcopyright{} 2020 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  file = {/home/harrisonpl/Documents/PDFs/Jiao et al. - 2020 - Statin-induced GGPP depletion blocks macropinocytosis and starves cells with.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {cancer,chemotaxis,mevalonate pathway,small GTPases,tumor organoids},
  language = {en},
  number = {8},
  pmid = {32051246}
}

@article{johnsonCausalNetworksCausal2015,
  title = {Causal {{Networks}} or {{Causal Islands}}? {{The Representation}} of {{Mechanisms}} and the {{Transitivity}} of {{Causal Judgment}}},
  shorttitle = {Causal {{Networks}} or {{Causal Islands}}?},
  author = {Johnson, Samuel G. B. and Ahn, Woo-kyoung},
  year = {2015},
  volume = {39},
  pages = {1468--1503},
  issn = {1551-6709},
  doi = {10.1111/cogs.12213},
  abstract = {Knowledge of mechanisms is critical for causal reasoning. We contrasted two possible organizations of causal knowledge\textemdash an interconnected causal network, where events are causally connected without any boundaries delineating discrete mechanisms; or a set of disparate mechanisms\textemdash causal islands\textemdash such that events in different mechanisms are not thought to be related even when they belong to the same causal chain. To distinguish these possibilities, we tested whether people make transitive judgments about causal chains by inferring, given A causes B and B causes C, that A causes C. Specifically, causal chains schematized as one chunk or mechanism in semantic memory (e.g., exercising, becoming thirsty, drinking water) led to transitive causal judgments. On the other hand, chains schematized as multiple chunks (e.g., having sex, becoming pregnant, becoming nauseous) led to intransitive judgments despite strong intermediate links ((Experiments 1\textendash 3). Normative accounts of causal intransitivity could not explain these intransitive judgments (Experiments 4 and 5).},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12213},
  copyright = {Copyright \textcopyright{} 2015 Cognitive Science Society, Inc.},
  file = {/home/harrisonpl/Documents/PDFs/Johnson, Ahn - 2015 - Causal Networks or Causal Islands.pdf;/home/harrisonpl/Zotero/storage/CATGYVWD/cogs.html},
  journal = {Cognitive Science},
  keywords = {Causal mechanisms,Causal reasoning,Knowledge representation,Transitive inference},
  language = {en},
  number = {7}
}

@article{johnstonAromataseInhibitorsBreast2003,
  ids = {Johnston2003},
  title = {Aromatase Inhibitors for Breast Cancer: {{Lessons}} from the Laboratory},
  shorttitle = {Aromatase Inhibitors for Breast Cancer},
  author = {Johnston, Stephen R.D. and Dowsett, Mitch},
  year = {2003},
  volume = {3},
  pages = {821--831},
  issn = {1474175X},
  doi = {10.1038/nrc1211},
  abstract = {Endocrine therapy with tamoxifen has been the mainstay of treatment for hormone-sensitive breast cancer for more than 20 years. An alternative strategy of oestrogen deprivation in postmenopausal women with aromatase inhibitors is set to replace tamoxifen based on better efficacy and a delay in the emergence of endocrine resistance. There are fundamental differences in how tamoxifen and aromatase inhibitors alter oestrogen-receptor signalling. Understanding the response and resistance to these different therapies is central to further improving therapeutic options for women with breast cancer.},
  file = {/home/harrisonpl/Documents/PDFs/Johnston, Dowsett - 2003 - Aromatase inhibitors for breast cancer.pdf},
  journal = {Nature Reviews Cancer},
  number = {11}
}

@article{jordanDevelopmentTamoxifenBreast1988,
  ids = {jordanDevelopmentTamoxifenBreast1988a,jordanDevelopmentTamoxifenBreast1988b},
  title = {The Development of Tamoxifen for Breast Cancer Therapy: A Tribute to the Late {{Arthur L}}. {{Walpole}}},
  shorttitle = {The Development of Tamoxifen for Breast Cancer Therapy},
  author = {Jordan, V. C.},
  year = {1988},
  month = jul,
  volume = {11},
  pages = {197--209},
  issn = {0167-6806},
  doi = {10.1007/BF01807278},
  abstract = {Tamoxifen, a nonsteroidal antiestrogen, is now the endocrine treatment most widely used in breast cancer, both in the adjuvant and advanced disease settings. Here we will trace the development of tamoxifen for advanced breast cancer in postmenopausal patients, consider the biological basis for the recent successful use of tamoxifen for long-term adjuvant therapy, and discuss the use of tamoxifen in premenopausal patients with advanced disease. In part, this will be a historical review offered as a tribute to the late Dr. Arthur L. Walpole, who must receive the chief credit for the discovery of tamoxifen and its subsequent application as an anticancer agent.},
  file = {/home/harrisonpl/Documents/PDFs/Jordan - 1988 - The development of tamoxifen for breast cancer therapy.pdf},
  journal = {Breast Cancer Research and Treatment},
  keywords = {Breast Neoplasms,England,Female,History; 20th Century,Humans,Tamoxifen},
  language = {eng},
  number = {3},
  pmid = {3048447}
}

@article{jordanTamoxifenCatalystChange2008,
  title = {Tamoxifen: Catalyst for the Change to Targeted Therapy},
  shorttitle = {Tamoxifen},
  author = {Jordan, V. Craig},
  year = {2008},
  month = jan,
  volume = {44},
  pages = {30--38},
  issn = {0959-8049},
  doi = {10.1016/j.ejca.2007.11.002},
  abstract = {In the early 1970's, a failed postcoital contraceptive, ICI 46,474, was reinvented as tamoxifen, the first targeted therapy for breast cancer. A cluster of papers published in the European Journal of Cancer described the idea of targeting tamoxifen to patients with oestrogen receptor positive tumours, and proposed the strategic value of using long-term tamoxifen therapy in an adjuvant setting with a consideration of the antitumour properties of the hydroxylated metabolites of tamoxifen. At the time, these laboratory results were slow to be embraced by the clinical community. Today, it is estimated that hundreds of thousands of breast cancer patients are alive today because of targeted long-term adjuvant tamoxifen therapy. Additionally, the first laboratory studies for the use of tamoxifen as a chemopreventive were published. Eventually, the worth of tamoxifen was tested as a chemopreventive and the drug is now known to have an excellent risk benefit ratio in high risk premenopausal women. Overall, the rigorous investigation of the pharmacology of tamoxifen facilitated tamoxifen's ubiquitous use for the targeted treatment of breast cancer, chemoprevention and pioneered the exploration of selective estrogen receptor modulators (SERMs). This new concept subsequently heralded the development of raloxifene, a failed breast cancer drug, for the prevention of osteoporosis and breast cancer without the troublesome side effect of endometrial cancer noted in postmenopausal women who take tamoxifen. Currently, the pharmaceutical industry is exploiting the SERM concept for all members of the nuclear receptor superfamily so that medicines can now be developed for diseases once thought impossible.},
  file = {/home/harrisonpl/Zotero/storage/PCT8GR7K/Jordan - 2008 - Tamoxifen catalyst for the change to targeted the.pdf},
  journal = {European journal of cancer (Oxford, England : 1990)},
  number = {1},
  pmcid = {PMC2566958},
  pmid = {18068350}
}

@article{joslynHypernetworkScienceMultidimensional2020,
  title = {Hypernetwork {{Science}}: {{From Multidimensional Networks}} to {{Computational Topology}}},
  shorttitle = {Hypernetwork {{Science}}},
  author = {Joslyn, Cliff A. and Aksoy, Sinan and Callahan, Tiffany J. and Hunter, Lawrence E. and Jefferson, Brett and Praggastis, Brenda and Purvine, Emilie A. H. and Tripodi, Ignacio J.},
  year = {2020},
  month = mar,
  abstract = {As data structures and mathematical objects used for complex systems modeling, hypergraphs sit nicely poised between on the one hand the world of network models, and on the other that of higher-order mathematical abstractions from algebra, lattice theory, and topology. They are able to represent complex systems interactions more faithfully than graphs and networks, while also being some of the simplest classes of systems representing topological structures as collections of multidimensional objects connected in a particular pattern. In this paper we discuss the role of (undirected) hypergraphs in the science of complex networks, and provide a mathematical overview of the core concepts needed for hypernetwork modeling, including duality and the relationship to bicolored graphs, quantitative adjacency and incidence, the nature of walks in hypergraphs, and available topological relationships and properties. We close with a brief discussion of two example applications: biomedical databases for disease analysis, and domain-name system (DNS) analysis of cyber data.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {2003.11782},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Joslyn et al. - 2020 - Hypernetwork Science.pdf},
  journal = {arXiv:2003.11782 [cs]},
  keywords = {\#nosource,05C65;,Computer Science - Discrete Mathematics,G.2.2},
  primaryClass = {cs}
}

@article{jovelCharacterizationGutMicrobiome2016,
  ids = {Jovel2016},
  title = {Characterization of the Gut Microbiome Using {{16S}} or Shotgun Metagenomics},
  author = {Jovel, Juan and Patterson, Jordan and Wang, Weiwei and Hotte, Naomi and O'Keefe, Sandra and Mitchel, Troy and Perry, Troy and Kao, Dina and Mason, Andrew L. and Madsen, Karen L. and Wong, Gane K.S.},
  year = {2016},
  volume = {7},
  pages = {459},
  issn = {1664302X},
  doi = {10.3389/fmicb.2016.00459},
  abstract = {The advent of next generation sequencing (NGS) has enabled investigations of the gut microbiome with unprecedented resolution and throughput. This has stimulated the development of sophisticated bioinformatics tools to analyze the massive amounts of data generated. Researchers therefore need a clear understanding of the key concepts required for the design, execution and interpretation of NGS experiments on microbiomes. We conducted a literature review and used our own data to determine which approaches work best. The two main approaches for analyzing the microbiome, 16S ribosomal RNA (rRNA) gene amplicons and shotgun metagenomics, are illustrated with analyses of libraries designed to highlight their strengths and weaknesses. Several methods for taxonomic classification of bacterial sequences are discussed. We present simulations to assess the number of sequences that are required to perform reliable appraisals of bacterial community structure. To the extent that fluctuations in the diversity of gut bacterial populations correlate with health and disease, we emphasize various techniques for the analysis of bacterial communities within samples ({$\alpha$}-diversity) and between samples ({$\beta$}-diversity). Finally, we demonstrate techniques to infer the metabolic capabilities of a bacteria community from these 16S and shotgun data.},
  file = {/home/harrisonpl/Documents/PDFs/Jovel et al. - 2016 - Characterization of the gut microbiome using 16S or shotgun metagenomics.pdf},
  journal = {Frontiers in Microbiology},
  keywords = {16S rRNA gene sequencing,Bioinformatics,Diversity analysis,Functional profiling,Gut microbiome,Shotgun metagenomics,Taxonomic classification},
  number = {APR},
  pmid = {27148170}
}

@inproceedings{judeapearlBayesianNetwcrksModel1985,
  title = {Bayesian Netwcrks: {{A}} Model Cf Self-Activated Memory for Evidential Reasoning.},
  booktitle = {Proceedings of the 7th {{Conference}} of the {{Cognitive Science Society}}},
  author = {{Judea Pearl}},
  year = {1985},
  address = {{University of California, Irvine, CA, USA}}
}

@article{kadurinCornucopiaMeaningfulLeads2017,
  ids = {Kadurin2017},
  title = {The Cornucopia of Meaningful Leads: {{Applying}} Deep Adversarial Autoencoders for New Molecule Development in Oncology},
  shorttitle = {The Cornucopia of Meaningful Leads},
  author = {Kadurin, Artur and Aliper, Alexander and Kazennov, Andrey and Mamoshina, Polina and Vanhaelen, Quentin and Khrabrov, Kuzma and Zhavoronkov, Alex},
  year = {2017},
  volume = {8},
  pages = {10883--10890},
  issn = {19492553},
  doi = {10.18632/oncotarget.14073},
  abstract = {Recent advances in deep learning and specifically in generative adversarial networks have demonstrated surprising results in generating new images and videos upon request even using natural language as input. In this paper we present the first application of generative adversarial autoencoders (AAE) for generating novel molecular fingerprints with a defined set of parameters. We developed a 7-layer AAE architecture with the latent middle layer serving as a discriminator. As an input and output the AAE uses a vector of binary fingerprints and concentration of the molecule. In the latent layer we also introduced a neuron responsible for growth inhibition percentage, which when negative indicates the reduction in the number of tumor cells after the treatment. To train the AAE we used the NCI-60 cell line assay data for 6252 compounds profiled on MCF-7 cell line. The output of the AAE was used to screen 72 million compounds in PubChem and select candidate molecules with potential anticancer properties. This approach is a proof of concept of an artificially-intelligent drug discovery engine, where AAEs are used to generate new molecular fingerprints with the desired molecular properties.},
  file = {/home/harrisonpl/Documents/PDFs/Kadurin et al. - 2017 - The cornucopia of meaningful leads.pdf},
  journal = {Oncotarget},
  keywords = {Adversarial autoencoder,Artificial intelligence,Deep learning,Drug discovery,Generative adversarian networks,hunter lab},
  mendeley-tags = {hunter lab},
  number = {7}
}

@phdthesis{kanwalConstructionBiologicalAssociation2019,
  ids = {Kanwal2019},
  title = {Construction {{Of Biological Association Network For Connected Components Analysis Using Graph Theory}}},
  author = {Kanwal, Attiya},
  year = {2019},
  abstract = {The concept of big data has been around for years. Big data analysis approaches can play an imperative part in health research. This can be a helpful asset for researchers because it can reveal veiled acquaintance from a massive sum of data. In order to get insights of metabolic pathways at molecular level there is a need to present a unit framework model that serves as the cutting edge technology of sys- tem biology. In this thesis data mining techniques were used to extract data from online databases automatically by developing a tool (PubMed Info Extractor). On inputing queries, relevant information about the association and interactions among biological entities have been found in the retrieved collection. The ap- proach has been applied on the case study of T2DM. Cell boundaries of the found gene components were also identified. Only those components were selected that show highly expressed genes in the pancreatic endocrine cells and skeletal muscle cell for determining the disrupted pathway after analyzing the normal functional pathway of T2DM. Further, bioinformatics tools were used for the topological analysis of obtained patterns. From a network generated by using the association rule mining technique showing associations, their nature of relationship, seven strongly connected components have been identified. These components represent P53, HNF1Alpha, HNF1Beta, INSR, INS, IL-6 and GnRH as the regulators or initiators of seven different biological regulatory pathways. This research is an evidence for the association of T2DM with the genes that are involved in different pathways of cancer cell metabolism, growth regulation, proliferation control etc along with insulin signaling pathway, mTOR pathway, MODY pathway, glycolysis, lipid homeostasis, Age-rage signaling pathway, MAPK pathway, p53 pathway. Self inhibition of ngn3 is also acknowledged in these components. In diabetic patients, pancreatic islets in case of fasting lessen PKA and mTOR activity and induce Sox2 and Ngn3 expression and insulin production. Self inhibition of Ngn3 can there- fore affect the insulin production. This research gives a unit framework model of system biology which gives better understanding of intrinsic disease mechanism. This research regarding T2DM which will facilitate the researchers to compre- hend the system of T2DM disease mechanism and how to cure it with respect to personalized drugs.},
  file = {/home/harrisonpl/Documents/PDFs/Kanwal - 2019 - Construction Of Biological Association Network For Connected Components.pdf},
  school = {CAPITAL UNIVERSITY OF SCIENCE AND TECHNOLOGY},
  type = {{{PhD Thesis}}}
}

@article{kazakovELKReasonerArchitecture,
  title = {{{ELK Reasoner}}: {{Architecture}} and {{Evaluation}}},
  author = {Kazakov, Yevgeny and Kr{\"o}tzsch, Markus and Siman{\v c}{\'i}k, Franti{\v s}ek},
  pages = {12},
  abstract = {ELK is a specialized reasoner for the lightweight ontology language OWL EL. The practical utility of ELK is in its combination of high performance and comprehensive support for language features. At its core, ELK employs a consequence-based reasoning engine that can take advantage of multi-core and multi-processor systems. A modular architecture allows ELK to be used as a stand-alone application, Prot\'eg\'e plug-in, or programming library (either with or without the OWL API). This system description presents the current state of ELK and experimental results with some difficult OWL EL ontologies.},
  file = {/home/harrisonpl/Zotero/storage/28WQU6L4/KazKroSim12ELK_ORE.pdf},
  language = {en}
}

@article{keskarCTRLConditionalTransformer2019,
  ids = {Keskar2019},
  title = {{{CTRL}}: {{A Conditional Transformer Language Model}} for {{Controllable Generation}}},
  author = {Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav R. and Xiong, Caiming and Socher, Richard},
  year = {2019},
  pages = {1--18},
  abstract = {Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release CTRL, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow CTRL to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of CTRL at https://github.com/salesforce/ctrl.},
  annotation = {\_eprint: 1909.05858},
  archivePrefix = {arXiv},
  arxivid = {1909.05858},
  eprint = {1909.05858},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Keskar et al. - 2019 - CTRL.pdf},
  keywords = {Attention,Favorite,machine learning,Transformer},
  mendeley-tags = {Attention,Transformer,machine learning}
}

@article{kimGREGGlobalLevel2020,
  ids = {Kim2020},
  title = {{{GREG}}: {{A Global Level Relation Extraction}} with {{Knowledge Graph Embedding}}},
  author = {Kim, Kuekyeng and Hur, Yuna and Kim, Gyeongmin and Lim, Heuiseok},
  year = {2020},
  volume = {10},
  pages = {1181},
  doi = {10.3390/app10031181},
  abstract = {In an age overflowing with information, the task of converting unstructured data into structured data are a vital task of great need. Currently, most relation extraction modules are more focused on the extraction of local mention-level relations\textemdash usually from short volumes of text. However, in most cases, the most vital and important relations are those that are described in length and detail. In this research, we propose GREG: A Global level Relation Extractor model using knowledge graph embeddings for document-level inputs. The model uses vector representations of mention-level `local' relation's to construct knowledge graphs that can represent the input document. The knowledge graph is then used to predict global level relations from documents or large bodies of text. The proposed model is largely divided into two modules which are synchronized during their training. Thus, each of the model's modules is designed to deal with local relations and global relations separately. This allows the model to avoid the problem of struggling against loss of information due to too much information crunched into smaller sized representations when attempting global level relation extraction. Through evaluation, we have shown that the proposed model yields high performances in both predicting global level relations and local level relations consistently.},
  file = {/home/harrisonpl/Documents/PDFs/Kim et al. - 2020 - GREG.pdf},
  journal = {Applied Sciences},
  keywords = {knowledge graph,language processing,machine learning,meta learning,natural,relation extraction,text summarization},
  number = {3}
}

@article{kitaevReformerEfficientTransformer2020,
  ids = {Kitaev2020},
  title = {Reformer: {{The Efficient Transformer}}},
  author = {Kitaev, Nikita and Kaiser, {\textbackslash}Lukasz and Levskaya, Anselm},
  year = {2020},
  month = jan,
  abstract = {Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(\$L\^2\$) to O(\$L\textbackslash backslashlog L\$), where \$L\$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of \$N\$ times, where \$N\$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.},
  annotation = {\_eprint: 2001.04451},
  archivePrefix = {arXiv},
  arxivid = {2001.04451},
  eprint = {2001.04451},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Kitaev et al. - 2020 - Reformer.pdf}
}

@article{KnowledgebaseenrichedRelationExtraction2017,
  ids = {KnowledgebaseenrichedRelationExtraction2017a},
  title = {Knowledge-Base-Enriched\mbox{} \mbox Relation\mbox{} \mbox Extraction},
  year = {2017},
  month = oct,
  volume = {1},
  pages = {163--6},
  file = {/home/harrisonpl/Documents/PDFs/2017 - Knowledge-base-enriched relation extraction.pdf},
  journal = {BioCreative VI},
  keywords = {HPL comprehensive exam},
  mendeley-tags = {HPL comprehensive exam}
}

@inproceedings{KnowledgebaseenrichedRelationExtraction2017a,
  title = {Knowledge-Base-Enriched Relation Extraction},
  year = {2017},
  volume = {1},
  pages = {163--6},
  file = {/home/harrisonpl/Documents/PDFs/2017 - Knowledge-base-enriched relation extraction.pdf},
  keywords = {comp-exam},
  mendeley-tags = {comp-exam}
}

@article{kosticGeneralTheoryTopological2020,
  ids = {Kostic2020,Kostic2020a},
  title = {General Theory of Topological Explanations and Explanatory Asymmetry},
  author = {Kosti{\'c}, Daniel},
  year = {2020},
  volume = {375},
  pages = {20190321},
  issn = {0962-8436},
  doi = {10.1098/rstb.2019.0321},
  abstract = {In this paper, I present a general theory of topological explanations, and illustrate its fruitfulness by showing how it accounts for explanatory asymmetry. My argument is developed in three steps....},
  file = {/home/harrisonpl/Documents/PDFs/Kostić - 2020 - General theory of topological explanations and explanatory asymmetry.pdf},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  keywords = {comp-exam,counterfactual dependencies,explanatory asymmetries,explanatory perspectivism,explanatory unification,facticity of explanation,HPL comprehensive exam,philosophical theory of explanation},
  mendeley-tags = {HPL comprehensive exam},
  number = {1796}
}

@article{kosticUnifyingEssentialConcepts2020,
  ids = {1394,Kostic,Kostic2020b,Kostica},
  title = {Unifying the Essential Concepts of Biological Networks: Biological Insights and Philosophical Foundations},
  author = {Kosti{\'c}, Daniel and Hilgetag, Claus C. and Tittgemeyer, Marc},
  year = {2020},
  month = apr,
  volume = {375},
  pages = {20190314},
  issn = {0962-8436},
  doi = {10.1098/rstb.2019.0314},
  abstract = {Over the last decades, network-based approaches have become highly popular in diverse fields of biology, including neuroscience, ecology, molecular biology and genetics. While these approaches continue to grow very rapidly, some of their conceptual and methodological aspects still require a programmatic foundation. This challenge particularly concerns the question of whether a generalized account of explanatory, organizational and descriptive levels of networks can be applied universally across biological sciences. To this end, this highly interdisciplinary theme issue focuses on the definition, motivation and application of key concepts in biological network science, such as explanatory power of distinctively network explanations, network levels and network hierarchies.},
  file = {/home/harrisonpl/Documents/PDFs/Kostić et al. - 2020 - Unifying the essential concepts of biological networks.pdf},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  keywords = {behaviour,ecology,Favorite,neuroscience},
  number = {1796}
}

@article{koziolCommentsChoosingOptimal2009,
  ids = {Koziol2009},
  title = {Comments on '{{Choosing}} an Optimal Method to Combine {{P}}-Values'},
  author = {Koziol, J. A.},
  year = {2009},
  volume = {28},
  pages = {3043--3045},
  issn = {02776715},
  doi = {10.1002/sim.3690},
  file = {/home/harrisonpl/Documents/PDFs/Koziol - 2009 - Comments on 'Choosing an optimal method to combine P-values'.pdf},
  journal = {Statistics in Medicine},
  keywords = {effect size,fisher,liptak},
  number = {24}
}

@article{krallingerHowLinkOntologies2012,
  ids = {Krallinger2012,Krallinger2012a,Krallinger2012b},
  title = {How to Link Ontologies and Protein-Protein Interactions to Literature: {{Text}}-Mining Approaches and the {{BioCreative}} Experience},
  author = {Krallinger, Martin and Leitner, Florian and Vazquez, Miguel and Salgado, David and Marcelle, Christophe and Tyers, Mike and Valencia, Alfonso and {Chatr-aryamontri}, Andrew},
  year = {2012},
  volume = {2012},
  pages = {1--12},
  issn = {17580463},
  doi = {10.1093/database/bas017},
  abstract = {There is an increasing interest in developing ontologies and controlled vocabularies to improve the efficiency and consistency of manual literature curation, to enable more formal biocuration workflow results and ultimately to improve analysis of biological data. Two ontologies that have been successfully used for this purpose are the Gene Ontology (GO) for annotating aspects of gene products and the Molecular Interaction ontology (PSI-MI) used by databases that archive protein-protein interactions. The examination of protein interactions has proven to be extremely promising for the understanding of cellular processes. Manual mapping of information from the biomedical literature to bio-ontology terms is one of the most challenging components in the curation pipeline. It requires that expert curators interpret the natural language descriptions contained in articles and infer their semantic equivalents in the ontology (controlled vocabulary). Since manual curation is a time-consuming process, there is strong motivation to implement text-mining techniques to automatically extract annotations from free text. A range of text mining strategies has been devised to assist in the automated extraction of biological data. These strategies either recognize technical terms used recurrently in the literature and propose them as candidates for inclusion in ontologies, or retrieve passages that serve as evidential support for annotating an ontology term, e.g. from the PSI-MI or GO controlled vocabularies. Here, we provide a general overview of current text-mining methods to automatically extract annotations of GO and PSI-MI ontology terms in the context of the BioCreative (Critical Assessment of Information Extraction Systems in Biology) challenge. Special emphasis is given to protein-protein interaction data and PSI-MI terms referring to interaction detection methods. \textcopyright{} The Author(s) 2012.},
  file = {/home/harrisonpl/Documents/PDFs/Krallinger et al. - 2012 - How to link ontologies and protein-protein interactions to literature.pdf},
  journal = {Database},
  keywords = {hunter lab},
  mendeley-tags = {hunter lab}
}

@article{kramerCausalAnalysisApproaches2014,
  ids = {Green2014,Kramer2014,Kramer2014b},
  title = {Causal Analysis Approaches in Ingenuity Pathway Analysis},
  author = {Kr{\"a}mer, Andreas and Green, Jeff and Pollard, Jack and Tugendreich, Stuart},
  year = {2014},
  volume = {30},
  pages = {523--530},
  issn = {13674803},
  doi = {10.1093/bioinformatics/btt703},
  abstract = {Motivation: Prior biological knowledge greatly facilitates the meaningful interpretation of gene-expression data. Causal networks constructed from individual relationships curated from the literature are particularly suited for this task, since they create mechanistic hypotheses that explain the expression changes observed in datasets.Results: We present and discuss a suite of algorithms and tools for inferring and scoring regulator networks upstream of gene-expression data based on a large-scale causal network derived from the Ingenuity Knowledge Base. We extend the method to predict downstream effects on biological functions and diseases and demonstrate the validity of our approach by applying it to example datasets.Availability: The causal analytics tools 'Upstream Regulator Analysis', 'Mechanistic Networks', 'Causal Network Analysis' and 'Downstream Effects Analysis' are implemented and available within Ingenuity Pathway Analysis (IPA, http://www.ingenuity.com).Supplementary information: Supplementary material is available at Bioinformatics online. \textcopyright{} 2013 The Author.},
  file = {/home/harrisonpl/Documents/PDFs/Krämer et al. - 2014 - Causal analysis approaches in ingenuity pathway analysis.pdf},
  journal = {Bioinformatics},
  keywords = {comp-exam,HPL comprehensive exam},
  mendeley-tags = {comp-exam},
  number = {4},
  pmid = {24336805}
}

@article{kuangImprovingNeuralRelation2019,
  ids = {Kuang2019},
  title = {Improving {{Neural Relation Extraction}} with {{Implicit Mutual Relations}}},
  author = {Kuang, Jun and Cao, Yixin and Zheng, Jianbing and He, Xiangnan and Gao, Ming and Zhou, Aoying},
  year = {2019},
  abstract = {Relation extraction (RE) aims at extracting the relation between two entities from the text corpora. It is a crucial task for Knowledge Graph (KG) construction. Most existing methods predict the relation between an entity pair by learning the relation from the training sentences, which contain the targeted entity pair. In contrast to existing distant supervision approaches that suffer from insufficient training corpora to extract relations, our proposal of mining implicit mutual relation from the massive unlabeled corpora transfers the semantic information of entity pairs into the RE model, which is more expressive and semantically plausible. After constructing an entity proximity graph based on the implicit mutual relations, we preserve the semantic relations of entity pairs via embedding each vertex of the graph into a low-dimensional space. As a result, we can easily and flexibly integrate the implicit mutual relations and other entity information, such as entity types, into the existing RE methods. Our experimental results on a New York Times and another Google Distant Supervision datasets suggest that our proposed neural RE framework provides a promising improvement for the RE task, and significantly outperforms the state-of-the-art methods. Moreover, the component for mining implicit mutual relations is so flexible that can help to improve the performance of both CNN-based and RNN-based RE models significant.},
  annotation = {\_eprint: 1907.05333},
  archivePrefix = {arXiv},
  arxivid = {1907.05333},
  eprint = {1907.05333},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Kuang et al. - 2019 - Improving Neural Relation Extraction with Implicit Mutual Relations.pdf}
}

@article{kuehneSEQLCategoryLearning2000,
  ids = {Kuehne2000,SEQLCategoryLearning2000},
  title = {{{SEQL}}: {{Category}} Learning as Progressive Abstraction Using Structure Mapping},
  shorttitle = {{{SEQL}}},
  author = {Kuehne, Sven Erik and Forbus, Kenneth D and Gentner, Dedre and {kuehne.proccogsci.22.2000.770 Quinn}, Bryan L B -},
  year = {2000},
  abstract = {The nature of categories and their acquisition is one of the central open questions in Cognitive Science. We suggest that categories are represented via structured descriptions and formed by a process of progressive abstraction, through successive comparison with incoming exemplars. This paper describes how SEQL (Skorstad, Gentner, \& Medin, 1988), a computer model for category learning, which is based on SME (Falkenhainer et al 1986, 1989; Forbus et al 1994) can be used to simulate a recent categorization experiment (Ramscar \& Pain, 1996), using a new algorithm, Generalization and Exemplar Learning (GEL). We demonstrate that SEQL produces behavior consistent with human subjects.},
  file = {/home/harrisonpl/Documents/PDFs/Kuehne et al. - 2000 - SEQL.pdf},
  journal = {22nd Annual Meeting of the Cognitive Science Society},
  keywords = {comp-exam,HPL comprehensive exam},
  mendeley-tags = {comp-exam}
}

@article{kumarSystematicAnalysisDrug2016,
  ids = {Kumar2016},
  title = {Systematic {{Analysis}} of {{Drug Targets Confirms Expression}} in {{Disease}}-{{Relevant Tissues}}},
  author = {Kumar, Vinod and Sanseau, Philippe and Simola, Daniel F. and Hurle, Mark R. and Agarwal, Pankaj},
  year = {2016},
  volume = {6},
  pages = {36205},
  issn = {20452322},
  doi = {10.1038/srep36205},
  abstract = {It is commonly assumed that drug targets are expressed in tissues relevant to their indicated diseases, even under normal conditions. While multiple anecdotal cases support this hypothesis, a comprehensive study has not been performed to verify it. We conducted a systematic analysis to assess gene and protein expression for all targets of marketed and phase III drugs across a diverse collection of normal human tissues. For 87\% of gene-disease pairs, the target is expressed in a disease-affected tissue under healthy conditions. This result validates the importance of confirming expression of a novel drug target in an appropriate tissue for each disease indication and strengthens previous findings showing that targets of efficacious drugs should be expressed in relevant tissues under normal conditions. Further characterization of the remaining 13\% of gene-disease pairs revealed that most genes are expressed in a different tissue linked to another disease. Our analysis demonstrates the value of extensive tissue specific expression resources.both in terms of tissue and cell diversity as well as techniques used to measure gene expression.},
  file = {/home/harrisonpl/Documents/PDFs/Kumar et al. - 2016 - Systematic Analysis of Drug Targets Confirms Expression in Disease-Relevant.pdf},
  journal = {Scientific Reports},
  keywords = {Lawrence Hunter,Recommendation},
  mendeley-tags = {Lawrence Hunter,Recommendation}
}

@article{lakiotakiDataDrivenApproach2019,
  ids = {Lakiotaki2019},
  title = {A Data Driven Approach Reveals Disease Similarity on a Molecular Level},
  author = {Lakiotaki, Kleanthi and Georgakopoulos, George and Castanas, Elias and R{\o}e, Oluf Dimitri and Borboudakis, Giorgos and Tsamardinos, Ioannis},
  year = {2019},
  volume = {5},
  publisher = {{Springer US}},
  issn = {20567189},
  doi = {10.1038/s41540-019-0117-0},
  abstract = {Could there be unexpected similarities between different studies, diseases, or treatments, on a molecular level due to common biological mechanisms involved? To answer this question, we develop a method for computing similarities between empirical, statistical distributions of high-dimensional, low-sample datasets, and apply it on hundreds of -omics studies. The similarities lead to dataset-to-dataset networks visualizing the landscape of a large portion of biological data. Potentially interesting similarities connecting studies of different diseases are assembled in a disease-to-disease network. Exploring it, we discover numerous non-trivial connections between Alzheimer's disease and schizophrenia, asthma and psoriasis, or liver cancer and obesity, to name a few. We then present a method that identifies the molecular quantities and pathways that contribute the most to the identified similarities and could point to novel drug targets or provide biological insights. The proposed method acts as a ``statistical telescope'' providing a global view of the constellation of biological data; readers can peek through it at: http://datascope.csd.uoc.gr:25000/.},
  file = {/home/harrisonpl/Documents/PDFs/Lakiotaki et al. - 2019 - A data driven approach reveals disease similarity on a molecular level.pdf},
  journal = {npj Systems Biology and Applications},
  number = {1}
}

@article{lampleDeepLearningSymbolic2019,
  ids = {Lample2019},
  title = {Deep {{Learning}} for {{Symbolic Mathematics}}},
  author = {Lample, Guillaume and Charton, Fran{\c c}ois},
  year = {2019},
  month = dec,
  pages = {1--24},
  abstract = {Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.},
  annotation = {\_eprint: 1912.01412},
  archivePrefix = {arXiv},
  arxivid = {1912.01412},
  eprint = {1912.01412},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Lample, Charton - 2019 - Deep Learning for Symbolic Mathematics.pdf}
}

@article{landInsights20Years2015,
  ids = {Land2015},
  title = {Insights from 20 Years of Bacterial Genome Sequencing},
  author = {Land, Miriam and Hauser, Loren and Jun, Se Ran and Nookaew, Intawat and Leuze, Michael R. and Ahn, Tae Hyuk and Karpinets, Tatiana and Lund, Ole and Kora, Guruprased and Wassenaar, Trudy and Poudel, Suresh and Ussery, David W.},
  year = {2015},
  volume = {15},
  pages = {141--161},
  issn = {14387948},
  doi = {10.1007/s10142-015-0433-4},
  abstract = {Since the first two complete bacterial genome sequences were published in 1995, the science of bacteria has dramatically changed. Using third-generation DNA sequencing, it is possible to completely sequence a bacterial genome in a few hours and identify some types of methylation sites along the genome as well. Sequencing of bacterial genome sequences is now a standard procedure, and the information from tens of thousands of bacterial genomes has had a major impact on our views of the bacterial world. In this review, we explore a series of questions to highlight some insights that comparative genomics has produced. To date, there are genome sequences available from 50 different bacterial phyla and 11 different archaeal phyla. However, the distribution is quite skewed towards a few phyla that contain model organisms. But the breadth is continuing to improve, with projects dedicated to filling in less characterized taxonomic groups. The clustered regularly interspaced short palindromic repeats (CRISPR)-Cas system provides bacteria with immunity against viruses, which outnumber bacteria by tenfold. How fast can we go? Second-generation sequencing has produced a large number of draft genomes (close to 90 \% of bacterial genomes in GenBank are currently not complete); third-generation sequencing can potentially produce a finished genome in a few hours, and at the same time provide methlylation sites along the entire chromosome. The diversity of bacterial communities is extensive as is evident from the genome sequences available from 50 different bacterial phyla and 11 different archaeal phyla. Genome sequencing can help in classifying an organism, and in the case where multiple genomes of the same species are available, it is possible to calculate the pan- and core genomes; comparison of more than 2000 Escherichia coli genomes finds an E. coli core genome of about 3100 gene families and a total of about 89,000 different gene families. Why do we care about bacterial genome sequencing? There are many practical applications, such as genome-scale metabolic modeling, biosurveillance, bioforensics, and infectious disease epidemiology. In the near future, high-throughput sequencing of patient metagenomic samples could revolutionize medicine in terms of speed and accuracy of finding pathogens and knowing how to treat them.},
  file = {/home/harrisonpl/Documents/PDFs/Land et al. - 2015 - Insights from 20 years of bacterial genome sequencing.pdf},
  journal = {Functional and Integrative Genomics},
  keywords = {Bacteria,Bacterial genomes,Comparative genomics,Core-genome,Metagenomics,Next-generation sequencing,Pan-genome},
  number = {2},
  pmid = {25722247}
}

@article{larkeyCABConnectionistAnalogy2003,
  ids = {Larkey2003},
  title = {{{CAB}}: {{Connectionist Analogy Builder}}},
  shorttitle = {{{CAB}}},
  author = {Larkey, Levi B. and Love, Bradley C.},
  year = {2003},
  volume = {27},
  pages = {781--794},
  issn = {1551-6709},
  doi = {10.1207/s15516709cog2705_5},
  abstract = {The ability to make informative comparisons is central to human cognition. Comparison involves aligning two representations and placing their elements into correspondence. Detecting correspondences is a necessary component of analogical inference, recognition, categorization, schema formation, and similarity judgment. Connectionist Analogy Builder (CAB) determines correspondences through a simple iterative computation that matches elements in one representation with elements playing compatible roles in the other representation while simultaneously enforcing structural constraints. CAB shows promise as a process model of comparison as its performance can be related to human performance (e.g., solution trajectory, error patterns, time-on-task). Furthermore, CAB's bounded working memory allows it to account for the inherent capacity limitations of human processing. CAB's strengths are its parsimony, transparency of operations, and ability to generate performance predictions. In this paper, CAB is evaluated against benchmark phenomena from the analogy literature.},
  file = {/home/harrisonpl/Documents/PDFs/Larkey, Love - 2003 - CAB.pdf},
  journal = {Cognitive Science},
  keywords = {Analogy,Artificial Intelligence,Cognitive modeling,Connectionism,HPL comprehensive exam,Knowledge representation,Psychology},
  language = {en},
  mendeley-tags = {Analogy,Artificial Intelligence,Cognitive modeling,Connectionism,HPL comprehensive exam,Knowledge representation,Psychology},
  number = {5}
}

@article{leekCapturingHeterogeneityGene2007,
  ids = {Leek2007},
  title = {Capturing Heterogeneity in Gene Expression Studies by Surrogate Variable Analysis},
  author = {Leek, Jeffrey T. and Storey, John D.},
  year = {2007},
  volume = {3},
  pages = {1724--1735},
  issn = {15537390},
  doi = {10.1371/journal.pgen.0030161},
  abstract = {It has unambiguously been shown that genetic, environmental, demographic, and technical factors may have substantial effects on gene expression levels. In addition to the measured variable(s) of interest, there will tend to be sources of signal due to factors that are unknown, unmeasured, or too complicated to capture through simple models. We show that failing to incorporate these sources of heterogeneity into an analysis can have widespread and detrimental effects on the study. Not only can this reduce power or induce unwanted dependence across genes, but it can also introduce sources of spurious signal to many genes. This phenomenon is true even for well-designed, randomized studies. We introduce "surrogate variable analysis" (SVA) to overcome the problems caused by heterogeneity in expression studies. SVA can be applied in conjunction with standard analysis techniques to accurately capture the relationship between expression and any modeled variables of interest. We apply SVA to disease class, time course, and genetics of gene expression studies. We show that SVA increases the biological accuracy and reproducibility of analyses in genome-wide expression studies. \textcopyright{} 2007 Leek and Storey.},
  file = {/home/harrisonpl/Documents/PDFs/Leek, Storey - 2007 - Capturing heterogeneity in gene expression studies by surrogate variable.pdf},
  journal = {PLoS Genetics},
  keywords = {Algorithms,Gene Expression,Genetic loci,Genetics of disease,Genomic signal processing,Microarrays,Trait locus analysis,Vector spaces},
  number = {9}
}

@article{leeNovelPhenotypicOutcomes2015,
  ids = {Lee2015},
  title = {Novel Phenotypic Outcomes Identified for a Public Collection of Approved Drugs from a Publicly Accessible Panel of Assays},
  author = {Lee, Jonathan A. and Shinn, Paul and Jaken, Susan and Oliver, Sarah and Willard, Francis S. and Heidler, Steven and Peery, Robert B. and Oler, Jennifer and Chu, Shaoyou and Southall, Noel and Dexheimer, Thomas S. and Smallwood, Jeffrey and Huang, Ruili and Guha, Rajarshi and Jadhav, Ajit and Cox, Karen and Austin, Christopher P. and Simeonov, Anton and Sittampalam, G. Sitta and Husain, Saba and Franklin, Natalie and Wild, David J. and Yang, Jeremy J. and Sutherland, Jeffrey J. and Thomas, Craig J.},
  year = {2015},
  volume = {10},
  pages = {e0130796},
  issn = {19326203},
  doi = {10.1371/journal.pone.0130796},
  abstract = {Phenotypic assays have a proven track record for generating leads that become first-inclass therapies. Whole cell assays that inform on a phenotype or mechanism also possess great potential in drug repositioning studies by illuminating new activities for the existing pharmacopeia. The National Center for Advancing Translational Sciences (NCATS) pharmaceutical collection (NPC) is the largest reported collection of approved small molecule therapeutics that is available for screening in a high-throughput setting. Via a wide-ranging collaborative effort, this library was analyzed in the Open Innovation Drug Discovery (OIDD) phenotypic assay modules publicly offered by Lilly. The results of these tests are publically available online at www.ncats.nih.gov/expertise/preclinical/pd2 and via the PubChem Database (https://pubchem.ncbi.nlm.nih.gov/) (AID 1117321). Phenotypic outcomes for numerous drugs were confirmed, including sulfonylureas as insulin secretagogues and the antiangiogenesis actions of multikinase inhibitors sorafenib, axitinib and pazopanib. Several novel outcomes were also noted including the Wnt potentiating activities of rotenone and the antifolate class of drugs, and the anti-angiogenic activity of cetaben.},
  file = {/home/harrisonpl/Documents/PDFs/Lee et al. - 2015 - Novel phenotypic outcomes identified for a public collection of approved drugs.pdf},
  journal = {PLoS ONE},
  keywords = {Angiogenesis,Drug Discovery,Drug research and development,Insulin secretion,Library screening,MAPK signaling cascades,Small molecules,Wnt signaling cascade},
  number = {7}
}

@article{leeProteomicsNaturalBacterial2018,
  ids = {Lee2018},
  title = {Proteomics of Natural Bacterial Isolates Powered by Deep Learning-Based de Novo Identification},
  author = {Lee, Joon-Yong and Mitchell, Hugh D and Burnet, Meagan C and Jenson, Sarah C and Merkley, Eric D and Shukla, Anil K and Nakayasu, Ernesto S and Payne, Samuel},
  year = {2018},
  month = sep,
  pages = {428334},
  publisher = {{Cold Spring Harbor Laboratory}},
  issn = {0001-1452},
  doi = {10.1101/428334},
  abstract = {The fundamental task in proteomic mass spectrometry is identifying peptides from their observed spectra. Where protein sequences are known, standard algorithms utilize these to narrow the list of peptide candidates. If protein sequences are unknown, a distinct class of algorithms must interpret spectra de novo. Despite decades of effort on algorithmic constructs and machine learning methods, de novo software tools remain inaccurate when used on environmentally diverse samples. Here we train a deep neural network on 5 million spectra from 55 phylogenetically diverse bacteria. This new model outperforms current methods by 25-100\%. The diversity of organisms used for training also improves the generality of the model, and ensures reliable performance regardless of where the sample comes from. Significantly, it also achieves a high accuracy in long peptides which assist in identifying taxa from samples of unknown origin. With the new tool, called Kaiko, we analyze proteomics data from six natural soil isolates for which a proteome database did not exist. Without any sequence information, we correctly identify the taxonomy of these soil microbes as well as annotate thousands of peptide spectra.},
  file = {/home/harrisonpl/Documents/PDFs/Lee et al. - 2018 - Proteomics of natural bacterial isolates powered by deep learning-based de novo.pdf},
  journal = {bioRxiv}
}

@misc{leonktLeonktZoteromemento2020,
  title = {Leonkt/Zotero-Memento},
  author = {{leonkt}},
  year = {2020},
  month = mar,
  abstract = {Zotero extension that combats link rot by archiving webpages and journal articles.},
  copyright = {MIT},
  keywords = {\#nosource}
}

@article{levyAbstractionOrganizationMechanisms2013a,
  title = {Abstraction and the {{Organization}} of {{Mechanisms}}},
  author = {Levy, Arnon and Bechtel, William},
  year = {2013},
  month = apr,
  volume = {80},
  pages = {241--261},
  publisher = {{The University of Chicago Press}},
  issn = {0031-8248},
  doi = {10.1086/670300},
  abstract = {Proponents of mechanistic explanation all acknowledge the importance of organization. But they have also tended to emphasize specificity with respect to parts and operations in mechanisms. We argue that in understanding one important mode of organization\textemdash patterns of causal connectivity\textemdash a successful explanatory strategy abstracts from the specifics of the mechanism and invokes tools such as those of graph theory to explain how mechanisms with a particular mode of connectivity will behave. We discuss the connection between organization, abstraction, and mechanistic explanation and illustrate our claims by looking at an example from recent research on so-called network motifs.},
  file = {/home/harrisonpl/Documents/PDFs/Levy, Bechtel - 2013 - Abstraction and the Organization of Mechanisms.pdf;/home/harrisonpl/Zotero/storage/ARJT96JC/670300.html},
  journal = {Philosophy of Science},
  number = {2}
}

@article{levyWorkingScientistPodcast2020,
  title = {Working {{Scientist}} Podcast: {{How}} to Write a Top-Notch Paper},
  author = {Levy, Adam},
  year = {2020},
  abstract = {Four researchers share the highs and lows of getting published for the first time, and what the experience taught them. Four researchers share the highs and lows of getting published for the first time, and what the experience taught them.},
  journal = {Nature 2020},
  keywords = {\#nosource,Careers}
}

@article{leyEcologicalEvolutionaryForces2006,
  ids = {Ley,Ley2006,Ley2006a},
  title = {Ecological and Evolutionary Forces Shaping Microbial Diversity in the Human Intestine},
  author = {Ley, Ruth E. and Peterson, Daniel A. and Gordon, Jeffrey I.},
  year = {2006},
  volume = {124},
  pages = {837--848},
  issn = {00928674},
  doi = {10.1016/j.cell.2006.02.017},
  abstract = {The human gut is populated with as many as 100 trillion cells, whose collective genome, the microbiome, is a reflection of evolutionary selection pressures acting at the level of the host and at the level of the microbial cell. The ecological rules that govern the shape of microbial diversity in the gut apply to mutualists and pathogens alike. \textcopyright 2006 Elsevier Inc.},
  file = {/home/harrisonpl/Documents/PDFs/Ley et al. - 2006 - Ecological and evolutionary forces shaping microbial diversity in the human.pdf},
  journal = {Cell},
  number = {4}
}

@article{liaoMoABankIntegratedDatabase2019,
  title = {{{MoABank}}: {{An Integrated Database}} for {{Drug Mode}} of {{Action Knowledge}}},
  shorttitle = {{{MoABank}}},
  author = {Liao, Yu-di and Jiang, Zhen-ran},
  year = {2019},
  volume = {14},
  pages = {446--449},
  issn = {15748936},
  doi = {10.2174/1574893614666190416151344},
  abstract = {\textcopyright{} 2019 Bentham Science Publishers. Background: With the declining trend of new drugs yield each year, more comprehensive knowledge of drug MoAs can help identify new applications of available drugs and discovery novel mechanism of drug action. Objective: Therefore, construction of a specialized drug mode of action (MoA) database is of paramount importance for new drug research \& development. Methods: This paper introduces an integrated database for drug mode of action knowledge (MoABank). Results: This database can provide the knowledge about drug MoAs, targets, pathways, side effects and other drug-related information for researchers. Conclusion: We believe MoABank can make it more convenient for users to obtain the drug MoA information in the future.},
  journal = {Current Bioinformatics},
  keywords = {\#nosource,HPL comprehensive exam},
  language = {en},
  number = {5}
}

@article{liDualCNNRelation2019,
  ids = {Li2019b},
  title = {Dual {{CNN}} for Relation Extraction with Knowledge-Based Attention and Word Embeddings},
  author = {Li, Jun and Huang, Guimin and Chen, Jianheng and Wang, Yabing},
  year = {2019},
  volume = {2019},
  issn = {16875273},
  doi = {10.1155/2019/6789520},
  abstract = {Relation extraction is the underlying critical task of textual understanding. However, the existing methods currently have defects in instance selection and lack background knowledge for entity recognition. In this paper, we propose a knowledge-based attention model, which can make full use of supervised information from a knowledge base, to select an entity. We also design a method of dual convolutional neural networks (CNNs) considering the word embedding of each word is restricted by using a single training tool. The proposed model combines a CNN with an attention mechanism. The model inserts the word embedding and supervised information from the knowledge base into the CNN, performs convolution and pooling, and combines the knowledge base and CNN in the full connection layer. Based on these processes, the model not only obtains better entity representations but also improves the performance of relation extraction with the help of rich background knowledge. The experimental results demonstrate that the proposed model achieves competitive performance.},
  file = {/home/harrisonpl/Documents/PDFs/Li et al. - 2019 - Dual CNN for relation extraction with knowledge-based attention and word.pdf},
  journal = {Computational Intelligence and Neuroscience}
}

@article{liebermanAlpha1antitrypsinPitypes9651986,
  title = {Alpha1-Antitrypsin {{Pi}}-Types in 965 {{COPD}} Patients},
  author = {Lieberman, J. and Winter, B. and Sastre, A.},
  year = {1986},
  volume = {89},
  pages = {370--373},
  issn = {00123692},
  doi = {10.1378/chest.89.3.370},
  abstract = {To study further the role of intermediate alpha1-antitrypsin (AAT) deficiency in chronic obstructive pulmonary disease (COPD), AAT Pi-types and serum-trypsin-inhibitory-capacity (STIC) were measured in 965 patients with COPD. Heterozygosity of the Z variant was the major cause of intermediate AAT deficiency (primarily the MZ phenotype), accounting for 8.0 percent of the patients compared to 2.9 percent of control subjects (p \textbackslash textless .0005). ZZ homozygosity was detected in 1.9 percent of the patients, compared to 0.04 percent of control studies pedrformed by others (non was present in our own control group of 1,380 subjects). The mean age for MS or MZ patients did not differ from that of the COPD patients as a whole, whereas the ZZ homozygotes were younger (55.9 resemble those of a previous study in 66 male veterans with pulmonary emphysema suggesting that the MZ phenotype, or intermediate AAT deficiency in general, probably does predispose to the development of COPD. However, the prevalence of AAT deficiency in COPD patients is small (approximately 10 percent). The number with an MS phenotype was not increased in this group of COPD patients.},
  journal = {Chest},
  keywords = {\#nosource},
  number = {3}
}

@article{liExposingAmbiguitiesRelationextraction2015,
  ids = {Li2015a},
  title = {Exposing Ambiguities in a Relation-Extraction Gold Standard with Crowdsourcing},
  author = {Li, Tong Shu and Good, Benjamin M. and Su, Andrew I.},
  year = {2015},
  month = may,
  pages = {2--5},
  abstract = {Semantic relation extraction is one of the frontiers of biomedical natural language processing research. Gold standards are key tools for advancing this research. It is challenging to generate these standards because of the high cost of expert time and the difficulty in establishing agreement between annotators. We implemented and evaluated a microtask crowdsourcing approach that can produce a gold standard for extracting drug-disease relations. The aggregated crowd judgment agreed with expert annotations from a pre-existing corpus on 43 of 60 sentences tested. The levels of crowd agreement varied in a similar manner to the levels of agreement among the original expert annotators. This work rein-forces the power of crowdsourcing in the process of assembling gold standards for relation extraction. Further, it high-lights the importance of exposing the levels of agreement between human annotators, expert or crowd, in gold standard corpora as these are reproducible signals indicating ambiguities in the data or in the annotation guidelines.},
  annotation = {\_eprint: 1505.06256},
  archivePrefix = {arXiv},
  arxivid = {1505.06256},
  eprint = {1505.06256},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Li et al. - 2015 - Exposing ambiguities in a relation-extraction gold standard with crowdsourcing.pdf},
  journal = {arXiv:1505.06256 [cs, q-bio]},
  keywords = {Computer Science - Computation and Language,Quantitative Biology - Quantitative Methods},
  mendeley-tags = {Computer Science - Computation and Language,Quantitative Biology - Quantitative Methods},
  number = {1},
  primaryClass = {cs, q-bio}
}

@article{liImprovingRelationExtraction2019,
  ids = {Li2019},
  title = {Improving {{Relation Extraction}} with {{Knowledge}}-Attention},
  author = {Li, Pengfei and Mao, Kezhi and Yang, Xuefeng and Li, Qi},
  year = {2019},
  pages = {229--239},
  doi = {10.18653/v1/d19-1022},
  abstract = {While attention mechanisms have been proven to be effective in many NLP tasks, majority of them are data-driven. We propose a novel knowledge-attention encoder which incorporates prior knowledge from external lexical resources into deep neural networks for relation extraction task. Furthermore, we present three effective ways of integrating knowledge-attention with self-attention to maximize the utilization of both knowledge and data. The proposed relation extraction system is end-to-end and fully attention-based. Experiment results show that the proposed knowledge-attention mechanism has complementary strengths with self-attention, and our integrated models outperform existing CNN, RNN, and self-attention based models. State-of-the-art performance is achieved on TACRED, a complex and large-scale relation extraction dataset.},
  annotation = {\_eprint: 1910.02724},
  archivePrefix = {arXiv},
  arxivid = {1910.02724},
  eprint = {1910.02724},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Li et al. - 2019 - Improving Relation Extraction with Knowledge-attention.pdf}
}

@article{limaLogicbasedRelationalLearning2019,
  ids = {Lima2019,Lima2019a},
  title = {A Logic-Based Relational Learning Approach to Relation Extraction: {{The OntoILPER}} System},
  author = {Lima, Rinaldo and Espinasse, Bernard and Freitas, Fred},
  year = {2019},
  volume = {78},
  pages = {142--157},
  publisher = {{Elsevier Ltd}},
  issn = {09521976},
  doi = {10.1016/j.engappai.2018.11.001},
  abstract = {Relation Extraction (RE), the task of detecting and characterizing semantic relations between entities in text, has gained much importance in the last two decades, mainly in the biomedical domain. Many papers have been published on Relation Extraction using supervised machine learning techniques. Most of these techniques rely on statistical methods, such as feature-based and tree-kernels-based methods. Such statistical learning techniques are usually based on a propositional hypothesis space for representing examples, i.e., they employ an attribute\textendash value representation of features. This kind of representation has some drawbacks, particularly in the extraction of complex relations which demand more contextual information about the involving instances, i.e., it is not able to effectively capture structural information from parse trees without loss of information. In this work, we present OntoILPER, a logic-based relational learning approach to Relation Extraction that uses Inductive Logic Programming for generating extraction models in the form of symbolic extraction rules. OntoILPER takes profit of a rich relational representation of examples, which can alleviate the aforementioned drawbacks. The proposed relational approach seems to be more suitable for Relation Extraction than statistical ones for several reasons that we argue. Moreover, OntoILPER uses a domain ontology that guides the background knowledge generation process and is used for storing the extracted relation instances. The induced extraction rules were evaluated on three protein\textendash protein interaction datasets from the biomedical domain. The performance of OntoILPER extraction models was compared with other state-of-the-art RE systems. The encouraging results seem to demonstrate the effectiveness of the proposed solution.},
  file = {/home/harrisonpl/Documents/PDFs/Lima et al. - 2019 - A logic-based relational learning approach to relation extraction.pdf},
  journal = {Engineering Applications of Artificial Intelligence},
  keywords = {Inductive logic programming,Information extraction,Relation extraction,Relational learning,Rule induction},
  number = {October 2018}
}

@article{liMetaanalysisBasedWeighted2014,
  ids = {Li2014},
  title = {Meta-Analysis Based on Weighted Ordered {{P}}-Values for Genomic Data with Heterogeneity},
  author = {Li, Yihan and Ghosh, Debashis},
  year = {2014},
  volume = {15},
  pages = {226},
  issn = {14712105},
  doi = {10.1186/1471-2105-15-226},
  abstract = {Background: Meta-analysis has become increasingly popular in recent years, especially in genomic data analysis, due to the fast growth of available data and studies that target the same questions. Many methods have been developed, including classical ones such as Fisher's combined probability test and Stouffer's Z-test. However, not all meta-analyses have the same goal in mind. Some aim at combining information to find signals in at least one of the studies, while others hope to find more consistent signals across the studies. While many classical meta-analysis methods are developed with the former goal in mind, the latter goal has much more practicality for genomic data analysis.Results: In this paper, we propose a class of meta-analysis methods based on summaries of weighted ordered p-values (WOP) that aim at detecting significance in a majority of studies. We consider weighted versions of classical procedures such as Fisher's method and Stouffer's method where the weight for each p-value is based on its order among the studies. In particular, we consider weights based on the binomial distribution, where the median of the p-values are weighted highest and the outlying p-values are down-weighted. We investigate the properties of our methods and demonstrate their strengths through simulations studies, comparing to existing procedures. In addition, we illustrate application of the proposed methodology by several meta-analysis of gene expression data.Conclusions: Our proposed weighted ordered p-value (WOP) methods displayed better performance compared to existing methods for testing the hypothesis that there is signal in the majority of studies. They also appeared to be much more robust in applications compared to the rth ordered p-value (rOP) method (Song and Tseng, Ann. Appl. Stat. 2014, 8(2):777-800). With the flexibility of incorporating different p-value combination methods and different weighting schemes, the weighted ordered p-values (WOP) methods have great potential in detecting consistent signal in meta-analysis with heterogeneity. \textcopyright{} 2014 Li and Ghosh; licensee BioMed Central Ltd.},
  file = {/home/harrisonpl/Documents/PDFs/Li, Ghosh - 2014 - Meta-analysis based on weighted ordered P-values for genomic data with.pdf},
  journal = {BMC Bioinformatics},
  keywords = {Fisher's combined probability test,Meta-analysis,Ordered p-values,Weighted order statistic},
  number = {1}
}

@inproceedings{liMultitaskLearningBased2019,
  title = {A Multi-Task Learning Based Approach to Biomedical Entity Relation Extraction},
  booktitle = {Proceedings - 2018 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}}, {{BIBM}} 2018},
  author = {Li, Qingqing and Yang, Zhihao and Luo, Ling and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian and Yang, Liang and Xu, Kan and Zhang, Yijia},
  year = {2019},
  month = jan,
  pages = {680--682},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  doi = {10.1109/BIBM.2018.8621284},
  abstract = {Automatic extraction of high-quality biomedical entity relations from biomedical texts plays an important role in biomedical text mining. Currently, existing methods generally focus on training a single task model for a specific task (e.g., drug-drug interaction extraction, protein-protein interaction extraction), ignoring the correlation among multiple tasks. To solve the problem, we used neural network-based multi-task learning method to explore the correlation among multiple biomedical relation extraction tasks. In our study, we constructed a fully-shared model (FSM) and a shared-private model (SPM) and further proposed an attention-based main-auxiliary model (Att-MAM). Experimental results on five public biomedical relation extraction datasets show that the multi-task learning can effectively learn the shared information among multiple tasks and obtain better performance than the single task method.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/Li et al. - 2019 - A multi-task learning based approach to biomedical entity relation extraction.pdf},
  isbn = {978-1-5386-5488-0},
  keywords = {Multi-task Learning,Neural Networks,Relation Extraction}
}

@article{lincoffEvacetrapibCardiovascularOutcomes2017,
  ids = {Lincoff2017,Lincoff2017a},
  title = {Evacetrapib and {{Cardiovascular Outcomes}} in {{High}}-{{Risk Vascular Disease}}},
  author = {Lincoff, A. Michael and Nicholls, Stephen J. and Riesmeyer, Jeffrey S. and Barter, Philip J. and Brewer, H. Bryan and Fox, Keith A.A. and Gibson, C. Michael and Granger, Christopher and Menon, Venu and Montalescot, Gilles and Rader, Daniel and Tall, Alan R. and McErlean, Ellen and Wolski, Kathy and Ruotolo, Giacomo and Vangerow, Burkhard and Weerakkody, Govinda and Goodman, Shaun G. and Conde, Diego and McGuire, Darren K. and Nicolau, Jose C. and {Leiva-Pons}, Jose L. and Pesant, Yves and Li, Weimin and Kandath, David and Kouz, Simon and Tahirkheli, Naeem and Mason, Denise and Nissen, Steven E.},
  year = {2017},
  volume = {376},
  pages = {1933--1942},
  issn = {15334406},
  doi = {10.1056/NEJMoa1609581},
  abstract = {BACKGROUND: The cholesteryl ester transfer protein inhibitor evacetrapib substantially raises the high-density lipoprotein (HDL) cholesterol level, reduces the low-density lipoprotein (LDL) cholesterol level, and enhances cellular cholesterol efflux capacity. We sought to determine the effect of evacetrapib on major adverse cardiovascular outcomes in patients with high-risk vascular disease. METHODS: In a multicenter, randomized, double-blind, placebo-controlled phase 3 trial, we enrolled 12,092 patients who had at least one of the following conditions: an acute coronary syndrome within the previous 30 to 365 days, cerebrovascular atherosclerotic disease, peripheral vascular arterial disease, or diabetes mellitus with coronary artery disease. Patients were randomly assigned to receive either evacetrapib at a dose of 130 mg or matching placebo, administered daily, in addition to standard medical therapy. The primary efficacy end point was the first occurrence of any component of the composite of death from cardiovascular causes, myocardial infarction, stroke, coronary revascularization, or hospitalization for unstable angina. RESULTS: At 3 months, a 31.1\% decrease in the mean LDL cholesterol level was observed with evacetrapib versus a 6.0\% increase with placebo, and a 133.2\% increase in the mean HDL cholesterol level was seen with evacetrapib versus a 1.6\% increase with placebo. After 1363 of the planned 1670 primary end-point events had occurred, the data and safety monitoring board recommended that the trial be terminated early because of a lack of efficacy. After a median of 26 months of evacetrapib or placebo, a primary end-point event occurred in 12.9\% of the patients in the evacetrapib group and in 12.8\% of those in the placebo group (hazard ratio, 1.01; 95\% confidence interval, 0.91 to 1.11; P = 0.91). CONCLUSIONS: Although the cholesteryl ester transfer protein inhibitor evacetrapib had favorable effects on established lipid biomarkers, treatment with evacetrapib did not result in a lower rate of cardiovascular events than placebo among patients with high-risk vascular disease.},
  file = {/home/harrisonpl/Documents/PDFs/Lincoff et al. - 2017 - Evacetrapib and Cardiovascular Outcomes in High-Risk Vascular Disease.pdf},
  journal = {New England Journal of Medicine},
  keywords = {cardio,mechanism},
  mendeley-tags = {cardio,mechanism},
  number = {20},
  pmid = {28514624}
}

@article{liuGeneratingWikipediaSummarizing2018,
  ids = {Liu2018},
  title = {Generating Wikipedia by Summarizing Long Sequences},
  author = {Liu, Peter J. and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, {\textbackslash}Lukasz and Shazeer, Noam},
  year = {2018},
  pages = {1--18},
  abstract = {We show that generating English Wikipedia articles can be approached as a multi-document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder-decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations.},
  annotation = {\_eprint: 1801.10198},
  archivePrefix = {arXiv},
  arxivid = {1801.10198},
  eprint = {1801.10198},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Liu et al. - 2018 - Generating wikipedia by summarizing long sequences.pdf},
  journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
  keywords = {attention,neural nets,nlp,openai,summarization,transformer}
}

@article{liuPredictionFunctionalMicroRNA2019,
  ids = {Liu2019},
  title = {Prediction of Functional {{microRNA}} Targets by Integrative Modeling of {{microRNA}} Binding and Target Expression Data},
  author = {Liu, Weijun and Wang, Xiaowei},
  year = {2019},
  volume = {20},
  pages = {1--10},
  publisher = {{Genome Biology}},
  issn = {1474760X},
  doi = {10.1186/s13059-019-1629-z},
  abstract = {We perform a large-scale RNA sequencing study to experimentally identify genes that are downregulated by 25 miRNAs. This RNA-seq dataset is combined with public miRNA target binding data to systematically identify miRNA targeting features that are characteristic of both miRNA binding and target downregulation. By integrating these common features in a machine learning framework, we develop and validate an improved computational model for genome-wide miRNA target prediction. All prediction data can be accessed at miRDB (http://mirdb.org).},
  file = {/home/harrisonpl/Documents/PDFs/Liu, Wang - 2019 - Prediction of functional microRNA targets by integrative modeling of microRNA.pdf},
  isbn = {1305901916},
  journal = {Genome Biology},
  keywords = {CLIP-seq,David Port,MicroRNA,Recommendation,RNA-seq,Target prediction},
  mendeley-tags = {David Port,Recommendation},
  number = {1}
}

@article{liuTextSummarizationTensorFlow2016,
  title = {Text Summarization with {{TensorFlow}}},
  author = {Liu, Peter and Pan, Xin},
  year = {2016},
  abstract = {Every day, people rely on a wide variety of sources to stay informed \textendash{} from news stories to social media posts to search results. Being able to develop Machine Learning models that can automatically deliver accurate summaries of longer text can be useful for digesting such large amounts of information in a compressed form, and is a long-term goal of the Google Brain team.},
  keywords = {\#nosource,Natural language processing}
}

@article{livingstonKaBOBOntologybasedSemantic2015,
  ids = {Livingston2015},
  title = {{{KaBOB}}: Ontology-Based Semantic Integration of Biomedical Databases},
  shorttitle = {{{KaBOB}}},
  author = {Livingston, Kevin M. and Bada, Michael and Baumgartner, William A. and Hunter, Lawrence E.},
  year = {2015},
  volume = {16},
  pages = {126},
  issn = {14712105},
  doi = {10.1186/s12859-015-0559-3},
  abstract = {Background: The ability to query many independent biological databases using a common ontology-based semantic model would facilitate deeper integration and more effective utilization of these diverse and rapidly growing resources. Despite ongoing work moving toward shared data formats and linked identifiers, significant problems persist in semantic data integration in order to establish shared identity and shared meaning across heterogeneous biomedical data sources. Results: We present five processes for semantic data integration that, when applied collectively, solve seven key problems. These processes include making explicit the differences between biomedical concepts and database records, aggregating sets of identifiers denoting the same biomedical concepts across data sources, and using declaratively represented forward-chaining rules to take information that is variably represented in source databases and integrating it into a consistent biomedical representation. We demonstrate these processes and solutions by presenting KaBOB (the Knowledge Base Of Biomedicine), a knowledge base of semantically integrated data from 18 prominent biomedical databases using common representations grounded in Open Biomedical Ontologies. An instance of KaBOB with data about humans and seven major model organisms can be built using on the order of 500 million RDF triples. All source code for building KaBOB is available under an open-source license. Conclusions: KaBOB is an integrated knowledge base of biomedical data representationally based in prominent, actively maintained Open Biomedical Ontologies, thus enabling queries of the underlying data in terms of biomedical concepts (e.g., genes and gene products, interactions and processes) rather than features of source-specific data schemas or file formats. KaBOB resolves many of the issues that routinely plague biomedical researchers intending to work with data from multiple data sources and provides a platform for ongoing data integration and development and for formal reasoning over a wealth of integrated biomedical data.},
  file = {/home/harrisonpl/Documents/PDFs/Livingston et al. - 2015 - KaBOB.pdf},
  journal = {BMC Bioinformatics},
  keywords = {Biomedical,Databases,hunter lab,Knowledge representation and reasoning,Open biomedical ontologies,OWL,RDF,Semantic data integration,Semantic web},
  mendeley-tags = {hunter lab},
  number = {1}
}

@article{londonArtificialIntelligenceBlackBox2019,
  ids = {London2019},
  title = {Artificial {{Intelligence}} and {{Black}}-{{Box Medical Decisions}}: {{Accuracy}} versus {{Explainability}}},
  author = {London, Alex John},
  year = {2019},
  volume = {49},
  pages = {15--21},
  issn = {1552146X},
  doi = {10.1002/hast.973},
  abstract = {Although decision-making algorithms are not new to medicine, the availability of vast stores of medical data, gains in computing power, and breakthroughs in machine learning are accelerating the pace of their development, expanding the range of questions they can address, and increasing their predictive power. In many cases, however, the most powerful machine learning techniques purchase diagnostic or predictive accuracy at the expense of our ability to access ``the knowledge within the machine.'' Without an explanation in terms of reasons or a rationale for particular decisions in individual cases, some commentators regard ceding medical decision-making to black box systems as contravening the profound moral responsibilities of clinicians. I argue, however, that opaque decisions are more common in medicine than critics realize. Moreover, as Aristotle noted over two millennia ago, when our knowledge of causal systems is incomplete and precarious\textemdash as it often is in medicine\textemdash the ability to explain how results are produced can be less important than the ability to produce such results and empirically verify their accuracy.},
  file = {/home/harrisonpl/Documents/PDFs/London - 2019 - Artificial Intelligence and Black-Box Medical Decisions.pdf},
  journal = {Hastings Center Report},
  number = {1}
}

@article{lowesMyocardialGeneExpression2002,
  ids = {Lowes2002,Lowes2002a},
  title = {Myocardial Gene Expression in Dilated Cardiomyopathy Treated with Beta-Blocking Agents},
  author = {Lowes, Brian D. and Gilbert, Edward M. and Abraham, William T. and Minobe, Wayne A. and Larrabee, Patti and Ferguson, Debra and Wolfel, Eugene E. and Lindenfeld, Jo Ann and Tsvetkova, Tatiana and Robertson, Alastair D. and Quaife, Robert A. and Bristow, Michael R.},
  year = {2002},
  month = may,
  volume = {346},
  pages = {1357--1365},
  issn = {00284793},
  doi = {10.1056/NEJMoa012630},
  abstract = {Background: Beta-blocker therapy may improve cardiac function in patients with idiopathic dilated cardiomyopathy. We tested the hypothesis that betablocker therapy produces favorable functional effects in dilated cardiomyopathy by altering the expression of myocardial genes that regulate contractility and pathologic hypertrophy. Methods: We randomly assigned 53 patients with idiopathic dilated cardiomyopathy to treatment with a {$\beta$}-adrenergic-receptor blocking agent (metoprolol or carvedilol) or placebo. The amount of messenger RNA (mRNA) for contractility-regulating genes (those encoding {$\beta$} 1- and {$\beta$} 2-adrenergic receptors, calcium ATPase in the sarcoplasmic reticulum, and {$\alpha$}- and {$\beta$}-myosin heavy-chain isoforms) and of genes associated with pathologic hypertrophy ({$\beta$}-myosin heavy chain and atrial natriuretic peptide) was measured with a quantitative reverse-transcription polymerase chain reaction in total RNA extracted from biopsy specimens of the right ventricular septal endomyocardium. Myocardial levels of {$\beta$}-adrenergic receptors were also measured. Measurements were conducted at base line and after six months of treatment, and changes in gene expression were compared with changes in the left ventricular ejection fraction as measured by radionuclide ventriculography. Results: Twenty-six of 32 beta-blocker-treated patients (those with complete mRNA measurements) had an improvement in left ventricular ejection fraction of at least 5 ejection-fraction (EF) units (mean [{$\pm$}SE] increase, 18.8{$\pm$}1.8). As compared with the six betablocker-treated patients who did not have a response (mean change, a decrease of 2.5{$\pm$}1.8 EF units), those who did have a response had an increase in sarcoplasmic-reticulum calcium ATPase mRNA and {$\alpha$}-myosin heavy chain mRNA and a decrease in {$\beta$}-myosin heavy chain mRNA. The change in sarcoplasmic-reticulum calcium ATPase was not present in the patients in the placebo group who had a spontaneous response. There were no differences between those who had a response and those who did not in terms of the change in mRNA or protein expression of {$\beta$}-adrenergic receptors. Conclusions: In idiopathic dilated cardiomyopathy, functional improvement related to treatment with beta-blockers is associated with changes in myocardial gene expression. Copyright \textcopyright{} 2002 Massachusetts Medical Society.},
  file = {/home/harrisonpl/Documents/PDFs/Lowes et al. - 2002 - Myocardial gene expression in dilated cardiomyopathy treated with beta-blocking.pdf},
  journal = {New England Journal of Medicine},
  keywords = {borg,HPL comprehensive exam},
  mendeley-tags = {HPL comprehensive exam,borg},
  number = {18}
}

@article{lozuponeGlobalPatternsBacterial2007,
  ids = {Lozupone2007},
  title = {Global Patterns in Bacterial Diversity},
  author = {Lozupone, Catherine A. and Knight, Rob},
  year = {2007},
  volume = {104},
  pages = {11436--11440},
  issn = {00278424},
  doi = {10.1073/pnas.0611525104},
  abstract = {Microbes are difficult to culture. Consequently, the primary source of information about a fundamental evolutionary topic, life's diversity, is the environmental distribution of gene sequences. We report the most comprehensive analysis of the environmental distribution of bacteria to date, based on 21,752 16S rRNA sequences compiled from 111 studies of diverse physical environments. We clustered the samples based on similarities in the phylogenetic lineages that they contain and found that, surprisingly, the major environmental determinant of microbial community composition is salinity rather than extremes of temperature, pH, or other physical and chemical factors represented in our samples. We find that sediments are more phylogenetically diverse than any other environment type. Surprisingly, soil, which has high species-level diversity, has below-average phylogenetic diversity. This work provides a framework for understanding the impact of environmental factors on bacterial evolution and for the direction of future sequencing efforts to discover new lineages. \textcopyright{} 2007 by The National Academy of Sciences of the USA.},
  file = {/home/harrisonpl/Documents/PDFs/Lozupone, Knight - 2007 - Global patterns in bacterial diversity.pdf},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  keywords = {Environmental distribution,Microbial ecology,Phylogenetic diversity,UniFrac},
  number = {27},
  pmid = {17592124}
}

@article{lozuponeUniFracNewPhylogenetic2005,
  ids = {Lozupone2005},
  title = {{{UniFrac}}: {{A}} New Phylogenetic Method for Comparing Microbial Communities},
  shorttitle = {{{UniFrac}}},
  author = {Lozupone, Catherine and Knight, Rob},
  year = {2005},
  volume = {71},
  pages = {8228--8235},
  issn = {00992240},
  doi = {10.1128/AEM.71.12.8228-8235.2005},
  abstract = {We introduce here a new method for computing differences between microbial communities based on phylogenetic information. This method, UniFrac, measures the phylogenetic distance between sets of taxa in a phylogenetic tree as the fraction of the branch length of the tree that leads to descendants from either one environment or the other, but not both. UniFrac can be used to determine whether communities are significantly different, to compare many communities simultaneously using clustering and ordination techniques, and to measure the relative contributions of different factors, such as chemistry and geography, to similarities between samples. We demonstrate the utility of UniFrac by applying it to published 16S rRNA gene libraries from cultured isolates and environmental clones of bacteria in marine sediment, water, and ice. Our results reveal that (i) cultured isolates from ice, water, and sediment resemble each other and environmental clone sequences from sea ice, but not environmental clone sequences from sediment and water; (ii) the geographical location does not correlate strongly with bacterial community differences in ice and sediment from the Arctic and Antarctic; and (iii) bacterial communities differ between terrestrially impacted seawater (whether polar or temperate) and warm oligotrophic seawater, whereas those in individual seawater samples are not more similar to each other than to those in sediment or ice samples. These results illustrate that UniFrac provides a new way of characterizing microbial communities, using the wealth of environmental rRNA sequences, and allows quantitative insight into the factors that underlie the distribution of lineages among environments. Copyright \textcopyright{} 2005, American Society for Microbiology. All Rights Reserved.},
  file = {/home/harrisonpl/Documents/PDFs/Lozupone, Knight - 2005 - UniFrac.pdf},
  journal = {Applied and Environmental Microbiology},
  number = {12},
  pmid = {16332807}
}

@article{lukassenSARSCoV2ReceptorACE22020,
  title = {{{SARS}}-{{CoV}}-2 Receptor {{ACE2}} and {{TMPRSS2}} Are Predominantly Expressed in a Transient Secretory Cell Type in Subsegmental Bronchial Branches},
  author = {Lukassen, Soeren and Chua, Robert Lorenz and Trefzer, Timo and Kahn, Nicolas C. and Schneider, Marc A. and Muley, Thomas and Winter, Hauke and Meister, Michael and Veith, Carmen and Boots, Agnes W. and Hennig, Bianca P. and Kreuter, Michael and Conrad, Christian and Eils, Roland},
  year = {2020},
  month = mar,
  pages = {2020.03.13.991455},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.03.13.991455},
  abstract = {{$<$}p{$>$}The SARS-CoV-2 pandemic affecting the human respiratory system severely challenges public health and urgently demands for increasing our understanding of COVID-19 pathogenesis, especially host factors facilitating virus infection and replication. SARS-CoV-2 was reported to enter cells via binding to ACE2, followed by its priming by TMPRSS2. Here, we investigate ACE2 and TMPRSS2 expression levels and their distribution across cell types in lung tissue (twelve donors, 39,778 cells) and in cells derived from subsegmental bronchial branches (four donors, 17,521 cells) by single nuclei and single cell RNA sequencing, respectively. While TMPRSS2 is expressed in both tissues, in the subsegmental bronchial branches ACE2 is predominantly expressed in a transient secretory cell type. Interestingly, these transiently differentiating cells show an enrichment for pathways related to RHO GTPase function and viral processes suggesting increased vulnerability for SARS-CoV-2 infection. Our data provide a rich resource for future investigations of COVID-19 infection and pathogenesis.{$<$}/p{$>$}},
  chapter = {New Results; http://web.archive.org/web/20200318211057/https://www.biorxiv.org/content/10.1101/2020.03.13.991455v2},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  file = {/home/harrisonpl/Documents/PDFs/Lukassen et al. - 2020 - SARS-CoV-2 receptor ACE2 and TMPRSS2 are predominantly expressed in a transient.pdf;/home/harrisonpl/Zotero/storage/TEB83SGA/2020.03.13.html},
  journal = {bioRxiv},
  language = {en}
}

@article{luoBiomedicalDataComputational2020,
  ids = {Luo2020},
  title = {Biomedical Data and Computational Models for Drug Repositioning: A Comprehensive Review},
  author = {Luo, Huimin and Li, Min and Yang, Mengyun and Wu, Fang-Xiang and Li, Yaohang and Wang, Jianxin},
  year = {2020},
  volume = {00},
  pages = {1--16},
  issn = {1467-5463},
  doi = {10.1093/bib/bbz176},
  abstract = {Drug repositioning can drastically decrease the cost and duration taken by traditional drug research and development while avoiding the occurrence of unforeseen adverse events. With the rapid advancement of high-throughput technologies and the explosion of various biological data and medical data, computational drug repositioning methods have been appealing and powerful techniques to systematically identify potential drug-target interactions and drug-disease interactions. In this review, we first summarize the available biomedical data and public databases related to drugs, diseases and targets. Then, we discuss existing drug repositioning approaches and group them based on their underlying computational models consisting of classical machine learning, network propagation, matrix factorization and completion, and deep learning based models. We also comprehensively analyze common standard data sets and evaluation metrics used in drug repositioning, and give a brief comparison of various prediction methods on the gold standard data sets. Finally, we conclude our review with a brief discussion on challenges in computational drug repositioning, which includes the problem of reducing the noise and incompleteness of biomedical data, the ensemble of various computation drug repositioning methods, the importance of designing reliable negative samples selection methods, new techniques dealing with the data sparseness problem, the construction of large-scale and comprehensive benchmark data sets and the analysis and explanation of the underlying mechanisms of predicted interactions.},
  file = {/home/harrisonpl/Documents/PDFs/Luo et al. - 2020 - Biomedical data and computational models for drug repositioning.pdf},
  journal = {Briefings in Bioinformatics},
  keywords = {computational model,data integration,drug repositioning,drug-disease prediction,drug-target prediction},
  number = {December 2019}
}

@inproceedings{luoLargescaleIntegrationHeterogeneous2018,
  ids = {Luo2018},
  title = {Large-Scale Integration of Heterogeneous Pharmacogenomic Data for Identifying Drug Mechanism of Action},
  booktitle = {Pacific {{Symposium}} on {{Biocomputing}}},
  author = {Luo, Yunan and Wang, Sheng and Xiao, Jinfeng and Peng, Jian},
  year = {2018},
  volume = {0},
  pages = {44--55},
  publisher = {{WORLD SCIENTIFIC}},
  issn = {23356936},
  doi = {10.1142/9789813235533_0005},
  abstract = {A variety of large-scale pharmacogenomic data, such as perturbation experiments and sensitivity profiles, enable the systematical identification of drug mechanism of actions (MoAs), which is a crucial task in the era of precision medicine. However, integrating these complementary pharmacogenomic datasets is inherently challenging due to the wild heterogeneity, high-dimensionality and noisy nature of these datasets. In this work, we develop Mania, a novel method for the scalable integration of large-scale pharmacogenomic data. Mania first constructs a drug-drug similarity network through integrating multiple heterogeneous data sources, including drug sensitivity, drug chemical structure, and perturbation assays. It then learns a compact vector representation for each drug to simultaneously encode its structural and pharmacogenomic properties. Extensive experiments demonstrate that Mania achieves substantially improved performance in both MoAs and targets prediction, compared to predictions based on individual data sources as well as a state-of-the-art integrative method. Moreover, Mania identifies drugs that target frequently mutated cancer genes, which provides novel insights into drug repurposing.},
  file = {/home/harrisonpl/Documents/PDFs/Luo et al. - 2018 - Large-scale integration of heterogeneous pharmacogenomic data for identifying.pdf},
  isbn = {978-981-323-552-6 978-981-323-553-3},
  keywords = {Data integration,Dimensionality reduction,Drug mechanisms of action,Drug similarity network,Drug target},
  number = {212669}
}

@article{luoNaturalLanguageProcessing2017,
  title = {Natural {{Language Processing}} for {{EHR}}-{{Based Pharmacovigilance}}: {{A Structured Review}}},
  shorttitle = {Natural {{Language Processing}} for {{EHR}}-{{Based Pharmaco}}},
  author = {Luo, Yuan and Thompson, William K. and Herr, Timothy M. and Zeng, Zexian and Berendsen, Mark A. and Jonnalagadda, Siddhartha R. and Carson, Matthew B. and Starren, Justin},
  year = {2017},
  month = jun,
  volume = {40},
  pages = {1075--1089},
  issn = {11791942},
  doi = {10.1007/s40264-017-0558-6},
  abstract = {The goal of pharmacovigilance is to detect, monitor, characterize and prevent adverse drug events (ADEs) with pharmaceutical products. This article is a comprehensive structured review of recent advances in applying natural language processing (NLP) to electronic health record (EHR) narratives for pharmacovigilance. We review methods of varying complexity and problem focus, summarize the current state-of-the-art in methodology advancement, discuss limitations and point out several promising future directions. The ability to accurately capture both semantic and syntactic structures in clinical narratives becomes increasingly critical to enable efficient and accurate ADE detection. Significant progress has been made in algorithm development and resource construction since 2000. Since 2012, statistical analysis and machine learning methods have gained traction in automation of ADE mining from EHR narratives. Current state-of-the-art methods for NLP-based ADE detection from EHRs show promise regarding their integration into production pharmacovigilance systems. In addition, integrating multifaceted, heterogeneous data sources has shown promise in improving ADE detection and has become increasingly adopted. On the other hand, challenges and opportunities remain across the frontier of NLP application to EHR-based pharmacovigilance, including proper characterization of ADE context, differentiation between off- and on-label drug-use ADEs, recognition of the importance of polypharmacy-induced ADEs, better integration of heterogeneous data sources, creation of shared corpora, and organization of shared-task challenges to advance the state-of-the-art.},
  journal = {Drug Safety},
  keywords = {\#nosource,Lawrence Hunter,Recommendation},
  language = {eng},
  number = {11}
}

@article{machamerThinkingMechanisms2000,
  ids = {Machamer2000},
  title = {Thinking about Mechanisms},
  author = {Machamer, Peter and Darden, Lindley and Craver, Carl F.},
  year = {2000},
  volume = {67},
  pages = {1--25},
  issn = {00318248},
  doi = {10.1086/392759},
  abstract = {The concept of mechanism is analyzed in terms of entities and activities, organized such that they are productive of regular changes. Examples show how mechanisms work in neurobiology and molecular biology. Thinking in terms of mechanisms provides a new framework for addressing many traditional philosophical issues: causality, laws, explanation, reduction, and scientific change.},
  file = {/home/harrisonpl/Documents/PDFs/Machamer et al. - 2000 - Thinking about mechanisms.pdf},
  journal = {Philosophy of Science},
  keywords = {hunter lab},
  mendeley-tags = {hunter lab},
  number = {1}
}

@article{makhzaniAdversarialAutoencoders2015,
  ids = {Makhzani2015},
  title = {Adversarial {{Autoencoders}}},
  author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  year = {2015},
  abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
  annotation = {\_eprint: 1511.05644},
  archivePrefix = {arXiv},
  arxivid = {1511.05644},
  eprint = {1511.05644},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Makhzani et al. - 2015 - Adversarial Autoencoders.pdf},
  keywords = {Computer Science - Learning,Favorite}
}

@article{mannDescriptionLogicHandbook2003,
  ids = {Mann2003,Mann2003a},
  title = {The {{Description Logic Handbook}} \textendash{} {{Theory}}, {{Implementation}} and {{Applications}}},
  author = {Mann, C.J.H.},
  year = {2003},
  volume = {32},
  pages = {500},
  issn = {0368-492X},
  doi = {10.1108/k.2003.06732iae.006},
  abstract = {Description logics are embodied in several knowledge-based systems and are used to develop various real-life applications. Now in paperback, The Description Logic Handbook provides a thorough account of the subject, covering all aspects of research in this field, namely: theory, implementation, and applications. Its appeal will be broad, ranging from more theoretically oriented readers, to those with more practically oriented interests who need a sound and modern understanding of knowledge representation systems based on description logics. As well as general revision throughout the book, this new edition presents a new chapter on ontology languages for the semantic web, an area of great importance for the future development of the web. In sum, the book will serve as a unique resource for the subject, and can also be used for self-study or as a reference for knowledge representation and artificial intelligence courses.},
  file = {/home/harrisonpl/Documents/PDFs/Mann - 2003 - The Description Logic Handbook – Theory, Implementation and Applications.pdf},
  journal = {Kybernetes},
  keywords = {Description logic,Description logics,ontology},
  mendeley-tags = {ontology},
  number = {9/10}
}

@article{manuscriptMammalianPhenotypeOnthology2010,
  ids = {Manuscript2010},
  title = {Mammalian {{Phenotype Onthology}}},
  author = {Manuscript, Author},
  year = {2010},
  volume = {1},
  pages = {390--399},
  doi = {10.1002/wsbm.44.The},
  file = {/home/harrisonpl/Documents/PDFs/Manuscript - 2010 - Mammalian Phenotype Onthology.pdf},
  journal = {Systematic Biology},
  keywords = {annotation,are studied,environment where such characteristics,mammal,many,model system,observable morphological,of an individual in,ontology,phenotype,phenotype refers to the,physiological and behavioral characteristics,the context of the},
  number = {3}
}

@article{martinRoleMetagenomicsUnderstanding2014,
  ids = {Martin2014},
  title = {The Role of Metagenomics in Understanding the Human Microbiome in Health and Disease},
  author = {Mart{\'i}n, Rebeca and Miquel, Sylvie and Langella, Philippe and {Berm{\'u}dez-Humar{\'a}n}, Luis G.},
  year = {2014},
  volume = {5},
  pages = {413--423},
  issn = {21505608},
  doi = {10.4161/viru.27864},
  abstract = {The term microbiome refers to the genetic material of the catalog of microbial taxa associated with humans. As in all ecosystems, the microbiota reaches a dynamic equilibrium in the human body, which can be altered by environmental factors and external stimuli. Metagenomics is a relatively new field of study of microbial genomes within diverse environmental samples, which is of increasing importance in microbiology. The introduction of this ecological perception of microbiology is the key to achieving real knowledge about the influence of the microbiota in human health and disease. The aim of this review is to summarize the link between the human microbiota (focusing on the intestinal, vaginal, skin, and airway body sites) and health from this ecological point of view, highlighting the contribution of metagenomics in the advance of this field. \textcopyright{} 2014 Landes Bioscience.},
  file = {/home/harrisonpl/Documents/PDFs/Martín et al. - 2014 - The role of metagenomics in understanding the human microbiome in health and.pdf},
  journal = {Virulence},
  keywords = {Airway microbiome,Dysbiosis,GIT microbiome,Lactobacilli,Skin microbiome,Vagina microbiome},
  number = {3},
  pmid = {24429972}
}

@article{massanet-vilaGraphTheorybasedMeasures2010,
  ids = {Massanet-Vila2010},
  title = {Graph Theory-Based Measures as Predictors of Gene Morbidity},
  author = {{Massanet-Vila}, Raimon and Caminal, Pere and Perera, Alexandre},
  year = {2010},
  issn = {1079-7114},
  doi = {10.1109/IEMBS.2010.5626521},
  abstract = {Previous studies have suggested that some graph properties of protein interaction networks might be related with gene morbidity. In particular, it has been suggested that when a polymorphism affects a gene, it is more likely to produce a disease if the node degree in the interaction network is higher than for other genes. However, these results do not take into account the possible bias introduced by the variance in the amount of information available for different genes. This work models the relationship between the morbidity associated with a gene and the degrees of the nodes in the protein interaction network controlling the amount of information available in the literature. A set of 7461 genes and 3665 disease identifiers reported in the Online Mendelian Inheritance in Man (OMIM) was mined jointly with 9630 nodes and 38756 interactions of the Human Proteome Resource Database (HPRD). The information available from a gene was measured through PubMed mining. Results suggest that the correlation between the degree of a node in the protein interaction network and its morbidity is largely contributed by the information available from the gene. Even though the results suggest a positive correlation between the degree of a node and its morbidity while controlling the information factor, we believe this correlation has to be taken with caution for it can be affected by other factors not taken into account in this study. \textcopyright{} 2010 IEEE.},
  annotation = {\_eprint: arXiv:1102.1087v6},
  archivePrefix = {arXiv},
  arxivid = {arXiv:1102.1087v6},
  eprint = {1102.1087v6},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Massanet-Vila et al. - 2010 - Graph theory-based measures as predictors of gene morbidity.pdf},
  journal = {2010 Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC'10},
  pmid = {21929175}
}

@article{matthiessenMechanisticExplanationSystems2017,
  title = {Mechanistic {{Explanation}} in {{Systems Biology}}: {{Cellular Networks}}},
  shorttitle = {Mechanistic {{Explanation}} in {{Systems Biology}}},
  author = {Matthiessen, Dana},
  year = {2017},
  month = mar,
  volume = {68},
  pages = {1--25},
  publisher = {{Oxford Academic}},
  issn = {0007-0882},
  doi = {10.1093/bjps/axv011},
  abstract = {Abstract.  It is argued that once biological systems reach a certain level of complexity, mechanistic explanations provide an inadequate account of many relevan},
  file = {/home/harrisonpl/Documents/PDFs/Matthiessen - 2017 - Mechanistic Explanation in Systems Biology.pdf;/home/harrisonpl/Zotero/storage/J85HBPHY/2995179.html},
  journal = {The British Journal for the Philosophy of Science},
  language = {en},
  number = {1}
}

@article{maUsingDeepLearning2018,
  ids = {Ma2018},
  title = {Using Deep Learning to Model the Hierarchical Structure and Function of a Cell},
  author = {Ma, Jianzhu and Yu, Michael Ku and Fong, Samson and Ono, Keiichiro and Sage, Eric and Demchak, Barry and Sharan, Roded and Ideker, Trey},
  year = {2018},
  volume = {15},
  pages = {290--298},
  publisher = {{Nature Publishing Group}},
  issn = {15487105},
  doi = {10.1038/nmeth.4627},
  abstract = {Although artificial neural networks are powerful classifiers, their internal structures are hard to interpret. In the life sciences, extensive knowledge of cell biology provides an opportunity to design visible neural networks (VNNs) that couple the model's inner workings to those of real systems. Here we develop DCell, a VNN embedded in the hierarchical structure of 2,526 subsystems comprising a eukaryotic cell (http://d-cell.ucsd.edu/). Trained on several million genotypes, DCell simulates cellular growth nearly as accurately as laboratory observations. During simulation, genotypes induce patterns of subsystem activities, enabling in silico investigations of the molecular mechanisms underlying genotype-phenotype associations. These mechanisms can be validated, and many are unexpected; some are governed by Boolean logic. Cumulatively, 80\% of the importance for growth prediction is captured by 484 subsystems (21\%), reflecting the emergence of a complex phenotype. DCell provides a foundation for decoding the genetics of disease, drug resistance and synthetic life.},
  file = {/home/harrisonpl/Documents/PDFs/Ma et al. - 2018 - Using deep learning to model the hierarchical structure and function of a cell.pdf},
  journal = {Nature Methods},
  number = {4}
}

@article{mengGeneSetMetaanalysis2019,
  ids = {Meng2019,Meng2019a},
  title = {Gene Set Meta-Analysis with Quantitative Set Analysis for Gene Expression ({{QuSAGE}})},
  author = {Meng, Hailong and Yaari, Gur and Bolen, Christopher R. and Avey, Stefan and Kleinstein, Steven H.},
  editor = {Pertea, Mihaela},
  year = {2019},
  month = apr,
  volume = {15},
  pages = {1--10},
  publisher = {{Public Library of Science}},
  issn = {15537358},
  doi = {10.1371/journal.pcbi.1006899},
  abstract = {Small sample sizes combined with high person-to-person variability can make it difficult to detect significant gene expression changes from transcriptional profiling studies. Subtle, but coordinated, gene expression changes may be detected using gene set analysis approaches. Meta-analysis is another approach to increase the power to detect biologically relevant changes by integrating information from multiple studies. Here, we present a framework that combines both approaches and allows for meta-analysis of gene sets. QuSAGE meta-analysis extends our previously published QuSAGE framework, which offers several advantages for gene set analysis, including fully accounting for gene-gene correlations and quantifying gene set activity as a full probability density function. Application of QuSAGE meta-analysis to influenza vaccination response shows it can detect significant activity that is not apparent in individual studies.},
  file = {/home/harrisonpl/Documents/PDFs/Meng et al. - 2019 - Gene set meta-analysis with quantitative set analysis for gene expression.pdf},
  journal = {PLoS Computational Biology},
  keywords = {Antibodies,Gene expression,Gene regulation,Influenza,Metaanalysis,Permutation,Plasma cells,Vaccination and immunization},
  language = {en},
  mendeley-tags = {Antibodies,Gene expression,Gene regulation,Influenza,Metaanalysis,Permutation,Plasma cells,Vaccination and immunization},
  number = {4}
}

@article{meystreNaturalLanguageProcessing2006,
  ids = {Meystre2006},
  title = {Natural Language Processing to Extract Medical Problems from Electronic Clinical Documents: {{Performance}} Evaluation},
  shorttitle = {Natural Language Processing to Extract Medical Pro},
  author = {Meystre, St{\'e}phane and Haug, Peter J.},
  year = {2006},
  month = dec,
  volume = {39},
  pages = {589--599},
  issn = {15320464},
  doi = {10.1016/j.jbi.2005.11.004},
  abstract = {In this study, we evaluate the performance of a Natural Language Processing (NLP) application designed to extract medical problems from narrative text clinical documents. The documents come from a patient's electronic medical record and medical problems are proposed for inclusion in the patient's electronic problem list. This application has been developed to help maintain the problem list and make it more accurate, complete, and up-to-date. The NLP part of this system-analyzed in this study-uses the UMLS MetaMap Transfer (MMTx) application and a negation detection algorithm called NegEx to extract 80 different medical problems selected for their frequency of use in our institution. When using MMTx with its default data set, we measured a recall of 0.74 and a precision of 0.756. A custom data subset for MMTx was created, making it faster and significantly improving the recall to 0.896 with a non-significant reduction in precision. \textcopyright{} 2005 Elsevier Inc. All rights reserved.},
  file = {/home/harrisonpl/Documents/PDFs/Meystre, Haug - 2006 - Natural language processing to extract medical problems from electronic.pdf},
  journal = {Journal of Biomedical Informatics},
  keywords = {hunter lab,Medical Records,MetaMap Transfer,Natural language processing,Natural Language Processing,NegEx,Problem oriented,Program evaluation},
  mendeley-tags = {Medical Records,MetaMap Transfer,Natural language processing,NegEx,Problem oriented,Program evaluation,hunter lab},
  number = {6}
}

@article{miDiseaseClassificationGene2019,
  ids = {Mi2019},
  title = {Disease Classification via Gene Network Integrating Modules and Pathways},
  author = {Mi, Zhilong and Guo, Binghui and Yin, Ziqiao and Li, Jiahui and Zheng, Zhiming},
  year = {2019},
  volume = {6},
  issn = {20545703},
  doi = {10.1098/rsos.190214},
  abstract = {Disease classification based on gene information has been of significance as the foundation for achieving precision medicine. Previous works focus on classifying diseases according to the gene expression data of patient samples, and constructing disease network based on the overlap of disease genes, as many genes have been confirmed to be associated with diseases. In this work, the effects of diseases on human biological functions are assessed from the perspective of gene network modules and pathways, and the distances between diseases are defined to carry out the classification models. In total, 1728 diseases are divided into 12 and 14 categories by the intensity and scope of effects on pathways, respectively. Each category is a mix of several types of diseases identified based on congenital and acquired factors as well as diseased tissues and organs. The disease classification models on the basis of gene network are parallel with traditional pathology classification based on anatomic and clinical manifestations, and enable us to look at diseases in the viewpoint of commonalities in etiology and pathology. Our models provide a foundation for exploring combination therapy of diseases, which in turn may inform strategies for future gene-targeted therapy.},
  file = {/home/harrisonpl/Documents/PDFs/Mi et al. - 2019 - Disease classification via gene network integrating modules and pathways.pdf},
  isbn = {0000000205},
  journal = {Royal Society Open Science},
  keywords = {Disease classification,Gene network,Modules,Pathways},
  number = {7}
}

@article{mikolovEfficientEstimationWord2013,
  ids = {Mikolov,Mikolov2013,Mikolov2013a},
  title = {Efficient Estimation of Word Representations in Vector Space},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  pages = {1--12},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  annotation = {\_eprint: 1301.3781},
  archivePrefix = {arXiv},
  arxivid = {1301.3781},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Mikolov et al. - 2013 - Efficient estimation of word representations in vector space.pdf},
  journal = {1st International Conference on Learning Representations, ICLR 2013 - Workshop Track Proceedings},
  keywords = {nlp,word vectors}
}

@article{millsOrganlevelProteinNetworks2020,
  ids = {Mills2020},
  title = {Organ-Level Protein Networks as a Reference for the Host Effects of the Microbiome},
  author = {Mills, Robert H. and Wozniak, Jacob M. and Vrbanac, Alison and Campeau, Anaamika and Chassaing, Benoit and Gewirtz, Andrew and Knight, Rob and Gonzalez, David J.},
  year = {2020},
  month = jan,
  pages = {gr.256875.119},
  publisher = {{Cold Spring Harbor Laboratory}},
  issn = {1088-9051},
  doi = {10.1101/gr.256875.119},
  file = {/home/harrisonpl/Documents/PDFs/Mills et al. - 2020 - Organ-level protein networks as a reference for the host effects of the.pdf},
  journal = {Genome Research}
}

@inproceedings{mintzDistantSupervisionRelation2009,
  ids = {Mintz2009},
  title = {Distant Supervision for Relation Extraction without Labeled Data},
  author = {Mintz, Mike and Bills, Steven and Snow, Rion and Jurafsky, Dan},
  year = {2009},
  volume = {2},
  pages = {1003},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.3115/1690219.1690287},
  abstract = {Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACE- style algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of enti- ties that appears in some Freebase relation, we find all sentences containing those entities in a large un- labeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 re- lations at a precision of 67.6\%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.},
  file = {/home/harrisonpl/Documents/PDFs/Mintz et al. - 2009 - Distant supervision for relation extraction without labeled data.pdf},
  isbn = {978-1-932432-46-6},
  keywords = {Natural language processing},
  mendeley-tags = {Natural language processing}
}

@misc{mitchellCanGPT3Make2020,
  title = {Can {{GPT}}-3 {{Make Analogies}}?},
  author = {Mitchell, Melanie},
  year = {2020},
  month = aug,
  abstract = {By Melanie Mitchell},
  file = {/home/harrisonpl/Zotero/storage/VY27Q755/can-gpt-3-make-analogies-16436605c446.html},
  howpublished = {https://medium.com/@melaniemitchell.me/can-gpt-3-make-analogies-16436605c446},
  journal = {Medium},
  language = {en}
}

@article{mitchellConceptualAbstractionAnalogy2020,
  title = {Conceptual {{Abstraction}} and {{Analogy}} in {{Artificial Intelligence}}},
  author = {Mitchell, Melanie},
  year = {2020},
  month = jul,
  volume = {32},
  pages = {7--7},
  publisher = {{MIT Press}},
  doi = {10.1162/isal_a_00354},
  abstract = {In 1955, John McCarthy and colleagues proposed an AI summer research project with the following aim: ``An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves.'' More than six decades later, all of these research topics remain open and actively investigated in the AI community. While AI has made dramatic progress over the last decade in areas such as vision, natural language processing, and robotics, current AI systems still almost entirely lack the ability to form humanlike concepts and abstractions. Some cognitive scientists have proposed that analogy-making is a central mechanism for conceptual abstraction and understanding in humans. Douglas Hofstadter called analogy-making ``the core of cognition'', and Hofstadter and co-author Emmanuel Sander noted, ``Without concepts there can be no thought, and without analogies there can be no concepts.'' In this talk I will reflect on the role played by analogy-making at all levels of intelligence, and on how analogy-making abilities will be central in developing AI systems with humanlike intelligence.},
  file = {/home/harrisonpl/Documents/PDFs/Mitchell - 2020 - Conceptual Abstraction and Analogy in Artificial Intelligence.pdf;/home/harrisonpl/Zotero/storage/AK3TXZB9/isal_a_00354.html},
  journal = {Artificial Life Conference Proceedings}
}

@article{mohamedDiscoveringProteinDrug2020,
  ids = {Mohamed},
  title = {Discovering Protein Drug Targets Using Knowledge Graph Embeddings},
  author = {Mohamed, Sameh K. and Nov{\'a}{\v c}ek, V{\'i}t and Nounu, Aayah},
  year = {2020},
  volume = {36},
  pages = {603--610},
  issn = {13674811},
  doi = {10.1093/bioinformatics/btz600},
  abstract = {MOTIVATION: Computational approaches for predicting drug-target interactions (DTIs) can provide valuable insights into the drug mechanism of action. DTI predictions can help to quickly identify new promising (on-target) or unintended (off-target) effects of drugs. However, existing models face several challenges. Many can only process a limited number of drugs and/or have poor proteome coverage. The current approaches also often suffer from high false positive prediction rates. RESULTS: We propose a novel computational approach for predicting drug target proteins. The approach is based on formulating the problem as a link prediction in knowledge graphs (robust, machine-readable representations of networked knowledge). We use biomedical knowledge bases to create a knowledge graph of entities connected to both drugs and their potential targets. We propose a specific knowledge graph embedding model, TriModel, to learn vector representations (i.e. embeddings) for all drugs and targets in the created knowledge graph. These representations are consequently used to infer candidate drug target interactions based on their scores computed by the trained TriModel model. We have experimentally evaluated our method using computer simulations and compared it to five existing models. This has shown that our approach outperforms all previous ones in terms of both area under ROC and precision-recall curves in standard benchmark tests. AVAILABILITY AND IMPLEMENTATION: The data, predictions and models are available at: drugtargets.insight-centre.org. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
  file = {/home/harrisonpl/Documents/PDFs/Mohamed et al. - 2020 - Discovering protein drug targets using knowledge graph embeddings.pdf},
  journal = {Bioinformatics (Oxford, England)},
  number = {2}
}

@article{mokHisCoMPAGESoftwareHierarchical2019,
  ids = {Mok2019},
  title = {{{HisCoM}}-{{PAGE}}: {{Software}} for Hierarchical Structural Component Models for Pathway Analysis of Gene Expression Data},
  author = {Mok, Lydia and Park, Taesung},
  year = {2019},
  volume = {17},
  issn = {22340742},
  doi = {10.5808/GI.2019.17.4.e45},
  abstract = {To identify pathways associated with survival phenotypes using gene expression data, we recently proposed the hierarchical structural component model for pathway analysis of gene expression data (HisCoM-PAGE) method. The HisCoM-PAGE software can consider hierarchical structural relationships between genes and pathways and analyze multiple pathways simultaneously. It can be applied to various types of gene expression data, such as microarray data or RNA sequencing data. We expect that the HisCoM-PAGE software will make our method more easily accessible to researchers who want to perform pathway analysis for survival times.},
  file = {/home/harrisonpl/Documents/PDFs/Mok, Park - 2019 - HisCoM-PAGE.pdf},
  journal = {Genomics and Informatics},
  keywords = {Gene expression,Hierarchical component model,Pathway analysis,Survival phenotype},
  number = {4}
}

@article{montecchi-palazziPSIMODCommunityStandard2008,
  title = {The {{PSI}}-{{MOD}} Community Standard for Representation of Protein Modification Data},
  booktitle = {Nature Biotechnology},
  author = {{Montecchi-Palazzi}, Luisa and Beavis, Ron and Binz, Pierre Alain and Chalkley, Robert J. and Cottrell, John and Creasy, David and Shofstahl, Jim and Seymour, Sean L. and Garavelli, John S.},
  year = {2008},
  volume = {26},
  pages = {864--866},
  issn = {10870156},
  doi = {10.1038/nbt0808-864},
  file = {/home/harrisonpl/Documents/PDFs/Montecchi-Palazzi et al. - 2008 - The PSI-MOD community standard for representation of protein modification data.pdf},
  journal = {Nature Biotechnology},
  keywords = {ontology},
  mendeley-tags = {ontology},
  number = {8}
}

@article{moonVisualizingStructureTransitions2019,
  ids = {Moon2019},
  title = {Visualizing Structure and Transitions in High-Dimensional Biological Data},
  author = {Moon, Kevin R. and {van Dijk}, David and Wang, Zheng and Gigante, Scott and Burkhardt, Daniel B. and Chen, William S. and Yim, Kristina and {van den Elzen}, Antonia and Hirn, Matthew J. and Coifman, Ronald R. and Ivanova, Natalia B. and Wolf, Guy and Krishnaswamy, Smita},
  year = {2019},
  volume = {37},
  pages = {1482--1492},
  publisher = {{Springer US}},
  issn = {15461696},
  doi = {10.1038/s41587-019-0336-3},
  abstract = {The high-dimensional data created by high-throughput technologies require visualization tools that reveal data structure and patterns in an intuitive form. We present PHATE, a visualization method that captures both local and global nonlinear structure using an information-geometric distance between data points. We compare PHATE to other tools on a variety of artificial and biological datasets, and find that it consistently preserves a range of patterns in data, including continual progressions, branches and clusters, better than other tools. We define a manifold preservation metric, which we call denoised embedding manifold preservation (DEMaP), and show that PHATE produces lower-dimensional embeddings that are quantitatively better denoised as compared to existing visualization methods. An analysis of a newly generated single-cell RNA sequencing dataset on human germ-layer differentiation demonstrates how PHATE reveals unique biological insight into the main developmental branches, including identification of three previously undescribed subpopulations. We also show that PHATE is applicable to a wide variety of data types, including mass cytometry, single-cell RNA sequencing, Hi-C and gut microbiome data.},
  file = {/home/harrisonpl/Documents/PDFs/Moon et al. - 2019 - Visualizing structure and transitions in high-dimensional biological data.pdf},
  journal = {Nature Biotechnology},
  number = {12}
}

@article{morzyMeasuringComplexityNetworks2017,
  ids = {Morzy2017},
  title = {On Measuring the Complexity of Networks: {{Kolmogorov}} Complexity versus Entropy},
  shorttitle = {On {{Measuring}} the {{Complexity}} of {{Networks}}},
  booktitle = {Complexity},
  author = {Morzy, Miko{\textbackslash}laj and Kajdanowicz, Tomasz and Kazienko, Przemys{\textbackslash}law},
  year = {2017},
  volume = {2017},
  issn = {10990526},
  doi = {10.1155/2017/3250301},
  abstract = {One of the most popular methods of estimating the complexity of networks is to measure the entropy of network invariants, such as adjacency matrices or degree sequences. Unfortunately, entropy and all entropy-based information-theoretic measures have several vulnerabilities. These measures neither are independent of a particular representation of the network nor can capture the properties of the generative process, which produces the network. Instead, we advocate the use of the algorithmic entropy as the basis for complexity definition for networks. Algorithmic entropy (also known as Kolmogorov complexity or K-complexity for short) evaluates the complexity of the description required for a lossless recreation of the network. This measure is not affected by a particular choice of network features and it does not depend on the method of network representation. We perform experiments on Shannon entropy and K-complexity for gradually evolving networks. The results of these experiments point to K-complexity as the more robust and reliable measure of network complexity. The original contribution of the paper includes the introduction of several new entropy-deceiving networks and the empirical comparison of entropy and K-complexity as fundamental quantities for constructing complexity measures for networks.},
  file = {/home/harrisonpl/Documents/PDFs/Morzy et al. - 2017 - On measuring the complexity of networks.pdf},
  journal = {Complexity},
  language = {en}
}

@article{mostovoyHybridApproachNovo2016,
  ids = {Mostovoy2016},
  title = {A Hybrid Approach for de Novo Human Genome Sequence Assembly and Phasing},
  author = {Mostovoy, Yulia and {Levy-Sakin}, Michal and Lam, Jessica and Lam, Ernest T. and Hastie, Alex R. and Marks, Patrick and Lee, Joyce and Chu, Catherine and Lin, Chin and Dzakula, Zeljko and Cao, Han and Schlebusch, Stephen A. and Giorda, Kristina and {Schnall-Levin}, Michael and Wall, Jeffrey D. and Kwok, Pui Yan},
  year = {2016},
  volume = {13},
  pages = {587--590},
  issn = {15487105},
  doi = {10.1038/nmeth.3865},
  abstract = {Despite tremendous progress in genome sequencing, the basic goal of producing a phased (haplotype-resolved) genome sequence with end-to-end contiguity for each chromosome at reasonable cost and effort is still unrealized. In this study, we describe an approach to performing de novo genome assembly and experimental phasing by integrating the data from Illumina short-read sequencing, 10X Genomics linked-read sequencing, and BioNano Genomics genome mapping to yield a high-quality, phased, de novo assembled human genome.},
  file = {/home/harrisonpl/Documents/PDFs/Mostovoy et al. - 2016 - A hybrid approach for de novo human genome sequence assembly and phasing.pdf},
  journal = {Nature Methods},
  keywords = {DNA sequencing,Genomics,Haplotypes},
  number = {7}
}

@article{mostovoyHybridApproachNovo2017,
  ids = {Mostovoy2017},
  title = {A {{Hybrid Approach}} for de Novo {{Human Genome Sequence Assembly}} and {{Phasing}}},
  author = {Mostovoy, Yulia and {Levy-Sakin}, Michal and Lam, Jessica and Lam, Ernest T and Hastie, Alex R and Marks, Patrick and Lee, Joyce and Chu, Catherine and Lin, Chin and D{\v z}akula, {\v Z}eljko and Cao, Han and Schlebusch, Stephen A. and Giorda, Kristina and {Schnall-Levin}, Michael and Wall, Jeffrey D. and Kwok, Pui-Yan},
  year = {2017},
  volume = {47},
  pages = {549--562},
  doi = {10.1097/CCM.0b013e31823da96d.Hydrogen},
  abstract = {Chromosome 15q11q13 is among the least stable regions in the genome due to its highly complex genomic architecture. Low copy repeat elements at 15q13.3 facilitate recurrent copy number variants (CNVs), with deletions established as pathogenic and CHRN CHRN A7 implicated as a candidate gene. However, the pathogenicity of duplications of A7 is unclear, as they are also found in reportedly healthy parents and unaffected control individuals. We evaluated 18 children with microduplications involving CHRN A7 identified by clinical chromosome microarray analysis (CMA). Comprehensive phenotyping revealed high prevalence of developmental delay/intellectual disability, autism spectrum disorder, and attention deficit/hyperactivity disorder. As duplications are the most common CNVs identified by clinical CMA, this study provides CHRN A7 anticipatory guidance for those involved with care of affected individuals. Keywords},
  file = {/home/harrisonpl/Documents/PDFs/Mostovoy et al. - 2017 - A Hybrid Approach for de novo Human Genome Sequence Assembly and Phasing.pdf},
  journal = {Author Manuscript},
  keywords = {free radicals,hydrogen,oxidative stress,subarachnoid hemorrhage},
  number = {3}
}

@article{mowerLearningPredictiveModels2018,
  ids = {Mower2018},
  title = {Learning Predictive Models of Drug Side-Effect Relationships from Distributed Representations of Literature-Derived Semantic Predications},
  author = {Mower, Justin and Subramanian, Devika and Cohen, Trevor},
  year = {2018},
  volume = {25},
  pages = {1339--1350},
  issn = {1527974X},
  doi = {10.1093/jamia/ocy077},
  abstract = {Objective The aim of this work is to leverage relational information extracted from biomedical literature using a novel synthesis of unsupervised pretraining, representational composition, and supervised machine learning for drug safety monitoring. Methods Using \^a \textperthousand 80 million concept-relationship-concept triples extracted from the literature using the SemRep Natural Language Processing system, distributed vector representations (embeddings) were generated for concepts as functions of their relationships utilizing two unsupervised representational approaches. Embeddings for drugs and side effects of interest from two widely used reference standards were then composed to generate embeddings of drug/side-effect pairs, which were used as input for supervised machine learning. This methodology was developed and evaluated using cross-validation strategies and compared to contemporary approaches. To qualitatively assess generalization, models trained on the Observational Medical Outcomes Partnership (OMOP) drug/side-effect reference set were evaluated against a list of \^a \textperthousand 1100 drugs from an online database. Results The employed method improved performance over previous approaches. Cross-validation results advance the state of the art (AUC 0.96; F1 0.90 and AUC 0.95; F1 0.84 across the two sets), outperforming methods utilizing literature and/or spontaneous reporting system data. Examination of predictions for unseen drug/side-effect pairs indicates the ability of these methods to generalize, with over tenfold label support enrichment in the top 100 predictions versus the bottom 100 predictions. Discussion and Conclusion Our methods can assist the pharmacovigilance process using information from the biomedical literature. Unsupervised pretraining generates a rich relationship-based representational foundation for machine learning techniques to classify drugs in the context of a putative side effect, given known examples.},
  file = {/home/harrisonpl/Documents/PDFs/Mower et al. - 2018 - Learning predictive models of drug side-effect relationships from distributed.pdf},
  journal = {Journal of the American Medical Informatics Association},
  keywords = {literature based discovery,machine learning,pharmacovigilance,representation learning,unsupervised pretraining},
  number = {10}
}

@misc{mukherjee,
  title = {A.{{I}}. {{Versus M}}.{{D}}.},
  author = {Mukherjee, Siddhartha},
  abstract = {What happens when diagnosis is automated?},
  file = {/home/harrisonpl/Zotero/storage/9H7JPBYM/ai-versus-md.html},
  howpublished = {https://www.newyorker.com/magazine/2017/04/03/ai-versus-md},
  journal = {The New Yorker},
  language = {en-us}
}

@article{mungallUberonIntegrativeMultispecies2012,
  ids = {Mungall2012},
  title = {Uberon, an Integrative Multi-Species Anatomy Ontology},
  author = {Mungall, Christopher J. and Torniai, Carlo and Gkoutos, Georgios V. and Lewis, Suzanna E. and Haendel, Melissa A.},
  year = {2012},
  volume = {13},
  pages = {R5},
  issn = {14747596},
  doi = {10.1186/gb-2012-13-1-r5},
  abstract = {We present Uberon, an integrated cross-species ontology consisting of over 6,500 classes representing a variety of anatomical entities, organized according to traditional anatomical classification criteria. The ontology represents structures in a species-neutral way and includes extensive associations to existing species-centric anatomical ontologies, allowing integration of model organism and human data. Uberon provides a necessary bridge between anatomical structures in different taxa for cross-species inference. It uses novel methods for representing taxonomic variation, and has proved to be essential for translational phenotype analyses. Uberon is available at http://uberon.org. \textcopyright{} 2012 Mungall et al.; licensee BioMed Central Ltd.},
  file = {/home/harrisonpl/Documents/PDFs/Mungall et al. - 2012 - Uberon, an integrative multi-species anatomy ontology.pdf},
  journal = {Genome Biology},
  keywords = {Anatomy Ontology,Gene Ontology,Human Phenotype Ontology,Logical Definition,ontology,Open Biomedical Ontology},
  mendeley-tags = {ontology},
  number = {1}
}

@inproceedings{mutharajuDistributedOWLReasoning2014,
  ids = {Mutharaju2014},
  title = {Distributed {{OWL EL}} Reasoning: {{The}} Story so Far},
  shorttitle = {Distributed {{OWL EL Reasoning}}},
  booktitle = {{{CEUR Workshop Proceedings}}},
  author = {Mutharaju, Raghava and Hitzler, Pascal and Mateti, Prabhaker},
  year = {2014},
  volume = {1261},
  pages = {61--76},
  issn = {16130073},
  abstract = {Automated generation of axioms from streaming data, such as traffic and text, can result in very large ontologies that single machine reasoners cannot handle. Reasoning with large ontologies requires distributed solutions. Scalable reasoning techniques for RDFS, OWL Horst and OWL 2 RL now exist. For OWL 2 EL, several distributed reasoning approaches have been tried, but are all perceived to be inefficient. We analyze this perception. We analyze completion rule based distributed approaches, using different characteristics, such as dependency among the rules, implementation optimizations, how axioms and rules are distributed. We also present a distributed queue approach for the classification of ontologies in description logic EL+ (fragment of OWL 2 EL).},
  file = {/home/harrisonpl/Documents/PDFs/Mutharaju et al. - 2014 - Distributed OWL EL reasoning.pdf},
  keywords = {Description logic,Distributed Computing,High Performance,ontology,Ontology (information science),RDF Schema,Streaming media,Web Ontology Language},
  mendeley-tags = {ontology}
}

@article{mutharajuDistributedRuleBasedOntology2016,
  title = {Distributed {{Rule}}-{{Based Ontology Reasoning}}},
  author = {Mutharaju, Raghava},
  year = {2016},
  pages = {173},
  keywords = {\#nosource,distributed reason,ontology,Rule-based reasoning}
}

@inproceedings{mutharajuDistributedScalableOWL2015,
  title = {Distributed and Scalable {{OWL EL}} Reasoning},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Mutharaju, Raghava and Hitzler, Pascal and Matet, Prabhaker and L{\`e}cu{\`e}, Freddy},
  editor = {Gandon, Fabien and Sabou, Marta and Sack, Harald and D'Amato, Claudia and {Cudr{\'e}-Mauroux}, Philippe and Zimmermann, Antoine},
  year = {2015},
  volume = {9088},
  pages = {88--108},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-18818-8_6},
  abstract = {OWL 2 EL is one of the tractable profiles of the Web Ontology Language (OWL) which is a W3C-recommended standard. OWL 2 EL provides sufficient expressivity to model large biomedical ontologies as well as streaming data such as traffic, while at the same time allows for efficient reasoning services. Existing reasoners for OWL 2 EL, however, use only a single machine and are thus constrained by memory and computational power. At the same time, the automated generation of ontological information from streaming data and text can lead to very large ontologies which can exceed the capacities of these reasoners. We thus describe a distributed reasoning system that scales well using a cluster of commodity machines. We also apply our system to a use case on city traffic data and show that it can handle volumes which cannot be handled by current single machine reasoners.},
  file = {/home/harrisonpl/Documents/PDFs/Mutharaju et al. - 2015 - Distributed and scalable OWL EL reasoning.pdf},
  isbn = {978-3-319-18817-1},
  keywords = {\#nosource,Description logic,Distributed Computing,elk,High Performance,ontology,Ontology (information science),owl,reasoner,Web Ontology Language},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{nagAmericanBrachytherapySociety2000,
  ids = {Nag,Nag2000},
  title = {The {{American Brachytherapy Society}} Recommendations for Permanent Prostate Brachytherapy Postimplant Dosimetric Analysis},
  author = {Nag, Subir and Bice, William and DeWyngaert, Keith and Prestidge, Bradley and Stock, Richard and Yu, Yan},
  year = {2000},
  volume = {46},
  pages = {221--230},
  issn = {03603016},
  doi = {10.1016/S0360-3016(99)00351-X},
  abstract = {Purpose: The purpose of this report is to establish guidelines for postimplant dosimetric analysis of permanent prostate brachytherapy. Methods: Members of the American Brachytherapy Society (ABS) with expertise in prostate dosimetry evaluation performed a literature review and supplemented with their clinical experience formulated guidelines for performing and analyzing postimplant dosimetry of permanent prostate brachytherapy.Results: The ABS recommends that postimplant dosimetry should be performed on all patients undergoing permanent prostate brachytherapy for optimal patient care. At present, computed tomography (CT)-based dosimetry is recommended, based on availability cost and the ability to image the prostate as well as the seeds. Additional plane radiographs should be obtained to verify the seed count. Until the ideal postoperative interval for CT scanning has been determined, each center should perform dosimetric evaluation of prostate implants at a consistent postoperative interval. This interval should be reported. Isodose displays should be obtained at 50\%, 80\%, 90\%, 100\%, 150\%, and 200\% of the prescription dose and displayed on multiple cross-sectional images of the prostate. A dose-volume histogram (DVH) of the prostate should be performed and the D90 (dose to 90\% of the prostate gland) reported by all centers. Additionally, the D80, D100, the fractional V80, V90, V100, V150, and V200, (i.e., the percentage of prostate volume receiving 80\%, 90\%, 100\%, 150\%, and 200\% of the prescribed dose, respectively), the rectal, and urethral doses should be reported and ultimately correlated with clinical outcome in the research environment. On-line real-time dosimetry, the effects of dose heterogeneity, and the effects of tissue heterogeneity need further investigation. Conclusion: It is essential that postimplant dosimetry should be performed on all patients undergoing permanent prostate brachytherapy. Guidelines were established for the performance and analysis of such dosimetry. Copyright (C) 2000 Elsevier Science Inc.},
  file = {/home/harrisonpl/Documents/PDFs/Nag et al. - 2000 - The American Brachytherapy Society recommendations for permanent prostate.pdf},
  journal = {International Journal of Radiation Oncology Biology Physics},
  keywords = {Brachytherapy,Dosimetry,Guidelines,Iodine,Palladium,Prostate neoplasm},
  number = {1},
  pmid = {10656396}
}

@article{napolitanoGep2PepBioconductorPackage2019,
  ids = {Napolitano2019b},
  title = {{{Gep2Pep}}: A {{Bioconductor Package}} for the {{Creation}} and {{Analysis}} of {{Pathway}}-{{Based Expression Profiles}}},
  author = {Napolitano, Farancesco and Carrella, Diego and Gao, Xin and {di Bernardo}, Diego},
  year = {2019},
  pages = {1--2},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btz803},
  file = {/home/harrisonpl/Documents/PDFs/Napolitano et al. - 2019 - Gep2Pep.pdf},
  journal = {Bioinformatics},
  number = {October}
}

@incollection{nareshImpactMachineLearning2020,
  title = {Impact of {{Machine Learning}} in {{Bioinformatics Research}}},
  author = {Naresh, E. and Vijaya Kumar, B. P. and {Ayesha} and Shankar, Sahana P.},
  year = {2020},
  pages = {41--62},
  publisher = {{Springer, Singapore}},
  keywords = {\#nosource}
}

@article{nataleProteinOntologyStructured2011,
  ids = {Natale2011},
  title = {The {{Protein Ontology}}: {{A}} Structured Representation of Protein Forms and Complexes},
  shorttitle = {The {{Protein Ontology}}},
  author = {Natale, Darren A. and Arighi, Cecilia N. and Barker, Winona C. and Blake, Judith A. and Bult, Carol J. and Caudy, Michael and Drabkin, Harold J. and D'Eustachio, Peter and Evsikov, Alexei V. and Huang, Hongzhan and Nchoutmboube, Jules and Roberts, Natalia V. and Smith, Barry and Zhang, Jian and Wu, Cathy H.},
  year = {2011},
  volume = {39},
  pages = {D539--D545},
  issn = {03051048},
  doi = {10.1093/nar/gkq907},
  abstract = {The Protein Ontology (PRO) provides a formal, logically-based classification of specific protein classes including structured representations of protein isoforms, variants and modified forms. Initially focused on proteins found in human, mouse and Escherichia coli, PRO now includes representations of protein complexes. The PRO Consortium works in concert with the developers of other biomedical ontologies and protein knowledge bases to provide the ability to formally organize and integrate representations of precise protein forms so as to enhance accessibility to results of protein research. PRO (http://pir .georgetown.edu/pro) is part of the Open Biomedical Ontology Foundry. \textcopyright{} The Author(s) 2010.},
  file = {/home/harrisonpl/Documents/PDFs/Natale et al. - 2011 - The Protein Ontology.pdf},
  journal = {Nucleic Acids Research},
  keywords = {ontology},
  mendeley-tags = {ontology},
  number = {SUPPL. 1},
  pmid = {20935045}
}

@article{nayakEffectiveModelingEncoderDecoder2020,
  ids = {Nayak2020},
  title = {Effective {{Modeling}} of {{Encoder}}-{{Decoder Architecture}} for {{Joint Entity}} and {{Relation Extraction}}},
  author = {Nayak, Tapas and Ng, Hwee Tou},
  year = {2020},
  abstract = {A relation tuple consists of two entities and the relation between them, and often such tuples are found in unstructured text. There may be multiple relation tuples present in a text and they may share one or both entities among them. Extracting such relation tuples from a sentence is a difficult task and sharing of entities or overlapping entities among the tu-ples makes it more challenging. Most prior work adopted a pipeline approach where entities were identified first followed by finding the relations among them, thus missing the interaction among the relation tuples in a sentence. In this paper, we propose two approaches to use encoder-decoder architecture for jointly extracting entities and relations. In the first approach, we propose a representation scheme for relation tuples which enables the decoder to generate one word at a time like machine translation models and still finds all the tuples present in a sentence with full entity names of different length and with overlapping entities. Next, we propose a pointer network-based decoding approach where an entire tu-ple is generated at every time step. Experiments on the publicly available New York Times corpus show that our proposed approaches outperform previous work and achieve significantly higher F1 scores.},
  file = {/home/harrisonpl/Documents/PDFs/Nayak, Ng - 2020 - Effective Modeling of Encoder-Decoder Architecture for Joint Entity and.pdf},
  journal = {Aaai}
}

@article{naylorMetaanalysisControlledClinical1989,
  ids = {Naylor1989},
  title = {Meta-Analysis of Controlled Clinical Trials.},
  author = {Naylor, C. D.},
  year = {1989},
  volume = {16},
  issn = {0315162X},
  doi = {10.1002/0470854200},
  abstract = {Over the last twenty years there has been a dramatic upsurge in the application of meta-analysis to medical research. This has mainly been due to greater emphasis on evidence-based medicine and the need for reliable summaries of the vast and expanding volume of clinical research. At the same time there have been great strides in the development and refinement of the associated statistical methodology. This book describes the planning, conduct and reporting of a meta-analysis as applied to a series of randomized controlled clinical trials. * The various approaches are presented within a general unified framework. * Meta-analysis techniques are described in detail, from their theoretical development through to practical implementation. * Each topic discussed is supported by detailed worked examples. * A comparison of fixed and random effects approaches is included, as well as a discussion of Bayesian methods and cumulative meta-analysis. * Fully documented programs using standard statistical procedures in SAS are available on the Web. Ideally suited for practising statisticians and statistically-minded medical professionals, the book will also be of use to graduate students of medical statistics. The book is a self-contained and comprehensive account of the subject and an essential purchase for anyone involved in clinical trials.},
  file = {/home/harrisonpl/Documents/PDFs/Naylor - 1989 - Meta-analysis of controlled clinical trials.pdf},
  journal = {Journal of Rheumatology},
  keywords = {Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochasti}
}

@misc{NCATSPharmaceuticalCollection,
  title = {{{NCATS Pharmaceutical Collection}}},
  abstract = {The NCATS Pharmaceutical Collection is a comprehensive, publicly accessible collection of approved and investigational molecular entities for high-throughput screening.},
  howpublished = {https://ncats.nih.gov/expertise/preclinical/npc},
  keywords = {\#nosource}
}

@article{nessFruitVegetablesCardiovascular1997,
  ids = {Ness1997,Ness1997a,Ness1997b},
  title = {Fruit and Vegetables, and Cardiovascular Disease: {{A}} Review},
  shorttitle = {Fruit and Vegetables, and Cardiovascular Disease},
  booktitle = {International Journal of Epidemiology},
  author = {Ness, Andrew R. and Powles, John W.},
  year = {1997},
  volume = {26},
  pages = {1--13},
  issn = {03005771},
  doi = {10.1093/ije/26.1.1},
  abstract = {Background. Increased interest in the potential cardio-protective effects of fruit and vegetables is currently unsupported by systematic reviews of the reported associations of these foods with risk. Method. All ecological, case-control, cohort studies and unconfounded trials in humans were eligible for inclusion. Eligible outcomes were symptomatic coronary heart disease, stroke and total circulatory disease. Only studies of diet that reported on fresh fruit and vegetables or a nutrient which could serve as a proxy (reversing the usual direction of inference) were included. MEDLINE (1966-1995) and EMBASE (1980-1995) were searched using the terms cerebrovascular disorder, coronary heart disease, fruit(s) and vegetable(s) as keywords. Personal bibliographies, books and reviews were also searched, as were citations in located reports. Results. For coronary heart disease nine of ten ecological studies, two of three case-control studies and six of 16 cohort studies found a significant protective association with consumption of fruit and vegetables or surrogate nutrients. For stroke three of five ecological studies, none (of one) case-control study and six of eight cohort studies found a significant protective association with consumption of fruit and vegetables or surrogate nutrients. For total circulatory disease, one of two cohort studies reported a significant protective association. No attempt was made to arrive at a summary measure of the association because of the differences in study type, study quality and the different exposure measures used. Conclusions. Although null findings may be underreported the results are consistent with a strong protective effect of fruit and vegetables for stroke and a weaker protective effect on coronary heart disease. Greater use of food-based hypotheses and analyses, would complement existing nutrient-based analyses and help guide the search for underlying causes.},
  file = {/home/harrisonpl/Documents/PDFs/Ness, Powles - 1997 - Fruit and vegetables, and cardiovascular disease.pdf},
  journal = {International Journal of Epidemiology},
  keywords = {cardiovascular disease,Coronary heart disease,Fruit,Stroke,Systematic review,Vegetables},
  mendeley-tags = {cardiovascular disease},
  number = {1}
}

@article{nevesBf3RSemEval2018Task2018,
  ids = {Neves2018},
  title = {{{Bf3R}} at {{SemEval}}-2018 {{Task}} 7: {{Evaluating Two Relation Extraction Tools}} for {{Finding Semantic Relations}} in {{Biomedical Abstracts}}},
  author = {Neves, Mariana and Butzke, Daniel and Sch{\"o}nfelder, Gilbert and Grune, Barbara},
  year = {2018},
  pages = {816--820},
  doi = {10.18653/v1/s18-1130},
  abstract = {Automatic extraction of semantic relations from text can support finding relevant information from scientific publications. We describe our participation in Task 7 of SemEval-2018 for which we experimented with two relations extraction tools - jSRE and TEES - for the extraction and classification of six relation types. The results we obtained with TEES were significantly superior than those with jSRE (33.4\% vs. 30.09\% and 20.3\% vs. 16\%). Additionally, we utilized the model trained with TEES for extracting semantic relations from biomedical abstracts, for which we present a preliminary evaluation.},
  file = {/home/harrisonpl/Documents/PDFs/Neves et al. - 2018 - Bf3R at SemEval-2018 Task 7.pdf}
}

@article{nevesExtensiveReviewTools2019,
  ids = {Neves2019},
  title = {An Extensive Review of Tools for Manual Annotation of Documents},
  author = {Neves, Mariana and {\v S}eva, Jurica},
  year = {2019},
  issn = {1477-4054},
  doi = {10.1093/bib/bbz130},
  abstract = {Annotation tools are applied to build training and test corpora, which are essential for the development and evaluation of new natural language processing algorithms. Further, annotation tools are also used to extract new information for a particular use case. However, owing to the high number of existing annotation tools, finding the one that best fits particular needs is a demanding task that requires searching the scientific literature followed by installing and trying various tools. Methods We searched for annotation tools and selected a subset of them according to five requirements with which they should comply, such as being Web-based or supporting the definition of a schema. We installed the selected tools (when necessary), carried out hands-on experiments and evaluated them using 26 criteria that covered functional and technical aspects. We defined each criterion on three levels of matches and a score for the final evaluation of the tools. Results We evaluated 78 tools and selected the following 15 for a detailed evaluation: BioQRator, brat, Catma, Djangology, ezTag, FLAT, LightTag, MAT, MyMiner, PDFAnno, prodigy, tagtog, TextAE, WAT-SL and WebAnno. Full compliance with our 26 criteria ranged from only 9 up to 20 criteria, which demonstrated that some tools are comprehensive and mature enough to be used on most annotation projects. The highest score of 0.81 was obtained by WebAnno (of a maximum value of 1.0).},
  file = {/home/harrisonpl/Documents/PDFs/Neves, Ševa - 2019 - An extensive review of tools for manual annotation of documents.pdf},
  journal = {Briefings in Bioinformatics},
  keywords = {Annotation,annotation tools,corpus construction,HPL comprehensive exam,Knowtator,manual annotation,Review},
  language = {en},
  mendeley-tags = {Annotation,HPL comprehensive exam,Knowtator,Review}
}

@article{neyGenotypeExtractionFalse2020,
  ids = {Ney2020},
  title = {Genotype {{Extraction}} and {{False Relative Attacks}}: {{Security Risks}} to {{Third}}-{{Party Genetic Genealogy Services Beyond Identity Inference}}},
  author = {Ney, Peter and Ceze, Luis and Kohno, Tadayoshi and Allen, Paul G},
  year = {2020},
  abstract = {Customers of direct-to-consumer (DTC) genetic testing services routinely download their raw genetic data and give it to third-party companies that support additional features. One type of analysis, called genetic genealogy, uses genetic data and genealogical methods to find new relatives. While genetic genealogy is quite popular, it has raised new privacy concerns. Genetic genealogy services can be leveraged to find the person corresponding to anonymous genetic data and have been used dozens of times by law enforcement to solve crimes. We hypothesized that the open design and broad API offered by some genetic genealogy services raise other significant security and privacy issues. To test this hypothesis, we analyzed the security practices of GEDmatch, the largest third-party genetic genealogy service. Here, we experimentally show how the GEDmatch API is vulnerable to a number of attacks from an adversary that only uploads normally formatted genetic data files and runs standard queries. Using a small number of specifically designed files and queries, an attacker can extract a large percentage of the genetic markers from other users; 92\% of markers can be extracted with 98\% accuracy, including hundreds of medically sensitive markers. We also find that an adversary can construct genetic data files that falsely appear like relatives to other samples in the database; in certain situations, these false relatives can be used to make the re-identification of genetic data more difficult. These attacks are possible because of the rich set of features supported by the API, including detailed visualizations, that are meant to enhance usability. We conclude with security recommendations for genetic genealogy services.},
  file = {/home/harrisonpl/Documents/PDFs/Ney et al. - 2020 - Genotype Extraction and False Relative Attacks.pdf},
  isbn = {1891562614},
  number = {February}
}

@article{nguyenIdentifyingSignificantlyImpacted2019,
  ids = {Nguyen2019a},
  title = {Identifying Significantly Impacted Pathways: {{A}} Comprehensive Review and Assessment},
  author = {Nguyen, Tuan Minh and Shafi, Adib and Nguyen, Tin and Draghici, Sorin},
  year = {2019},
  volume = {20},
  pages = {1--15},
  issn = {1474760X},
  doi = {10.1186/s13059-019-1790-4},
  abstract = {Background: Many high-throughput experiments compare two phenotypes such as disease vs. healthy, with the goal of understanding the underlying biological phenomena characterizing the given phenotype. Because of the importance of this type of analysis, more than 70 pathway analysis methods have been proposed so far. These can be categorized into two main categories: non-topology-based (non-TB) and topology-based (TB). Although some review papers discuss this topic from different aspects, there is no systematic, large-scale assessment of such methods. Furthermore, the majority of the pathway analysis approaches rely on the assumption of uniformity of p values under the null hypothesis, which is often not true. Results: This article presents the most comprehensive comparative study on pathway analysis methods available to date. We compare the actual performance of 13 widely used pathway analysis methods in over 1085 analyses. These comparisons were performed using 2601 samples from 75 human disease data sets and 121 samples from 11 knockout mouse data sets. In addition, we investigate the extent to which each method is biased under the null hypothesis. Together, these data and results constitute a reliable benchmark against which future pathway analysis methods could and should be tested. Conclusion: Overall, the result shows that no method is perfect. In general, TB methods appear to perform better than non-TB methods. This is somewhat expected since the TB methods take into consideration the structure of the pathway which is meant to describe the underlying phenomena. We also discover that most, if not all, listed approaches are biased and can produce skewed results under the null.},
  file = {/home/harrisonpl/Documents/PDFs/Nguyen et al. - 2019 - Identifying significantly impacted pathways2.pdf},
  journal = {Genome Biology},
  keywords = {Bias,Metabolic pathways,Network topology,Pathway analysis,Signaling pathways,Statistical significance},
  number = {1}
}

@article{nguyenTenQuickTips2019,
  ids = {Nguyen2019},
  title = {Ten Quick Tips for Effective Dimensionality Reduction},
  author = {Nguyen, Lan Huong and Holmes, Susan},
  editor = {Ouellette, Francis},
  year = {2019},
  month = jun,
  volume = {15},
  pages = {e1006907},
  publisher = {{Public Library of Science}},
  issn = {15537358},
  doi = {10.1371/journal.pcbi.1006907},
  file = {/home/harrisonpl/Documents/PDFs/Nguyen, Holmes - 2019 - Ten quick tips for effective dimensionality reduction.pdf},
  journal = {PLoS Computational Biology},
  number = {6}
}

@article{nicholsonConceptMechanismBiology2012,
  title = {The Concept of Mechanism in Biology},
  author = {Nicholson, Daniel J.},
  year = {2012},
  month = mar,
  volume = {43},
  pages = {152--163},
  issn = {13698486},
  doi = {10.1016/j.shpsc.2011.05.014},
  abstract = {The concept of mechanism in biology has three distinct meanings. It may refer to a philosophical thesis about the nature of life and biology (`mechanicism'), to the internal workings of a machine-like structure (`machine mechanism'), or to the causal explanation of a particular phenomenon (`causal mechanism'). In this paper I trace the conceptual evolution of `mechanism' in the history of biology, and I examine how the three meanings of this term have come to be featured in the philosophy of biology, situating the new `mechanismic program' in this context. I argue that the leading advocates of the mechanismic program (i.e., Craver, Darden, Bechtel, etc.) inadvertently conflate the different senses of `mechanism'. Specifically, they all inappropriately endow causal mechanisms with the ontic status of machine mechanisms, and this invariably results in problematic accounts of the role played by mechanism-talk in scientific practice. I suggest that for effective analyses of the concept of mechanism, causal mechanisms need to be distinguished from machine mechanisms, and the new mechanismic program in the philosophy of biology needs to be demarcated from the traditional concerns of mechanistic biology.},
  file = {/home/harrisonpl/Documents/PDFs/Nicholson - 2012 - The concept of mechanism in biology.pdf},
  journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
  language = {en},
  number = {1}
}

@article{nielsenAntibioticLethalityImpacted2019,
  ids = {Nielsen2019},
  title = {Antibiotic {{Lethality Is Impacted}} by {{Nutrient Availabilities}}: {{New Insights}} from {{Machine Learning}}},
  author = {Nielsen, Jens},
  year = {2019},
  volume = {177},
  pages = {1373--1374},
  publisher = {{Elsevier Inc.}},
  issn = {10974172},
  doi = {10.1016/j.cell.2019.05.015},
  abstract = {In this issue of Cell, Yang, Wright et al. describe a machine learning approach that that can provide mechanistic insight from chemical screens. They use this approach to uncover how the nutritional availability for Escherichia coli impacts lethality toward three widely used antibiotics.},
  file = {/home/harrisonpl/Documents/PDFs/Nielsen - 2019 - Antibiotic Lethality Is Impacted by Nutrient Availabilities.pdf},
  journal = {Cell},
  number = {6}
}

@article{nikfarjamPharmacovigilanceSocialMedia2015,
  ids = {Nikfarjam2015},
  title = {Pharmacovigilance from Social Media: {{Mining}} Adverse Drug Reaction Mentions Using Sequence Labeling with Word Embedding Cluster Features},
  author = {Nikfarjam, Azadeh and Sarker, Abeed and O'Connor, Karen and Ginn, Rachel and Gonzalez, Graciela},
  year = {2015},
  month = may,
  volume = {22},
  pages = {671--681},
  issn = {1527974X},
  doi = {10.1093/jamia/ocu041},
  abstract = {Objective Social media is becoming increasingly popular as a platform for sharing personal health-related information. This information can be utilized for public health monitoring tasks, particularly for pharmacovigilance, via the use of natural language processing (NLP) techniques. However, the language in social media is highly informal, and userexpressed medical concepts are often nontechnical, descriptive, and challenging to extract. There has been limited progress in addressing these challenges, and thus far, advanced machine learning-based NLP techniques have been underutilized. Our objective is to design a machine learning-based approach to extract mentions of adverse drug reactions (ADRs) from highly informal text in social media. Methods: We introduce ADRMine, a machine learning-based concept extraction system that uses conditional random fields (CRFs). ADRMine utilizes a variety of features, including a novel feature for modeling words' semantic similarities. The similarities are modeled by clustering words based on unsupervised, pretrained word representation vectors (embeddings) generated from unlabeled user posts in social media using a deep learning technique. Results: ADRMine outperforms several strong baseline systems in the ADR extraction task by achieving an F-measure of 0.82. Feature analysis demonstrates that the proposed word cluster features significantly improve extraction performance. Conclusion: It is possible to extract complex medical concepts, with relatively high performance, from informal, usergenerated content. Our approach is particularly scalable, suitable for social media mining, as it relies on large volumes of unlabeled data, thus diminishing the need for large, annotated training data sets.},
  file = {/home/harrisonpl/Documents/PDFs/Nikfarjam et al. - 2015 - Pharmacovigilance from social media.pdf},
  journal = {Journal of the American Medical Informatics Association},
  keywords = {ADR,Adverse drug reaction,Deep learning word embeddings,Machine learning,Natural language processing,Pharmacovigilance,Social media mining},
  number = {3},
  pmid = {25755127}
}

@article{nodzenskiMetabomxtrPackageMixturemodel2014,
  ids = {Nodzenski2014},
  title = {Metabomxtr: {{An R}} Package for Mixture-Model Analysis of Non-Targeted Metabolomics Data},
  shorttitle = {Metabomxtr},
  author = {Nodzenski, Michael and Muehlbauer, Michael J. and Bain, James R. and Reisetter, Anna C. and Lowe, William L. and Scholtens, Denise M.},
  year = {2014},
  volume = {30},
  pages = {3287--3288},
  issn = {14602059},
  doi = {10.1093/bioinformatics/btu509},
  abstract = {Non-targeted metabolomics technologies often yield data in which abundance for any given metabolite is observed and quantified for some samples and reported as missing for other samples. Apparent missingness can be due to true absence of the metabolite in the sample or presence at a level below detectability. Mixture-model analysis can formally account for metabolite 'missingness' due to absence or undetectability, but software for this type of analysis in the high-throughput setting is limited. The R package metabomxtr has been developed to facilitate mixture-model analysis of non-targeted metabolomics data in which only a portion of samples have quantifiable abundance for certain metabolites.},
  file = {/home/harrisonpl/Documents/PDFs/Nodzenski et al. - 2014 - Metabomxtr.pdf},
  journal = {Bioinformatics},
  keywords = {Female,Humans,metabolomics,Models,Pregnancy,Software,Statistical},
  number = {22},
  pmid = {25075114}
}

@article{obaBayesianMissingValue2003,
  title = {A {{Bayesian}} Missing Value Estimation Method for Gene Expression Profile Data},
  author = {Oba, Shigeyuki and Sato, Masa Aki and Takemasa, Ichiro and Monden, Morito and Matsubara, Ken Ichi and Ishii, Shin},
  year = {2003},
  volume = {19},
  pages = {2088--2096},
  issn = {13674803},
  doi = {10.1093/bioinformatics/btg287},
  abstract = {Motivation: Gene expression profile analyses have been used in numerous studies covering a broad range of areas in biology. When unreliable measurements are excluded, missing values are introduced in gene expression profiles. Although existing multivariate, analysis methods have difficulty with the treatment of missing values, this problem has received little attention. There are many options for dealing with missing values, each of which reaches drastically different results. Ignoring missing values is the simplest method and is frequently applied. This approach, however, has its flaws. In this article, we propose an estimation method for missing values, which is based on Bayesian principal component analysis (BPCA). Although the methodology that a probabilistic model and latent variables are estimated simultaneously within the framework of Bayes inference is not new in principle, actual BPCA implementation that makes it possible to estimate arbitrary missing variables is new in terms of statistical methodology. Results: When applied to DNA microarray data from various experimental conditions, the BPCA method exhibited markedly better estimation ability than other recently proposed methods, such as singular value decomposition and K-nearest neighbors. While the estimation performance of existing methods depends on model parameters whose determination is difficult, our BPCA method is free from this difficulty. Accordingly, the BPCA method provides accurate and convenient estimation for missing values.},
  file = {/home/harrisonpl/Documents/PDFs/Oba et al. - 2003 - A Bayesian missing value estimation method for gene expression profile data.pdf},
  journal = {Bioinformatics},
  keywords = {Algorithms,Artifacts,Bayes Theorem,Cell Cycle,Colorectal Neoplasms,DNA,Gene Expression Profiling,Genetic,Humans,Models,Oligonucleotide Array Sequence Analysis,Principal Component Analysis,Quality Control,Reproducibility of Results,Sensitivity and Specificity,Sequence Analysis,Statistical,Yeasts},
  number = {16}
}

@article{ogrenCoordinationResolutionBiomedical2011,
  title = {Coordination Resolution in Biomedical Texts},
  author = {Ogren, Philip Victor},
  year = {2011},
  pages = {124},
  keywords = {\#nosource,hunter lab,Natural language processing}
}

@article{OntologyBiomedicalInvestigations,
  title = {The Ontology for Biomedical Investigations},
  file = {/home/harrisonpl/Documents/PDFs/The ontology for biomedical investigations.pdf}
}

@article{osumi-sutherlandDeadSimpleOWL2017,
  ids = {Osumi-Sutherland2017},
  title = {Dead Simple {{OWL}} Design Patterns},
  author = {{Osumi-Sutherland}, David and Courtot, Melanie and Balhoff, James P. and Mungall, Christopher},
  year = {2017},
  volume = {8},
  pages = {18},
  issn = {20411480},
  doi = {10.1186/s13326-017-0126-0},
  abstract = {Background: Bio-ontologies typically require multiple axes of classification to support the needs of their users. Development of such ontologies can only be made scalable and sustainable by the use of inference to automate classification via consistent patterns of axiomatization. Many bio-ontologies originating in OBO or OWL follow this approach. These patterns need to be documented in a form that requires minimal expertise to understand and edit and that can be validated and applied using any of the various programmatic approaches to working with OWL ontologies. Results: Here we describe a system, Dead Simple OWL Design Patterns (DOS-DPs), which fulfills these requirements, illustrating the system with examples from the Gene Ontology. Conclusions: The rapid adoption of DOS-DPs by multiple ontology development projects illustrates both the ease-of use and the pressing need for the simple design pattern system we have developed.},
  file = {/home/harrisonpl/Documents/PDFs/Osumi-Sutherland et al. - 2017 - Dead simple OWL design patterns.pdf},
  journal = {Journal of Biomedical Semantics},
  keywords = {Design pattern,Lawrence Hunter,OBO,OWL,Recommendation},
  mendeley-tags = {Lawrence Hunter,Recommendation},
  number = {1}
}

@article{OvidSevereSide,
  title = {Ovid: {{Severe}} Side Effects and Drug Plasma Concentrations in Preterm Infants Treated with Doxapram.},
  keywords = {\#nosource}
}

@article{owenKarlPearsonMetaanalysis2009,
  ids = {Owen2009},
  title = {Karl {{Pearson}}'s Meta-Analysis Revisited},
  author = {Owen, Art B.},
  year = {2009},
  volume = {37},
  pages = {3867--3892},
  issn = {00905364},
  doi = {10.1214/09-AOS697},
  abstract = {This paper revisits a meta-analysis method proposed by Pearson [Biometrika 26 (1934) 425-442] and first used by David [Biometrika 26 (1934) 1-11]. It was thought to be inadmissible for over fifty years, dating back to a paper of Birnbaum [J. Amer. Statist. Assoc. 49 (1954) 559-574]. It turns out that the method Birnbaum analyzed is not the one that Pearson proposed. We show that Pearson's proposal is admissible. Because it is admissible, it has better power than the standard test of Fisher [Statistical Methods for Research Workers (1932) Oliver and Boyd] at some alternatives, and worse power at others. Pearson's method has the advantage when all or most of the nonzero parameters share the same sign. Pearson's test has proved useful in a genomic setting, screening for age-related genes. This paper also presents an FFT-based method for getting hard upper and lower bounds on the CDF of a sum of nonnegative random variables. \textcopyright{} Institute of Mathematical Statistics, 2009.},
  file = {/home/harrisonpl/Documents/PDFs/Owen - 2009 - Karl Pearson's meta-analysis revisited.pdf},
  journal = {Annals of Statistics},
  keywords = {Admissibility,Fast fourier transform,Hypothesis testing,Microarrays},
  number = {6 B}
}

@article{ozturkComparativeStudySMILESbased2016,
  ids = {Ozturk,Ozturk2016a},
  title = {A Comparative Study of {{SMILES}}-Based Compound Similarity Functions for Drug-Target Interaction Prediction},
  author = {{\"O}zt{\"u}rk, Hakime and Ozkirimli, Elif and {\"O}zg{\"u}r, Arzucan},
  year = {2016},
  volume = {17},
  pages = {1--11},
  publisher = {{BMC Bioinformatics}},
  issn = {14712105},
  doi = {10.1186/s12859-016-0977-x},
  abstract = {Background: Molecular structures can be represented as strings of special characters using SMILES. Since each molecule is represented as a string, the similarity between compounds can be computed using SMILES-based string similarity functions. Most previous studies on drug-target interaction prediction use 2D-based compound similarity kernels such as SIMCOMP. To the best of our knowledge, using SMILES-based similarity functions, which are computationally more efficient than the 2D-based kernels, has not been investigated for this task before. Results: In this study, we adapt and evaluate various SMILES-based similarity methods for drug-target interaction prediction. In addition, inspired by the vector space model of Information Retrieval we propose cosine similarity based SMILES kernels that make use of the Term Frequency (TF) and Term Frequency-Inverse Document Frequency (TF-IDF) weighting approaches. We also investigate generating composite kernels by combining our best SMILES-based similarity functions with the SIMCOMP kernel. With this study, we provided a comparison of 13 different ligand similarity functions, each of which utilizes the SMILES string of molecule representation. Additionally, TF and TF-IDF based cosine similarity kernels are proposed. Conclusion: The more efficient SMILES-based similarity functions performed similarly to the more complex 2D-based SIMCOMP kernel in terms of AUC-ROC scores. The TF-IDF based cosine similarity obtained a better AUC-PR score than the SIMCOMP kernel on the GPCR benchmark data set. The composite kernel of TF-IDF based cosine similarity and SIMCOMP achieved the best AUC-PR scores for all data sets.},
  file = {/home/harrisonpl/Documents/PDFs/Öztürk et al. - 2016 - A comparative study of SMILES-based compound similarity functions for.pdf},
  journal = {BMC Bioinformatics},
  keywords = {Chemoinformatics,Drug-target interaction prediction,SMILES,SMILES based drug similarity},
  number = {1}
}

@article{parkCancerTypeDependent2015,
  ids = {Park2015},
  title = {Cancer Type-dependent Genetic Interactions between Cancer Driver Alterations Indicate Plasticity of Epistasis across Cell Types},
  author = {Park, Solip and Lehner, Ben},
  year = {2015},
  volume = {11},
  pages = {824},
  issn = {1744-4292},
  doi = {10.15252/msb.20156102},
  abstract = {Cancers, like many diseases, are normally caused by combinations of genetic alterations rather than by changes affecting single genes. It is well established that the genetic alterations that drive cancer often interact epistatically, having greater or weaker consequences in combination than expected from their individual effects. In a stringent statistical analysis of data from \textbackslash textgreater 3,000 tumors, we find that the co-occurrence and mutual exclusivity relationships between cancer driver alterations change quite extensively in different types of cancer. This cannot be accounted for by variation in tumor heterogeneity or unrecognized cancer subtypes. Rather, it suggests that how genomic alterations interact cooperatively or partially redundantly to driver cancer changes in different types of cancers. This re-wiring of epistasis across cell types is likely to be a basic feature of genetic architecture, with important implications for understanding the evolution of multicellularity and human genetic diseases. In addition, if this plasticity of epistasis across cell types is also true for synthetic lethal interactions, a synthetic lethal strategy to kill cancer cells may frequently work in one type of cancer but prove ineffective in another.},
  file = {/home/harrisonpl/Documents/PDFs/Park, Lehner - 2015 - Cancer type‐dependent genetic interactions between cancer driver alterations.pdf},
  journal = {Molecular Systems Biology},
  number = {7}
}

@article{parsiaOWLReasonerEvaluation2017,
  ids = {Parsia2017},
  title = {The {{OWL Reasoner Evaluation}} ({{ORE}}) 2015 {{Competition Report}}},
  author = {Parsia, Bijan and Matentzoglu, Nicolas and Gon{\c c}alves, Rafael S. and Glimm, Birte and Steigmiller, Andreas},
  year = {2017},
  volume = {59},
  pages = {455--482},
  issn = {15730670},
  doi = {10.1007/s10817-017-9406-8},
  abstract = {The OWL Reasoner Evaluation competition is an annual competition (with an associated workshop) that pits OWL 2 compliant reasoners against each other on various standard reasoning tasks over naturally occurring problems. The 2015 competition was the third of its sort and had 14 reasoners competing in six tracks comprising three tasks (consistency, classification, and realisation) over two profiles (OWL 2 DL and EL). In this paper, we discuss the design, execution and results of the 2015 competition with particular attention to lessons learned for benchmarking, comparative experiments, and future competitions.},
  file = {/home/harrisonpl/Documents/PDFs/Parsia et al. - 2017 - The OWL Reasoner Evaluation (ORE) 2015 Competition Report.pdf},
  journal = {Journal of Automated Reasoning},
  keywords = {Ontologies,ontology,OWL,Reasoning},
  mendeley-tags = {ontology},
  number = {4},
  pmid = {30069067}
}

@article{pathanFunRichOpenAccess2015,
  title = {{{FunRich}}: {{An}} Open Access Standalone Functional Enrichment and Interaction Network Analysis Tool},
  shorttitle = {{{FunRich}}},
  author = {Pathan, Mohashin and Keerthikumar, Shivakumar and Ang, Ching-Seng and Gangoda, Lahiru and Quek, Camelia Y. J. and Williamson, Nicholas A. and Mouradov, Dmitri and Sieber, Oliver M. and Simpson, Richard J. and Salim, Agus and Bacic, Antony and Hill, Andrew F. and Stroud, David A. and Ryan, Michael T. and Agbinya, Johnson I. and Mariadason, John M. and Burgess, Antony W. and Mathivanan, Suresh},
  year = {2015},
  volume = {15},
  pages = {2597--2601},
  issn = {1615-9861},
  doi = {10.1002/pmic.201400515},
  abstract = {As high-throughput techniques including proteomics become more accessible to individual laboratories, there is an urgent need for a user-friendly bioinformatics analysis system. Here, we describe FunRich, an open access, standalone functional enrichment and network analysis tool. FunRich is designed to be used by biologists with minimal or no support from computational and database experts. Using FunRich, users can perform functional enrichment analysis on background databases that are integrated from heterogeneous genomic and proteomic resources ({$>$}1.5 million annotations). Besides default human specific FunRich database, users can download data from the UniProt database, which currently supports 20 different taxonomies against which enrichment analysis can be performed. Moreover, the users can build their own custom databases and perform the enrichment analysis irrespective of organism. In addition to proteomics datasets, the custom database allows for the tool to be used for genomics, lipidomics and metabolomics datasets. Thus, FunRich allows for complete database customization and thereby permits for the tool to be exploited as a skeleton for enrichment analysis irrespective of the data type or organism used. FunRich (http://www.funrich.org) is user-friendly and provides graphical representation (Venn, pie charts, bar graphs, column, heatmap and doughnuts) of the data with customizable font, scale and color (publication quality).},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/pmic.201400515},
  copyright = {\textcopyright{} 2015 WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
  file = {/home/harrisonpl/Documents/PDFs/Pathan et al. - 2015 - FunRich.pdf;/home/harrisonpl/Zotero/storage/BVK7KYUU/pmic.html},
  journal = {PROTEOMICS},
  keywords = {Bioinformatics,Enrichment analysis,Interaction networks,Omics},
  language = {en},
  number = {15}
}

@article{paulheimKnowledgeGraphRefinement2016,
  title = {Knowledge Graph Refinement: {{A}} Survey of Approaches and Evaluation Methods},
  shorttitle = {Knowledge Graph Refinement},
  author = {Paulheim, Heiko},
  editor = {Cimiano, Philipp},
  year = {2016},
  month = dec,
  volume = {8},
  pages = {489--508},
  issn = {22104968, 15700844},
  doi = {10.3233/SW-160218},
  abstract = {In the recent years, different Web knowledge graphs, both free and commercial, have been created. While Google coined the term ``Knowledge Graph'' in 2012, there are also a few openly available knowledge graphs, with DBpedia, YAGO, and Freebase being among the most prominent ones. Those graphs are often constructed from semi-structured knowledge, such as Wikipedia, or harvested from the web with a combination of statistical and linguistic methods. The result are large-scale knowledge graphs that try to make a good trade-off between completeness and correctness. In order to further increase the utility of such knowledge graphs, various refinement methods have been proposed, which try to infer and add missing knowledge to the graph, or identify erroneous pieces of information. In this article, we provide a survey of such knowledge graph refinement approaches, with a dual look at both the methods being proposed as well as the evaluation methodologies used.},
  file = {/home/harrisonpl/Documents/PDFs/Paulheim - 2016 - Knowledge graph refinement.pdf},
  journal = {Semantic Web},
  language = {en},
  number = {3}
}

@article{pawarRelationExtractionSurvey2017,
  ids = {Pawar2017},
  title = {Relation {{Extraction}} : {{A Survey}}},
  shorttitle = {Relation {{Extraction}}},
  author = {Pawar, Sachin and Palshikar, Girish K. and Bhattacharyya, Pushpak},
  year = {2017},
  abstract = {With the advent of the Internet, large amount of digital text is generated everyday in the form of news articles, research publications, blogs, question answering forums and social media. It is important to develop techniques for extracting information automatically from these documents, as lot of important information is hidden within them. This extracted information can be used to improve access and management of knowledge hidden in large text corpora. Several applications such as Question Answering, Information Retrieval would benefit from this information. Entities like persons and organizations, form the most basic unit of the information. Occurrences of entities in a sentence are often linked through well-defined relations; e.g., occurrences of person and organization in a sentence may be linked through relations such as employed at. The task of Relation Extraction (RE) is to identify such relations automatically. In this paper, we survey several important supervised, semi-supervised and unsupervised RE techniques. We also cover the paradigms of Open Information Extraction (OIE) and Distant Supervision. Finally, we describe some of the recent trends in the RE techniques and possible future research directions. This survey would be useful for three kinds of readers - i) Newcomers in the field who want to quickly learn about RE; ii) Researchers who want to know how the various RE techniques evolved over time and what are possible future research directions and iii) Practitioners who just need to know which RE technique works best in various settings.},
  annotation = {\_eprint: 1712.05191},
  archivePrefix = {arXiv},
  arxivid = {1712.05191},
  eprint = {1712.05191},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Pawar et al. - 2017 - Relation Extraction.pdf},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Natural language processing},
  mendeley-tags = {Natural language processing}
}

@article{pawlikRTEDRobustAlgorithm2011,
  title = {{{RTED}}: {{A Robust Algorithm}} for the {{Tree Edit Distance}}},
  shorttitle = {{{RTED}}},
  author = {Pawlik, Mateusz and Augsten, Nikolaus},
  year = {2011},
  month = dec,
  abstract = {We consider the classical tree edit distance between ordered labeled trees, which is defined as the minimum-cost sequence of node edit operations that transform one tree into another. The state-of-the-art solutions for the tree edit distance are not satisfactory. The main competitors in the field either have optimal worst-case complexity, but the worst case happens frequently, or they are very efficient for some tree shapes, but degenerate for others. This leads to unpredictable and often infeasible runtimes. There is no obvious way to choose between the algorithms. In this paper we present RTED, a robust tree edit distance algorithm. The asymptotic complexity of RTED is smaller or equal to the complexity of the best competitors for any input instance, i.e., RTED is both efficient and worst-case optimal. We introduce the class of LRH (Left-Right-Heavy) algorithms, which includes RTED and the fastest tree edit distance algorithms presented in literature. We prove that RTED outperforms all previously proposed LRH algorithms in terms of runtime complexity. In our experiments on synthetic and real world data we empirically evaluate our solution and compare it to the state-of-the-art.},
  archivePrefix = {arXiv},
  eprint = {1201.0230},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Pawlik, Augsten - 2011 - RTED.pdf;/home/harrisonpl/Zotero/storage/WA3FT7IE/1201.html},
  journal = {arXiv:1201.0230 [cs]},
  keywords = {Computer Science - Databases},
  primaryClass = {cs}
}

@article{payneChapterBiomedicalKnowledge2012,
  ids = {Payne2012},
  title = {Chapter 1: {{Biomedical Knowledge Integration}}},
  author = {Payne, Philip R.O.},
  editor = {Lewitter, Fran and Kann, Maricel},
  year = {2012},
  month = dec,
  volume = {8},
  pages = {e1002826},
  publisher = {{Public Library of Science}},
  issn = {1553734X},
  doi = {10.1371/journal.pcbi.1002826},
  abstract = {The modern biomedical research and healthcare delivery domains have seen an unparalleled increase in the rate of innovation and novel technologies over the past several decades. Catalyzed by paradigm-shifting public and private programs focusing upon the formation and delivery of genomic and personalized medicine, the need for high-throughput and integrative approaches to the collection, management, and analysis of heterogeneous data sets has become imperative. This need is particularly pressing in the translational bioinformatics domain, where many fundamental research questions require the integration of large scale, multi-dimensional clinical phenotype and bio-molecular data sets. Modern biomedical informatics theory and practice has demonstrated the distinct benefits associated with the use of knowledge-based systems in such contexts. A knowledge-based system can be defined as an intelligent agent that employs a computationally tractable knowledge base or repository in order to reason upon data in a targeted domain and reproduce expert performance relative to such reasoning operations. The ultimate goal of the design and use of such agents is to increase the reproducibility, scalability, and accessibility of complex reasoning tasks. Examples of the application of knowledge-based systems in biomedicine span a broad spectrum, from the execution of clinical decision support, to epidemiologic surveillance of public data sets for the purposes of detecting emerging infectious diseases, to the discovery of novel hypotheses in large-scale research data sets. In this chapter, we will review the basic theoretical frameworks that define core knowledge types and reasoning operations with particular emphasis on the applicability of such conceptual models within the biomedical domain, and then go on to introduce a number of prototypical data integration requirements and patterns relevant to the conduct of translational bioinformatics that can be addressed via the design and use of knowledge-based systems. \textcopyright{} 2012 Philip R.},
  file = {/home/harrisonpl/Documents/PDFs/Payne - 2012 - Chapter 1.pdf},
  journal = {PLoS Computational Biology},
  keywords = {HPL comprehensive exam},
  mendeley-tags = {HPL comprehensive exam},
  number = {12}
}

@article{pearlSevenToolsCausal2019,
  ids = {Pearl2019},
  title = {The Seven Tools of Causal Inference, with Reflections on Machine Learning},
  author = {Pearl, Judea},
  year = {2019},
  volume = {62},
  pages = {54--60},
  issn = {15577317},
  doi = {10.1145/3241036},
  abstract = {THE DRAMATIC SUCCESS In machine learning has led to an explosion of artificial intelligence (AI) applications and increasing expectations for autonomous systems that exhibit human-level intelligence. These expectations have, however, met with fundamental obstacles that cut across many application areas. One such obstacle is adaptability, or robustness. Machine learning researchers have noted current systems lack the ability to recognize or react to new circumstances they have not been specifically programmed or trained for.},
  file = {/home/harrisonpl/Documents/PDFs/Pearl - 2019 - The seven tools of causal inference, with reflections on machine learning.pdf},
  journal = {Communications of the ACM},
  number = {3}
}

@article{pena-torresHowAdaptDeep2019,
  ids = {Pena-Torres2019},
  title = {How to {{Adapt Deep Learning Models}} to a {{New Domain}}: {{The Case}} of {{Biomedical Relation Extraction}}},
  author = {{Pe{\~n}a-Torres}, Jefferson A. and Guti{\'e}rrez, Ra{\'u}l E. and Bucheli, V{\'i}ctor A. and Gonz{\'a}lez, Fabio A.},
  year = {2019},
  volume = {22},
  pages = {49--62},
  issn = {0123-7799},
  doi = {10.22430/22565337.1483},
  abstract = {In this article, we study the relation extraction problem from Natural Language Processing (NLP) implementing a domain adaptation setting without external resources. We trained a Deep Learning (DL) model for Relation Extraction (RE), which extracts semantic relations in the biomedical domain. However, can the model be applied to different domains? The model should be adaptable to automatically extract relationships across different domains using the DL network. Completely training DL models in a short time is impractical because the models should quickly adapt to different datasets in several domains without delay. Therefore, adaptation is crucial for intelligent systems, where changing factors and unanticipated perturbations are common. In this study, we present a detailed analysis of the problem, as well as preliminary experimentation, results, and their evaluation.},
  file = {/home/harrisonpl/Documents/PDFs/Peña-Torres et al. - 2019 - How to Adapt Deep Learning Models to a New Domain.pdf},
  journal = {TecnoL\'ogicas}
}

@article{pendletonAssemblyDiploidArchitecture2015,
  ids = {Pendleton2015},
  title = {Assembly and Diploid Architecture of an Individual Human Genome via Single-Molecule Technologies},
  author = {Pendleton, Matthew and Sebra, Robert and Pang, Andy Wing Chun and Ummat, Ajay and Franzen, Oscar and Rausch, Tobias and St{\"u}tz, Adrian M. and Stedman, William and Anantharaman, Thomas and Hastie, Alex and Dai, Heng and Fritz, Markus Hsi Yang and Cao, Han and Cohain, Ariella and Deikus, Gintaras and Durrett, Russell E. and Blanchard, Scott C. and Altman, Roger and Chin, Chen Shan and Guo, Yan and Paxinos, Ellen E. and Korbel, Jan O. and Darnell, Robert B. and McCombie, W. Richard and Kwok, Pui Yan and Mason, Christopher E. and Schadt, Eric E. and Bashir, Ali},
  year = {2015},
  volume = {12},
  pages = {780--786},
  issn = {15487105},
  doi = {10.1038/nmeth.3454},
  abstract = {We present the first comprehensive analysis of a diploid human genome that combines single-molecule sequencing with single-molecule genome maps. Our hybrid assembly markedly improves upon the contiguity observed from traditional shotgun sequencing approaches, with scaffold N50 values approaching 30 Mb, and we identified complex structural variants (SVs) missed by other high-throughput approaches. Furthermore, by combining Illumina short-read data with long reads, we phased both single-nucleotide variants and SVs, generating haplotypes with over 99\% consistency with previous trio-based studies. Our work shows that it is now possible to integrate single-molecule and high-throughput sequence data to generate de novo assembled genomes that approach reference quality.},
  file = {/home/harrisonpl/Documents/PDFs/Pendleton et al. - 2015 - Assembly and diploid architecture of an individual human genome via.pdf},
  journal = {Nature Methods},
  keywords = {Genome assembly algorithms,Genome informatics,Genomics,Structural variation},
  number = {8}
}

@inproceedings{penningtonGloveGlobalVectors2014,
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {Glove},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  pages = {1532--1543},
  publisher = {{Association for Computational Linguistics}},
  address = {{Doha, Qatar}},
  doi = {10.3115/v1/D14-1162},
  abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
  file = {/home/harrisonpl/Zotero/storage/KSMMPFHY/D14-1162.pdf},
  language = {en}
}

@article{PersistentHomologyTutorial,
  title = {Persistent Homology Tutorial},
  pages = {139},
  keywords = {\#nosource},
  language = {English}
}

@article{planaUseStatinsAcute2001,
  ids = {Plana2001},
  title = {The Use of Statins in Acute Coronary Syndromes: The Mechanisms behind the Outcomes.},
  shorttitle = {The Use of Statins in Acute Coronary Syndromes},
  author = {Plana, J. C. and Jones, P. H.},
  year = {2001},
  volume = {3},
  pages = {355--364},
  issn = {15233804},
  doi = {10.1007/s11883-001-0073-0},
  abstract = {Lipid-lowering drugs, in particular statin treatments, have been shown to reduce the incidence of initial and recurrent coronary heart disease (CHD) events within several years of initiating therapy. This effect can be clinically detected within the first 1 to 2 years in randomized trials. Recent observational and clinical trial data suggest that lipid-lowering therapy initiated at the time of an acute coronary event can reduce recurrent events, and possibly all-cause mortality, in a much shorter period of time. The possible mechanisms by which this benefit occurs include the effect of reduced lipoprotein levels, as well as an independent effect of statins on endothelial function. Statins improve endothelial-dependent flow-mediated vasodilation by increasing the bioavailability of nitric oxide. They stabilize the plaque by modulating the inflammatory response within the vessel wall. They also decrease clot formation by decreasing the adherence of platelets to the ruptured plaque and by acting on the extrinsic coagulation cascade pathway. This review examines these effects of statins and lipoproteins on vascular function, as well as the clinical evidence supporting early treatment in acute coronary syndromes.},
  file = {/home/harrisonpl/Documents/PDFs/Plana, Jones - 2001 - The use of statins in acute coronary syndromes.pdf},
  journal = {Current atherosclerosis reports},
  keywords = {mechanism},
  mendeley-tags = {mechanism},
  number = {5}
}

@article{pliakosMiningBiomedicalNetworks2019,
  ids = {Pliakos2019},
  title = {Mining Biomedical Networks Exploiting Structure and Background Information},
  author = {Pliakos, Konstantinos},
  year = {2019},
  file = {/home/harrisonpl/Documents/PDFs/Pliakos - 2019 - Mining biomedical networks exploiting structure and background information.pdf},
  number = {September}
}

@article{plowrightTargetDiscoveryValidation2019,
  title = {Target {{Discovery}} and {{Validation}}: {{Methods}} and {{Strategies}} for {{Drug Discovery}}},
  author = {Plowright, Alleyn T. and Mannhold, Raimund and Buschmann, Helmut and Holenz, Jorg},
  year = {2019},
  abstract = {The modern drug developers? guide for making informed choices among the diverse target identification methods Target Discovery and Validation: Methods and Strategies for Drug Discovery offers a hands-on review of the modern technologies for drug target identification and validation. With contributions from noted industry and academic experts, the book addresses the most recent chemical, biological, and computational methods. Additionally, the book highlights techologies that are applicable to ?difficult? targets and drugs directed at multiple targets, including chemoproteomics, activity-based protein profiling, pathway mapping, genome-wide association studies, and array-based profiling. Throughout, the authors highlight a range of diverse approaches, and target validation studies reveal how these methods can support academic and drug discovery scientists in their target discovery and validation research. This resource: -Offers a guide to identifying and validating targets, a key enabling technology without which no new drug development is possible -Presents the information needed for choosing the appropriate assay method from the ever-growing range of available options -Provides practical examples from recent drug development projects, e. g. in kinase inhibitor profiling Written for medicinal chemists, pharmaceutical professionals, biochemists, biotechnology professionals, and pharmaceutical chemists, Target Discovery and Validation explores the current methods for the identification and validation of drug targets in one comrpehensive volume. It also includes numerous practical examples.},
  keywords = {\#nosource}
}

@article{popovaDeepReinforcementLearning2018,
  ids = {Popova2018},
  title = {Deep Reinforcement Learning for de Novo Drug Design},
  author = {Popova, Mariya and Isayev, Olexandr and Tropsha, Alexander},
  year = {2018},
  month = jul,
  volume = {4},
  pages = {eaap7885},
  publisher = {{American Association for the Advancement of Science}},
  issn = {23752548},
  doi = {10.1126/sciadv.aap7885},
  abstract = {We have devised and implemented a novel computational strategy for de novo design of molecules with desired properties termed ReLeaSE (Reinforcement Learning for Structural Evolution). On the basis of deep and reinforcement learning (RL) approaches, ReLeaSE integrates two deep neural networks\textemdash generative and predictive\textemdash that are trained separately but are used jointly to generate novel targeted chemical libraries. ReLeaSE uses simple representation of molecules by their simplified molecular-input line-entry system (SMILES) strings only. Generative models are trained with a stack-augmented memory network to produce chemically feasible SMILES strings, and predictive models are derived to forecast the desired properties of the de novo\textendash generated compounds. In the first phase of the method, generative and predictive models are trained separately with a supervised learning algorithm. In the second phase, both models are trained jointly with the RL approach to bias the generation of new chemical structures toward those with the desired physical and/or biological properties. In the proof-of-concept study, we have used the ReLeaSE method to design chemical libraries with a bias toward structural complexity or toward compounds with maximal, minimal, or specific range of physical properties, such as melting point or hydrophobicity, or toward compounds with inhibitory activity against Janus protein kinase 2. The approach proposed herein can find a general use for generating targeted chemical libraries of novel compounds optimized for either a single desired property or multiple properties.},
  annotation = {\_eprint: 1711.10907},
  archivePrefix = {arXiv},
  arxivid = {1711.10907},
  eprint = {1711.10907},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Popova et al. - 2018 - Deep reinforcement learning for de novo drug design.pdf},
  journal = {Science Advances},
  language = {en},
  number = {7},
  pmid = {30050984}
}

@article{pouriyehComprehensiveSurveyOntology2018,
  ids = {Pouriyeh2018},
  title = {A {{Comprehensive Survey}} of {{Ontology Summarization}}: {{Measures}} and {{Methods}}},
  shorttitle = {A {{Comprehensive Survey}} of {{Ontology Summarization}}},
  author = {Pouriyeh, Seyedamin and Allahyari, Mehdi and Kochut, Krys and Arabnia, Hamid Reza},
  year = {2018},
  abstract = {The Semantic Web is becoming a large scale framework that enables data to be published, shared, and reused in the form of ontologies. The ontology which is considered as basic building block of semantic web consists of two layers including data and schema layer. With the current exponential development of ontologies in both data size and complexity of schemas, ontology understanding which is playing an important role in different tasks such as ontology engineering, ontology learning, etc., is becoming more difficult. Ontology summarization as a way to distill knowledge from an ontology and generate an abridge version to facilitate a better understanding is getting more attention recently. There are various approaches available for ontology summarization which are focusing on different measures in order to produce a proper summary for a given ontology. In this paper, we mainly focus on the common metrics which are using for ontology summarization and meet the state-of-the-art in ontology summarization.},
  annotation = {\_eprint: 1801.01937},
  archivePrefix = {arXiv},
  arxivid = {1801.01937},
  eprint = {1801.01937},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Pouriyeh et al. - 2018 - A Comprehensive Survey of Ontology Summarization.pdf},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval,Natural language processing},
  mendeley-tags = {Natural language processing}
}

@article{powersGSEAInContextIdentifyingNovel2018,
  ids = {powersGSEAInContextIdentifyingNovel},
  title = {{{GSEA}}-{{InContext}}: {{Identifying}} Novel and Common Patterns in Expression Experiments},
  shorttitle = {{{GSEA}}-{{InContext}}},
  author = {Powers, Rani K. and Goodspeed, Andrew and {Pielke-Lombardo}, Harrison and Tan, Aik Choon and Costello, James C.},
  year = {2018},
  volume = {34},
  pages = {i555--i564},
  issn = {14602059},
  doi = {10.1093/bioinformatics/bty271},
  abstract = {Motivation: Gene Set Enrichment Analysis (GSEA) is routinely used to analyze and interpret coordinate pathway-level changes in transcriptomics experiments. For an experiment where less than seven samples per condition are compared, GSEA employs a competitive null hypothesis to test significance. A gene set enrichment score is tested against a null distribution of enrichment scores generated from permuted gene sets, where genes are randomly selected from the input experiment. Looking across a variety of biological conditions, however, genes are not randomly distributed with many showing consistent patterns of up- or down-regulation. As a result, common patterns of positively and negatively enriched gene sets are observed across experiments. Placing a single experiment into the context of a relevant set of background experiments allows us to identify both the common and experiment-specific patterns of gene set enrichment. Results: We compiled a compendium of 442 small molecule transcriptomic experiments and used GSEA to characterize common patterns of positively and negatively enriched gene sets. To identify experiment-specific gene set enrichment, we developed the GSEA-InContext method that accounts for gene expression patterns within a background set of experiments to identify statistically significantly enriched gene sets. We evaluated GSEA-InContext on experiments using small molecules with known targets to show that it successfully prioritizes gene sets that are specific to each experiment, thus providing valuable insights that complement standard GSEA analysis.},
  annotation = {ZSCC: 0000006},
  file = {/home/harrisonpl/Documents/PDFs/Powers et al. - 2018 - GSEA-InContext.pdf},
  journal = {Bioinformatics},
  number = {13}
}

@article{pradeComputationalApproachesAnalogical2014,
  title = {Computational Approaches to Analogical Reasoning: {{Current}} Trends},
  author = {Prade, Henri and Richard, Gilles},
  year = {2014},
  volume = {548},
  issn = {1860949X},
  doi = {10.1007/978-3-642-54516-0},
  file = {/home/harrisonpl/Documents/PDFs/Prade, Richard - 2014 - Computational approaches to analogical reasoning.pdf},
  journal = {Studies in Computational Intelligence},
  keywords = {\#nosource}
}

@article{prewettRemoteDevelopmentEMACS2009,
  ids = {Prewett2009},
  title = {Remote {{Development}} with {{EMACS}}},
  author = {Prewett, James E},
  year = {2009},
  file = {/home/harrisonpl/Documents/PDFs/Prewett - 2009 - Remote Development with EMACS.pdf}
}

@article{prijicMechanismsBetaBlockersAction2014,
  title = {Mechanisms of {{Beta}}-{{Blockers Action}} in {{Patients}} with {{Heart Failure}}},
  author = {Prijic, Sergej and Buchhorn, Reiner},
  year = {2014},
  volume = {9},
  abstract = {Patients with chronic heart failure have prolonged sympathetic stimulation and subsequent worsening of the failing heart function. Beta-blockers (non-selective, cardio-selective, and non-selective with ancillary properties) counteract the effects of prolonged sympathetic stimulation. Beta-blocker therapy results in the improvement of the left ventricular systolic and diastolic function, reversal remodeling, heart rate control, effective prevention of the malignant arrhythmias, and lowering of the both cardiac afterload and preload in patients with chronic heart failure.},
  keywords = {\#nosource,borg,HPL comprehensive exam},
  language = {en}
}

@inproceedings{Proceedings18thBioNLP2019,
  ids = {Proceedings18thBioNLP2019a},
  title = {Proceedings of the 18th {{BioNLP Workshop}} and {{Shared Task}}},
  year = {2019},
  file = {/home/harrisonpl/Documents/PDFs/2019 - Proceedings of the 18th BioNLP Workshop and Shared Task.pdf},
  isbn = {978-1-950737-28-4}
}

@article{ProjectMUSEEthics,
  title = {Project {{MUSE}} - {{Ethics}} beyond {{Computation}}: {{Why We Can}}'t (and {{Shouldn}}'t) {{Replace Human Moral Judgment}} with {{Algorithms}}},
  keywords = {\#nosource}
}

@article{provostMachineLearningImbalanced2000,
  title = {Machine Learning from Imbalanced Data Sets 101},
  author = {Provost, Foster},
  year = {2000},
  pages = {3},
  doi = {10.1.1.33.507},
  abstract = {For research to progress most effectively, we first should establish common ground regarding just what is the problem that imbalanced data sets present to machine learning systems. Why and when should imbalanced data sets be problematic? When is the problem simply an ...},
  journal = {Proceedings of the AAAI'2000 Workshop on \textbackslash ldots},
  keywords = {\#nosource}
}

@article{pruthiCombatingAdversarialMisspellings2019,
  ids = {Pruthi2019},
  title = {Combating {{Adversarial Misspellings}} with {{Robust Word Recognition}}},
  author = {Pruthi, Danish and Dhingra, Bhuwan and Lipton, Zachary C.},
  year = {2019},
  pages = {5582--5591},
  doi = {10.18653/v1/p19-1561},
  abstract = {To combat adversarial spelling mistakes, we propose placing a word recognition model in front of the downstream classifier. Our word recognition models build upon the RNN semi-character architecture, introducing several new backoff strategies for handling rare and unseen words. Trained to recognize words corrupted by random adds, drops, swaps, and keyboard mistakes, our method achieves 32\% relative (and 3.3\% absolute) error reduction over the vanilla semi-character model. Notably, our pipeline confers robustness on the downstream classifier, outperforming both adversarial training and off-the-shelf spell checkers. Against a BERT model fine-tuned for sentiment analysis, a single adversarially-chosen character attack lowers accuracy from 90.3\% to 45.8\%. Our defense restores accuracy to 75\%. Surprisingly, better word recognition does not always entail greater robustness. Our analysis reveals that robustness also depends upon a quantity that we denote the sensitivity.},
  annotation = {\_eprint: 1905.11268},
  archivePrefix = {arXiv},
  arxivid = {1905.11268},
  eprint = {1905.11268},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Pruthi et al. - 2019 - Combating Adversarial Misspellings with Robust Word Recognition.pdf}
}

@article{qinPredictionMechanismsAction2020,
  ids = {Qin2020,Qin2020b},
  title = {Prediction of the Mechanisms of Action of {{Shenkang}} in Chronic Kidney Disease: {{A}} Network Pharmacology Study and Experimental Validation},
  author = {Qin, Tianyu and Wu, Lili and Hua, Qian and Song, Zilin and Pan, Yajing and Liu, Tonghua},
  year = {2020},
  month = jan,
  volume = {246},
  pages = {112128},
  publisher = {{Elsevier Ireland Ltd}},
  issn = {18727573},
  doi = {10.1016/j.jep.2019.112128},
  abstract = {Ethnopharmacological relevance: Traditional Chinese medicine provides a unique curative treatment of complex chronic diseases, including chronic kidney disease (CKD), which is not effectively treated with the current therapies. The pharmacological mechanisms of Shenkang (SK), a herbal medicine containing rhubarb (Rheum palmatum L. or R. tanguticum Maxim. ex Balf.), red sage (Salvia miltiorrhiza Bunge), safflower (Carthamus tinctorius L.), and astragalus (Astragalus mongholicus Bunge), widely used to treat CKD in China, are still unclear. Aim of the study: In this study, the comprehensive approach used for elucidating the pharmacological mechanisms of SK included the identification of the effective constituents, target prediction and network analysis, by investigating the interacting pathways between these molecules in the context of CKD. These results were validated by performing an in vivo study and by comparison with literature reviews. Materials and methods: This approach involved the following main steps: first, we constructed a molecular database for SK and screened for active molecules by conducting drug-likeness and drug half-life evaluations; second, we used a weighted ensemble similarity drug-targeting model to accurately identify the direct drug targets of the bioactive constituents; third, we constructed compound\textendash target, target\textendash pathway, and target\textendash disease networks using the Cytoscape 3.2 software and determined the distribution of the targets in tissues and organs according to the BioGPS database. Finally, the resulting drug\textendash target mechanisms were compared with those proposed by previous research on SK and validated in a mouse model of CKD. Results: By using Network analysis, 88 potential bioactive compounds in the four component herbs of SK and 85 CKD-related targets were identified, including pathways that involve the nuclear factor-{$\kappa$}B, mitogen-activated protein kinase, transient receptor potential, and vascular endothelial growth factor, which were categorized as inflammation, proliferation, migration, and permeability modules. The results also included different tissues (kidneys, liver, lungs, and heart) and different disease types (urogenital, metabolic, endocrine, cardiovascular, and immune diseases as well as pathological processes) closely related to CKD. These findings agreed with those reported in the literature. However, our findings with the network pharmacology prediction did not account for all the effects reported for SK found in the literature, such as regulation of the hemodynamics, inhibition of oxidative stress and apoptosis, and the involvement of the transforming growth factor-{$\beta$}/SMAD3, sirtuin/forkhead box protein O (SIRT/FOXO) and B-cell lymphoma-2-associated X protein pathways. The in vivo validation experiment revealed that SK ameliorated CKD through antifibrosis and anti-inflammatory effects, by downregulating the levels of vascular cell adhesion protein 1, vitamin D receptor, cyclooxygenase-2, and matrix metalloproteinase 9 proteins in the unilateral ureteral obstruction mouse model. This was consistent with the predicted target and pathway networks. Conclusions: SK exerted a curative effect on CKD and CKD-related diseases by targeting different organs, regulating inflammation and proliferation processes, and inhibiting abnormal extracellular matrix accumulation. Thus, pharmacological network analysis with in vivo validation explained the potential effects and mechanisms of SK in the treatment of CKD. However, these findings need to be further confirmed with clinical studies.},
  file = {/home/harrisonpl/Documents/PDFs/Qin et al. - 2020 - Prediction of the mechanisms of action of Shenkang in chronic kidney disease.pdf},
  journal = {Journal of Ethnopharmacology},
  keywords = {Chronic kidney disease,Network analysis,Shenkang,Target prediction,Traditional Chinese medicine},
  number = {November 2018}
}

@article{quickReferenceBacterialGenome2014,
  ids = {Quick,Quick2014},
  title = {A Reference Bacterial Genome Dataset Generated on the {{MinION}}\texttrademark{} Portable Single-Molecule Nanopore Sequencer},
  author = {Quick, Joshua and Quinlan, Aaron R. and Loman, Nicholas J.},
  year = {2014},
  volume = {3},
  pages = {1--6},
  issn = {2047217X},
  doi = {10.1186/2047-217X-3-22},
  abstract = {Background: The MinION\texttrademark{} is a new, portable single-molecule sequencer developed by Oxford Nanopore Technologies. It measures four inches in length and is powered from the USB 3.0 port of a laptop computer. The MinION\texttrademark{} measures the change in current resulting from DNA strands interacting with a charged protein nanopore. These measurements can then be used to deduce the underlying nucleotide sequence. Findings: We present a read dataset from whole-genome shotgun sequencing of the model organism Escherichia coli K-12 substr. MG1655 generated on a MinION\texttrademark{} device during the early-access MinION\texttrademark{} Access Program (MAP). Sequencing runs of the MinION\texttrademark{} are presented, one generated using R7 chemistry (released in July 2014) and one using R7.3 (released in September 2014).Conclusions: Base-called sequence data are provided to demonstrate the nature of data produced by the MinION\texttrademark{} platform and to encourage the development of customised methods for alignment, consensus and variant calling, de novo assembly and scaffolding. FAST5 files containing event data within the HDF5 container format are provided to assist with the development of improved base-calling methods.},
  file = {/home/harrisonpl/Documents/PDFs/Quick et al. - 2014 - A reference bacterial genome dataset generated on the MinION™ portable.pdf},
  journal = {GigaScience},
  keywords = {Genomics,Nanopore sequencing},
  number = {1}
}

@article{quilletImprovingBioinformaticsPrediction2020,
  ids = {Quillet2020},
  title = {Improving {{Bioinformatics Prediction}} of {{microRNA Targets}} by {{Ranks Aggregation}}},
  author = {Quillet, Aur{\'e}lien and Saad, Chadi and Ferry, Ga{\"e}tan and Anouar, Youssef and Vergne, Nicolas and Lecroq, Thierry and Dubessy, Christophe},
  year = {2020},
  volume = {10},
  pages = {1--14},
  issn = {16648021},
  doi = {10.3389/fgene.2019.01330},
  abstract = {microRNAs are noncoding RNAs which downregulate a large number of target mRNAs and modulate cell activity. Despite continued progress, bioinformatics prediction of microRNA targets remains a challenge since available software still suffer from a lack of accuracy and sensitivity. Moreover, these tools show fairly inconsistent results from one another. Thus, in an attempt to circumvent these difficulties, we aggregated all human results of four important prediction algorithms (miRanda, PITA, SVmicrO, and TargetScan) showing additional characteristics in order to rerank them into a single list. Instead of deciding which prediction tool to use, our method clearly helps biologists getting the best microRNA target predictions from all aggregated databases. The resulting database is freely available through a webtool called miRabel1 which can take either a list of miRNAs, genes, or signaling pathways as search inputs. Receiver operating characteristic curves and precision-recall curves analysis carried out using experimentally validated data and very large data sets show that miRabel significantly improves the prediction of miRNA targets compared to the four algorithms used separately. Moreover, using the same analytical methods, miRabel shows significantly better predictions than other popular algorithms such as MBSTAR, miRWalk, ExprTarget and miRMap. Interestingly, an F-score analysis revealed that miRabel also significantly improves the relevance of the top results. The aggregation of results from different databases is therefore a powerful and generalizable approach to many other species to improve miRNA target predictions. Thus, miRabel is an efficient tool to guide biologists in their search for miRNA targets and integrate them into a biological context.},
  file = {/home/harrisonpl/Documents/PDFs/Quillet et al. - 2020 - Improving Bioinformatics Prediction of microRNA Targets by Ranks Aggregation.pdf},
  journal = {Frontiers in Genetics},
  keywords = {aggregation,database,David Port,F-score,microRNA,precision and recall,prediction,receiver operating characteristic,Recommendation,target mRNA},
  mendeley-tags = {David Port,Recommendation},
  number = {January}
}

@article{radevIntroductionSpecialIssue2002,
  ids = {Radev,Radev2002,Radev2002a},
  title = {Introduction to the {{Special Issue}} on {{Summarization}}},
  author = {Radev, Dragomir R. and Hovy, Eduard and McKeown, Kathleen},
  year = {2002},
  volume = {28},
  pages = {399--408},
  issn = {0891-2017},
  doi = {10.1162/089120102762671927},
  abstract = {To provide high-quality and safe care, clinicians must be able to optimally collect, distill, and interpret patient information. Despite advances in text summarization, only limited research exists on clinical summarization, the complex and heterogeneous process of gathering, organizing and presenting patient data in various forms.},
  file = {/home/harrisonpl/Documents/PDFs/Radev et al. - 2002 - Introduction to the Special Issue on Summarization.pdf},
  journal = {Computational Linguistics},
  keywords = {hunter lab},
  mendeley-tags = {hunter lab},
  number = {4}
}

@article{radfordBetterLanguageModels2019,
  title = {Better {{Language Models}} and {{Their Implications}}, {{https://openai.com/blog/better-language-models/}}},
  author = {Radford, Alec and Wu, Jeffrey and Amodei, Dario Daniela and Amodei, Dario Daniela and Clark, Jack and Brundage, Miles and Sutskever, Ilya},
  year = {2019},
  abstract = {We've trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization\textemdash all without task-specific training.},
  keywords = {\#nosource,Favorite}
}

@article{radfordLanguageModelsAre2018,
  ids = {Radford2018},
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2018},
  pages = {24},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension , and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset-matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  file = {/home/harrisonpl/Documents/PDFs/Radford et al. - 2018 - Language Models are Unsupervised Multitask Learners.pdf},
  keywords = {ai,autoencoder,byte paire encoding,language models,neural nets,nlp,openai,question answering,summarization,transformer,translation,zero shot learning}
}

@article{rajkomarEnsuringFairnessMachine2018,
  ids = {Rajkomar2018},
  title = {Ensuring Fairness in Machine Learning to Advance Health Equity},
  author = {Rajkomar, Alvin and Hardt, Michaela and Howell, Michael D. and Corrado, Greg and Chin, Marshall H.},
  year = {2018},
  volume = {169},
  pages = {866--872},
  issn = {15393704},
  doi = {10.7326/M18-1990},
  abstract = {Machine learning is used increasingly in clinical care to improve diagnosis, treatment selection, and health system efficiency. Because machine-learning models learn from historically collected data, populations that have experienced human and structural biases in the past - called protected groups - are vulnerable to harm by incorrect predictions or withholding of resources. This article describes how model design, biases in data, and the interactions of model predictions with clinicians and patients may exacerbate health care disparities. Rather than simply guarding against these harms passively, machine-learning systems should be used proactively to advance health equity. For that goal to be achieved, principles of distributive justice must be incorporated into model design, deployment, and evaluation. The article describes several technical implementations of distributive justice - specifically those that ensure equality in patient outcomes, performance, and resource allocation - and guides clinicians as to when they should prioritize each principle. Machine learning is providing increasingly sophisticated decision support and population-level monitoring, and it should encode principles of justice to ensure that models benefit all patients.},
  file = {/home/harrisonpl/Documents/PDFs/Rajkomar et al. - 2018 - Ensuring fairness in machine learning to advance health equity.pdf},
  journal = {Annals of Internal Medicine},
  number = {12}
}

@article{rareyFeatureTreesNew1998,
  ids = {Rarey1998},
  title = {Feature Trees: {{A}} New Molecular Similarity Measure Based on Tree Matching},
  author = {Rarey, Matthias and Dixon, J. Scott},
  year = {1998},
  volume = {12},
  pages = {471--490},
  publisher = {{Springer Netherlands}},
  issn = {0920654X},
  doi = {10.1023/A:1008068904628},
  abstract = {In this paper we present a new method for evaluating molecular similarity between small organic compounds. Instead of a linear representation like fingerprints, a more complex description, a feature tree, is calculated for a molecule. A feature tree represents hydrophobic fragments and functional groups of the molecule and the way these groups are linked together. Each node in the tree is labeled with a set of features representing chemical properties of the part of the molecule corresponding to the node. The comparison of feature trees is based on matching subtrees of two feature trees onto each other. Two algorithms for tackling the matching problem are described throughout this paper. On a dataset of about 1000 molecules, we demonstrate the ability of our approach to identify molecules belonging to the same class of inhibitors. With a second dataset of 58 molecules with known binding modes taken from the Brookhaven Protein Data Bank, we show that the matchings produced by our algorithms are compatible with the relative orientation of the molecules in the active site in 61\% of the test cases. The average computation time for a pair comparison is about 50 ms on a current workstation.},
  file = {/home/harrisonpl/Documents/PDFs/Rarey, Dixon - 1998 - Feature trees.pdf},
  journal = {Journal of Computer-Aided Molecular Design},
  keywords = {Database screening,Molecular descriptors,Molecular similarity,Molecular superposition,Structural alignment},
  number = {5},
  pmid = {9834908}
}

@article{ratermannDrugsThatAct2018,
  ids = {Ratermann2018},
  title = {Drugs {{That Act}} on the {{Immune System}}: {{Cytokines}} and {{Monoclonal Antibodies}}},
  author = {Ratermann, Kelley and Cox, Jessica and Benitez, Lydia and Davis, Frank},
  year = {2018},
  volume = {40},
  pages = {477--487},
  issn = {03786080},
  doi = {10.1016/bs.seda.2018.07.010},
  abstract = {This chapter serves as a review of publications in 2017 pertaining to new or unique adverse drug events associated with cytokines and monoclonal antibodies. Case reports and large trials detailing such previously unknown adverse effects are summarized for colony-stimulating factors, thrombopoietin agonists, interleukins, tumor necrosis factor alpha antagonists and many monoclonal antibodies. There are also several new-to-the-market biosimilars that have begun to be studied and are included here for the first time. New monoclonal antibodies included in this edition are avelumab, brodalumab, and pinatuzumab. This review also encompasses a number of drugs with new and expanded FDA-approved uses including tocilizumab, nivolumab, pembrolizumab, and ustekinumab.},
  file = {/home/harrisonpl/Documents/PDFs/Ratermann et al. - 2018 - Drugs That Act on the Immune System.pdf},
  isbn = {9780444641199},
  journal = {Side Effects of Drugs Annual},
  keywords = {Cancer,Cytokines,Monoclonal antibodies,Natural language processing,Side effects,Toxicity},
  mendeley-tags = {Natural language processing}
}

@article{rebholz-schuhmannForewordThirdInternational2008,
  title = {Foreword to the Third {{International Symposium}} for {{Semantic Mining}} in {{Biomedicine}} ( {{SMBM}} 2008 )},
  author = {{Rebholz-schuhmann}, Dietrich and Cohen, Kevin and Friedman, Carol},
  year = {2008},
  keywords = {\#nosource,hunter lab},
  number = {August}
}

@article{rectorGruberOntologiesToday2019,
  title = {On beyond {{Gruber}}: ``{{Ontologies}}'' in Today's Biomedical Information Systems and the Limits of {{OWL}}},
  author = {Rector, Alan and Schulz, Stefan and Rodrigues, Jean Marie and Chute, Christopher G. and Solbrig, Harold},
  year = {2019},
  month = jun,
  volume = {2},
  abstract = {The word ``ontology'' was introduced to information systems when only closed-world reasoning systems were available. It was ``borrowed'' from philosophy, but literal links to its philosophical meaning were explicitly disavowed. Since then, open-world reasoning systems based on description logics have been developed, OWL has become a standard, and philosophical issues have been raised. The result has too often been confusion. The question ``What statements are ontological'' receives a variety of answers. A clearer vocabulary that is better suited to today's information systems is needed. The project to base ICD-11 on a ``Common Ontology'' required addressing this confusion. This paper sets out to systematise the lessons of that experience and subsequent discussions. We explore the semantics of open-world and closed-world systems. For specifying knowledge bases and software, we propose ``invariants'' or, more fully, ``the first order invariant part of the background domain knowledge base'' as an alternative to the words ``ontology'' and ``ontological.'' We discuss the role and limitations of OWL and description logics and how they are complementary to closed world systems such as frames and to less formal ``knowledge organisation systems''. We illustrate why the conventions of classifications such as ICD cannot be formulated directly in OWL, but can be linked to OWL knowledge bases by queries. We contend that while OWL and description logics are major advances for representing invariants and terminologies, they must be combined with other technologies to represent broader background knowledge faithfully. The ICD-11 architecture is one approach. We argue that such hybrid architectures can and should be developed further.},
  keywords = {\#nosource,Description logics,ICD,Knowledge representation,Ontology,OWL,Terminology}
}

@article{redestigCompensationSystematicCrosscontribution2009,
  ids = {Redestig2009},
  title = {Compensation for Systematic Cross-Contribution Improves Normalization of Mass Spectrometry Based Metabolomics Data},
  author = {Redestig, Henning and Fukushima, Atsushi and Stenlund, Hans and Moritz, Thomas and Arita, Masanori and Saito, Kazuki and Kusano, Miyako},
  year = {2009},
  volume = {81},
  pages = {7974--7980},
  issn = {00032700},
  doi = {10.1021/ac901143w},
  abstract = {Most mass spectrometry based metabolomics studies are semiquantitative and depend on efficient normalization techniques to suppress systematic error. A common approach is to include isotope-labeled internal standards (ISs) and then express the estimated metabolite abundances relative to the IS. Because of problems such as insufficient chromatographic resolution, however, the analytes may directly influence estimates of the IS, a phenomenon known as cross-contribution (CC). Normalization using ISs that suffer from CC effects will cause significant loss of information if the interfering analytes are associated with the studied factors. We present a novel normalization algorithm, which compensates for systematic CC effects that can be traced back to a linear association with the experimental design. The proposed method was found to be superior at purifying the signal of interest compared to current normalization methods when applied to two biological data sets and a multicomponent dilution mixture. Our method is applicable to data from randomized and designed experiments that use ISs to monitor the systematic error. \textcopyright{} 2009 American Chemical Society.},
  file = {/home/harrisonpl/Documents/PDFs/Redestig et al. - 2009 - Compensation for systematic cross-contribution improves normalization of mass.pdf},
  journal = {Analytical Chemistry},
  keywords = {Algorithms,Isotope Labeling,Mass Spectrometry,metabolomics},
  number = {19},
  pmid = {19743813}
}

@article{reeseStandardVariationFile2010,
  title = {A Standard Variation File Format for Human Genome Sequences.},
  booktitle = {Genome Biology},
  author = {Reese, Martin G. and Moore, Barry and Batchelor, Colin and Salas, Fidel and Cunningham, Fiona and Marth, Gabor T. and Stein, Lincoln and Flicek, Paul and Yandell, Mark and Eilbeck, Karen},
  year = {2010},
  volume = {11},
  issn = {14656914},
  doi = {10.1186/gb-2010-11-8-r88},
  abstract = {Here we describe the Genome Variation Format (GVF) and the 10Gen dataset. GVF, an extension of Generic Feature Format version 3 (GFF3), is a simple tab-delimited format for DNA variant files, which uses Sequence Ontology to describe genome variation data. The 10Gen dataset, ten human genomes in GVF format, is freely available for community analysis from the Sequence Ontology website and from an Amazon elastic block storage (EBS) snapshot for use in Amazon's EC2 cloud computing environment.},
  file = {/home/harrisonpl/Documents/PDFs/Reese et al. - 2010 - A standard variation file format for human genome sequences.pdf},
  keywords = {ontology},
  mendeley-tags = {ontology},
  number = {8}
}

@incollection{reinholdPreanalyticConsiderationsMass2019,
  title = {Pre-Analytic Considerations for Mass Spectrometry-Based Untargeted Metabolomics Data},
  author = {Reinhold, Dominik and {Pielke-Lombardo}, Harrison and Jacobson, Sean and Ghosh, Debashis and Kechris, Katerina},
  year = {2019},
  volume = {1978},
  abstract = {Metabolomics is the science of characterizing and quantifying small molecule metabolites in biological systems. These metabolites give organisms their biochemical characteristics, providing a link between genotype, environment, and phenotype. With these opportunities also come data challenges, such as compound annotation, missing values, and batch effects. We present the steps of a general pipeline to process untargeted mass spectrometry data to alleviate the latter two challenges. We assume to have a matrix with metabolite abundances, with metabolites in rows and samples in columns. The steps in the pipeline include summarizing technical replicates (if available), filtering, imputing, transforming, and normalizing the data. In each of these steps, a method and parameters should be chosen based on assumptions one is willing to make, the question of interest, and diagnostic tools. Besides giving a general pipeline that can be adapted by the reader, our goal is to review diagnostic tools and criteria that are helpful when making decisions in each step of the pipeline and assessing the effectiveness of normalization and batch correction. We conclude by giving a list of useful packages and discuss some alternative approaches that might be more appropriate for the reader's data.},
  annotation = {ZSCC: 0000001},
  keywords = {\#nosource,Filtering,Imputation,Mass spectrometry,Metabolomics,Normalization,Pre-analytic,Processing,Technical replicates,Untargeted}
}

@article{ribeiroMechanismCatalyticSite2018,
  ids = {Ribeiro2018},
  title = {Mechanism and {{Catalytic Site Atlas}} ({{M}}-{{CSA}}): {{A}} Database of Enzyme Reaction Mechanisms and Active Sites},
  shorttitle = {Mechanism and {{Catalytic Site Atlas}} ({{M}}-{{CSA}})},
  author = {Ribeiro, Ant{\'o}nio J.M. and Holliday, Gemma L. and Furnham, Nicholas and Tyzack, Jonathan D. and Ferris, Katherine and Thornton, Janet M.},
  year = {2018},
  volume = {46},
  pages = {D618--D623},
  issn = {13624962},
  doi = {10.1093/nar/gkx1012},
  abstract = {M-CSA (Mechanism and Catalytic Site Atlas) is a database of enzyme active sites and reaction mechanisms that can be accessed at www.ebi.ac.uk/thornton-srv/m-csa. Our objectives with M-CSA are to provide an open data resource for the community to browse known enzyme reaction mechanisms and catalytic sites, and to use the dataset to understand enzyme function and evolution. M-CSA results from the merging of two existing databases, MACiE (Mechanism, Annotation and Classification in Enzymes), a database of enzyme mechanisms, and CSA (Catalytic Site Atlas), a database of catalytic sites of enzymes. We are releasing M-CSA as a new website and underlying database architecture. At the moment, M-CSA contains 961 entries, 423 of these with detailed mechanism information, and 538 with information on the catalytic site residues only. In total, these cover 81\% (195/241) of third level EC numbers with a PDB structure, and 30\% (840/2793) of fourth level EC numbers with a PDB structure, out of 6028 in total. By searching for close homologues, we are able to extend M-CSA coverage of PDB and UniProtKB to 51 993 structures and to over five million sequences, respectively, of which about 40\% and 30\% have a conserved active site.},
  file = {/home/harrisonpl/Documents/PDFs/Ribeiro et al. - 2018 - Mechanism and Catalytic Site Atlas (M-CSA).pdf},
  journal = {Nucleic Acids Research},
  keywords = {mechanism},
  mendeley-tags = {mechanism},
  number = {D1}
}

@article{rignanoConceptPurposeBiology1931,
  title = {The {{Concept}} of {{Purpose}} in {{Biology}}},
  author = {RIGNANO, EUGENIO},
  year = {1931},
  volume = {XL},
  pages = {335--340},
  issn = {0026-4423},
  doi = {10.1093/mind/xl.159.335},
  journal = {Mind},
  keywords = {\#nosource},
  number = {159}
}

@article{robinsonOntologizingGeneexpressionMicroarray2004,
  ids = {Robinson2004},
  title = {Ontologizing Gene-Expression Microarray Data: {{Characterizing}} Clusters with Gene Oncology},
  shorttitle = {Ontologizing Gene-Expression Microarray Data},
  author = {Robinson, Peter N. and Wollstein, Andreas and B{\"o}hme, Ulrike and Beattie, Brad},
  year = {2004},
  volume = {20},
  pages = {979--981},
  issn = {13674803},
  doi = {10.1093/bioinformatics/bth040},
  abstract = {Summary: An XML-based Java application is described that provides a function-oriented overview of the results of cluster analysis of gene-expression microarray data based on Gene Ontology terms and associations. The application generates one HTML page with listings of the frequencies of explicit and implicit Gene Ontology annotations for each cluster, and separate, linked pages with listings of explicit annotations for each gene in a cluster. \textcopyright{} Oxford University Press 2004; all rights reserved.},
  file = {/home/harrisonpl/Documents/PDFs/Robinson et al. - 2004 - Ontologizing gene-expression microarray data.pdf},
  journal = {Bioinformatics},
  keywords = {Natural language processing},
  mendeley-tags = {Natural language processing},
  number = {6}
}

@article{rommArtificialIntelligenceDrug2020,
  ids = {Romm2020},
  title = {Artificial {{Intelligence}} in {{Drug Treatment}}},
  author = {Romm, Eden L. and Tsigelny, Igor F.},
  year = {2020},
  volume = {60},
  pages = {353--369},
  issn = {0362-1642},
  doi = {10.1146/annurev-pharmtox-010919-023746},
  abstract = {The most common applications of artificial intelligence (AI) in drug treatment have to do with matching patients to their optimal drug or combination of drugs, predicting drug-target or drug-drug interactions, and optimizing treatment protocols. This review outlines some of the recently developed AI methods aiding the drug treatment and administration process. Selection of the best drug(s) for a patient typically requires the integration of patient data, such as genetics or proteomics, with drug data, like compound chemical descriptors, to score the therapeutic efficacy of drugs. The prediction of drug interactions often relies on similarity metrics, assuming that drugs with similar structures or targets will have comparable behavior or may interfere with each other. Optimizing the dosage schedule for administration of drugs is performed using mathematical models to interpret pharmacokinetic and pharmacodynamic data. The recently developed and powerful models for each of these tasks are addressed, explained, and analyzed here.},
  file = {/home/harrisonpl/Documents/PDFs/Romm, Tsigelny - 2020 - Artificial Intelligence in Drug Treatment.pdf},
  journal = {Annual Review of Pharmacology and Toxicology},
  keywords = {artificial intelligence,combination therapy,deep learning,drug combination,HPL comprehensive exam,machine learning,medicine,personalized},
  mendeley-tags = {HPL comprehensive exam},
  number = {1}
}

@article{rougierImplicitExplicitRepresentations2009,
  ids = {Rougier2009},
  title = {Implicit and {{Explicit Representations To}}},
  author = {Rougier, Nicolas P},
  year = {2009},
  volume = {22},
  pages = {155--160},
  doi = {10.1016/j.neunet.2009.01.00},
  abstract = {During the past decades, the symbol grounding problem, as it has been identified by [9], became a prominent problem in the cognitive science society. The idea that a symbol was much more than a mere meaningless token that can be processed through some algorithm, sheds a new light on higher brain functions such as lan- guage and cognition. We present in this article a computational framework that may help our understanding of the nature of grounded representations. Two models are briefly introduced that aim at emphasizing the difference we make between implicit and explicit representations.},
  file = {/home/harrisonpl/Documents/PDFs/Rougier - 2009 - Implicit and Explicit Representations To.pdf},
  keywords = {cognition,Computational Neuroscience,Embodied,Representation,Symbol},
  number = {2}
}

@article{rubinTranscriptionFactorEnrichment2020,
  ids = {Rubin2020},
  title = {Transcription Factor Enrichment Analysis ({{TFEA}}): {{Quantifying}} the Activity of Hundreds of Transcription Factors from a Single Experiment},
  author = {Rubin, Jonathan D. and Stanley, Jacob T. and Sigauke, Rutendo F. and Levandowski, Cecilia B. and Maas, Zachary L. and Westfall, Jessica and Taatjes, Dylan J. and Dowell, Robin D.},
  year = {2020},
  pages = {2020.01.25.919738},
  doi = {10.1101/2020.01.25.919738},
  abstract = {Detecting differential activation of transcription factors (TFs) in response to perturbation provides insight into cellular processes. Transcription Factor Enrichment Analysis (TFEA) is a robust and reliable computational method that detects differential activity of hundreds of TFs given any set of perturbation data. TFEA draws inspiration from GSEA and detects positional motif enrichment within a list of ranked regions of interest (ROIs). As ROIs are typically inferred from the data, we also introduce muMerge , a statistically principled method of generating a consensus list of ROIs from multiple replicates and conditions. TFEA is broadly applicable to data that informs on transcriptional regulation including nascent (eg. PRO-Seq), CAGE, ChIP-Seq, and accessibility (e.g. ATAC-Seq). TFEA not only identifies the key regulators responding to a perturbation, but also temporally unravels regulatory networks with time series data. Consequently, TFEA serves as a hypothesis-generating tool that provides an easy, rigorous, and cost-effective means to broadly assess TF activity yielding new biological insights.},
  file = {/home/harrisonpl/Documents/PDFs/Rubin et al. - 2020 - Transcription factor enrichment analysis (TFEA).pdf},
  journal = {bioRxiv}
}

@article{ruderUnsupervisedCrossLingualRepresentation2019,
  ids = {Ruder2019,Ruder2019a},
  title = {Unsupervised {{Cross}}-{{Lingual Representation Learning}}},
  author = {Ruder, Sebastian and S{\o}gaard, Anders and Vuli{\'c}, Ivan},
  year = {2019},
  pages = {31--38},
  doi = {10.18653/v1/p19-4007},
  abstract = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8\% average accuracy on XNLI, +12.3\% average F1 score on MLQA, and +2.1\% average F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 11.8\% in XNLI accuracy for Swahili and 9.2\% for Urdu over the previous XLM model. We also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-Ris very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make XLM-R code, data, and models publicly available.},
  annotation = {\_eprint: 1911.02116},
  archivePrefix = {arXiv},
  arxivid = {1911.02116},
  eprint = {1911.02116},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Ruder et al. - 2019 - Unsupervised Cross-Lingual Representation Learning.pdf}
}

@article{rudraPredictiveModelingMiRNAmediated2018,
  title = {Predictive Modeling of {{miRNA}}-Mediated Predisposition to Alcohol-Related Phenotypes in Mouse},
  author = {Rudra, Pratyaydipta and Shi, Wen J. and Russell, Pamela and Vestal, Brian and Tabakoff, Boris and Hoffman, Paula and Kechris, Katerina and Saba, Laura},
  year = {2018},
  month = aug,
  volume = {19},
  pages = {639},
  issn = {1471-2164},
  doi = {10.1186/s12864-018-5004-3},
  abstract = {MicroRNAs (miRNAs) are small non-coding RNAs that bind messenger RNAs and promote their degradation or repress their translation. There is increasing evidence of miRNAs playing an important role in alcohol related disorders. However, the role of miRNAs as mediators of the genetic effect on alcohol phenotypes is not fully understood. We conducted a high-throughput sequencing study to measure miRNA expression levels in alcohol na\"ive animals in the LXS panel of recombinant inbred (RI) mouse strains. We then combined the sequencing data with genotype data, microarry gene expression data, and data on alcohol-related behavioral phenotypes such as 'Drinking in the dark', 'Sleep time', and 'Low dose activation' from the same RI panel. SNP-miRNA-gene triplets with strong association within the triplet that were also associated with one of the 4 alcohol phenotypes were selected and a Bayesian network analysis was used to aggregate results into a directed network model.},
  file = {/home/harrisonpl/Documents/PDFs/Rudra et al. - 2018 - Predictive modeling of miRNA-mediated predisposition to alcohol-related.pdf},
  journal = {BMC Genomics},
  language = {en},
  number = {1}
}

@article{samanviSubgraphSimilaritySearch2015,
  ids = {Samanvi2015},
  title = {Subgraph {{Similarity Search}} in {{Large Graphs}}},
  author = {Samanvi, Kanigalpula and Sivadasan, Naveen},
  year = {2015},
  abstract = {One of the major challenges in applications related to social networks, computational biology, collaboration networks etc., is to efficiently search for similar patterns in their underlying graphs. These graphs are typically noisy and contain thousands of vertices and millions of edges. In many cases, the graphs are unlabeled and the notion of similarity is also not well defined. We study the problem of searching an induced subgraph in a large target graph that is most similar to the given query graph. We assume that the query graph and target graph are undirected and unlabeled. We use graphlet kernels \$\textbackslash backslash\$cite\{shervashidze2009efficient\} to define graph similarity. Graphlet kernels are known to perform better than other kernels in different applications. Our algorithm maps topological neighborhood information of vertices in the query and target graphs to vectors. These local topological informations are then combined to find a target subgraph having highly similar global topology with the given query graph. We tested our algorithm on several real world networks such as facebook network, google plus network, youtube network, amazon network etc. Most of them contain thousands of vertices and million edges. Our algorithm is able to detect highly similar matches when queried in these networks. Our multi-threaded implementation takes about one second to find the match on a 32 core machine, excluding the time for one time preprocessing. Computationally expensive parts of our algorithm can be further scaled to standard parallel and distributed frameworks like map-reduce.},
  annotation = {\_eprint: 1512.05256},
  archivePrefix = {arXiv},
  arxivid = {1512.05256},
  eprint = {1512.05256},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Samanvi, Sivadasan - 2015 - Subgraph Similarity Search in Large Graphs.pdf},
  keywords = {-similarity search,graph kernel,nearest neighbors search,subgraph similarity search}
}

@article{sankarasubramaniamTextSummarizationUsing2014,
  title = {Text Summarization Using {{Wikipedia}}},
  author = {Sankarasubramaniam, Yogesh and Ramanathan, Krishnan and Ghosh, Subhankar},
  year = {2014},
  volume = {50},
  pages = {443--461},
  issn = {03064573},
  doi = {10.1016/j.ipm.2014.02.001},
  abstract = {Automatic text summarization has been an active field of research for many years. Several approaches have been proposed, ranging from simple position and word-frequency methods, to learning and graph based algorithms. The advent of human-generated knowledge bases like Wikipedia offer a further possibility in text summarization - they can be used to understand the input text in terms of salient concepts from the knowledge base. In this paper, we study a novel approach that leverages Wikipedia in conjunction with graph-based ranking. Our approach is to first construct a bipartite sentence-concept graph, and then rank the input sentences using iterative updates on this graph. We consider several models for the bipartite graph, and derive convergence properties under each model. Then, we take up personalized and query-focused summarization, where the sentence ranks additionally depend on user interests and queries, respectively. Finally, we present a Wikipedia-based multi-document summarization algorithm. An important feature of the proposed algorithms is that they enable real-time incremental summarization - users can first view an initial summary, and then request additional content if interested. We evaluate the performance of our proposed summarizer using the ROUGE metric, and the results show that leveraging Wikipedia can significantly improve summary quality. We also present results from a user study, which suggests that using incremental summarization can help in better understanding news articles. \textcopyright{} 2014 Elsevier Ltd. All rights reserved.},
  journal = {Information Processing and Management},
  keywords = {\#nosource,hunter lab,Natural language processing,Personalization,Sentence ranking,Summarization,Wikipedia},
  number = {3}
}

@article{santosComprehensiveMapMolecular2016,
  ids = {Santos2016},
  title = {A Comprehensive Map of Molecular Drug Targets},
  author = {Santos, Rita and Ursu, Oleg and Gaulton, Anna and Bento, A. Patr{\'i}cia and Donadi, Ramesh S. and Bologa, Cristian G. and Karlsson, Anneli and {Al-Lazikani}, Bissan and Hersey, Anne and Oprea, Tudor I. and Overington, John P.},
  year = {2016},
  volume = {16},
  pages = {19--34},
  issn = {14741784},
  doi = {10.1038/nrd.2016.230},
  abstract = {The success of mechanism-based drug discovery depends on the definition of the drug target. This definition becomes even more important as we try to link drug response to genetic variation, understand stratified clinical efficacy and safety, rationalize the differences between drugs in the same therapeutic class and predict drug utility in patient subgroups. However, drug targets are often poorly defined in the literature, both for launched drugs and for potential therapeutic agents in discovery and development. Here, we present an updated comprehensive map of molecular targets of approved drugs. We curate a total of 893 human and pathogen-derived biomolecules through which 1,578 US FDA-approved drugs act. These biomolecules include 667 human-genome-derived proteins targeted by drugs for human disease. Analysis of these drug targets indicates the continued dominance of privileged target families across disease areas, but also the growth of novel first-in-class mechanisms, particularly in oncology. We explore the relationships between bioactivity class and clinical success, as well as the presence of orthologues between human and animal models and between pathogen and human genomes. Through the collaboration of three independent teams, we highlight some of the ongoing challenges in accurately defining the targets of molecular therapeutics and present conventions for deconvoluting the complexities of molecular pharmacology and drug efficacy.},
  file = {/home/harrisonpl/Documents/PDFs/Santos et al. - 2016 - A comprehensive map of molecular drug targets.pdf},
  journal = {Nature Reviews Drug Discovery},
  keywords = {Biotechnology,Business strategy in drug development,Clinical trials,Drug development,Drug therapy,Mechanism of action},
  number = {1}
}

@article{santosEuropePMCFunders2019,
  ids = {Santos2019},
  title = {Europe {{PMC Funders Group Europe PMC Funders Author Manuscripts A}} Comprehensive Map of Molecular Drug Targets},
  author = {Santos, Rita and Ursu, Oleg and Gaulton, Anna and Bento, A Patr{\'i}cia and Donadi, Ramesh S and Bologa, G and Karlsson, Anneli and {Al-lazikani}, Bissan and Hersey, Anne and Oprea, Tudor I},
  year = {2019},
  volume = {16},
  pages = {19--34},
  doi = {10.1038/nrd.2016.230.A},
  file = {/home/harrisonpl/Documents/PDFs/Santos et al. - 2019 - Europe PMC Funders Group Europe PMC Funders Author Manuscripts A comprehensive.pdf},
  number = {1}
}

@article{sarmaHowWeCapture2018,
  title = {How Do We Capture Structure in Relational Data?},
  author = {Sarma, Matthew Das},
  year = {2018},
  abstract = {It's notoriously challenging to extract meaningful features from graphs.},
  keywords = {\#nosource,mechanism}
}

@book{schankDynamicMemoryTheory1982,
  title = {Dynamic Memory: A Theory of Reminding and Learning in Computers and People},
  shorttitle = {Dynamic Memory},
  author = {Schank, Roger C.},
  year = {1982},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge [Cambridgeshire] : New York}},
  isbn = {978-0-521-24858-7 978-0-521-27029-8},
  keywords = {Artificial intelligence,Comprehension,Learning; Psychology of,Memory},
  lccn = {BF371 .S36 1982}
}

@article{schenkmanExercisePeopleEarly2012,
  ids = {ExercisePeopleEarly2012,Schenkman2012,Schenkman2012a},
  title = {Exercise for {{People}} in {{Early}}- or {{Mid}}-{{Stage Parkinson Disease}}: {{A}} 16-{{Month Randomized Controlled Trial}}},
  shorttitle = {Exercise for People in Early- or Mid-Stage {{Parkins}}},
  author = {Schenkman, Margaret and Hall, Deborah A. and Bar{\'o}n, Anna E. and Schwartz, Robert S. and Mettler, Pamela and Kohrt, Wendy M.},
  year = {2012},
  month = nov,
  volume = {92},
  pages = {1395--1410},
  issn = {0031-9023},
  doi = {10.2522/ptj.20110472},
  abstract = {BACKGROUND: Exercise confers short-term benefits for individuals with Parkinson disease (PD). OBJECTIVE: The purpose of the study was to compare short- and long-term responses among 2 supervised exercise programs and a home-based control exercise program. DESIGN: The 16-month randomized controlled exercise intervention investigated 3 exercise approaches: flexibility/balance/function exercise (FBF), supervised aerobic exercise (AE), and home-based exercise (control). SETTING: This study was conducted in outpatient clinics. PATIENTS: The participants were 121 individuals with PD (Hoehn \& Yahr stages 1-3). INTERVENTIONS: The FBF program (individualized spinal and extremity flexibility exercises followed by group balance/functional training) was supervised by a physical therapist. The AE program (using a treadmill, bike, or elliptical trainer) was supervised by an exercise trainer. Supervision was provided 3 days per week for 4 months, and then monthly (16 months total). The control group participants exercised at home using the National Parkinson Foundation Fitness Counts program, with 1 supervised, clinic-based group session per month. MEASUREMENTS: Outcomes, obtained by blinded assessors, were determined at 4, 10, and 16 months. The primary outcome measures were overall physical function (Continuous Scale-Physical Functional Performance [CS-PFP]), balance (Functional Reach Test [FRT]), and walking economy (oxygen uptake [mL/kg/min]). Secondary outcome measures were symptom severity (Unified Parkinson's Disease Rating Scale [UPDRS] activities of daily living [ADL] and motor subscales) and quality of life (39-item Parkinson's Disease Quality of Life Scale [PDQ-39]). RESULTS: Of the 121 participants, 86.8\%, 82.6\%, and 79.3\% completed 4, 10, and 16 months, respectively, of the intervention. At 4 months, improvement in CS-PFP scores was greater in the FBF group than in the control group (mean difference=4.3, 95\% confidence interval [CI]=1.2 to 7.3) and the AE group (mean difference=3.1, 95\% CI=0.0 to 6.2). Balance was not different among groups at any time point. Walking economy improved in the AE group compared with the FBF group at 4 months (mean difference=-1.2, 95\% CI=-1.9 to -0.5), 10 months (mean difference=-1.2, 95\% CI=-1.9 to -0.5), and 16 months (mean difference=-1.7, 95\% CI=-2.5 to -1.0). The only secondary outcome that showed significant differences was UPDRS ADL subscale scores: the FBF group performed better than the control group at 4 months (mean difference=-1.47, 95\% CI=-2.79 to -0.15) and 16 months (mean difference=-1.95, 95\% CI=-3.84 to -0.08). LIMITATIONS: Absence of a non-exercise control group was a limitation of the study. CONCLUSIONS: Findings demonstrated overall functional benefits at 4 months in the FBF group and improved walking economy (up to 16 months) in the AE group.},
  file = {/home/harrisonpl/Documents/PDFs/Schenkman et al. - 2012 - Exercise for People in Early- or Mid-Stage Parkinson Disease.pdf},
  journal = {Physical Therapy},
  keywords = {Activities of Daily Living,Disability Evaluation,Exercise Therapy,Female,Humans,Male,Parkinson Disease,Physical Therapy Modalities,Postural Balance,Quality of Life,Severity of Illness Index,Treatment Outcome,Walking},
  language = {eng},
  mendeley-tags = {Activities of Daily Living,Disability Evaluation,Exercise Therapy,Female,Folder - BIOS 6611,Humans,Male,Parkinson Disease,Physical Therapy Modalities,Postural Balance,Quality of Life,Severity of Illness Index,Treatment Outcome,Walking},
  number = {11}
}

@article{schmiesterEfficientParameterizationLargescale2020,
  ids = {Schmiester},
  title = {Efficient Parameterization of Large-Scale Dynamic Models Based on Relative Measurements},
  author = {Schmiester, Leonard and Sch{\"a}lte, Yannik and Fr{\"o}hlich, Fabian and Hasenauer, Jan and Weindl, Daniel},
  year = {2020},
  volume = {36},
  pages = {594--602},
  issn = {13674811},
  doi = {10.1093/bioinformatics/btz581},
  abstract = {MOTIVATION: Mechanistic models of biochemical reaction networks facilitate the quantitative understanding of biological processes and the integration of heterogeneous datasets. However, some biological processes require the consideration of comprehensive reaction networks and therefore large-scale models. Parameter estimation for such models poses great challenges, in particular when the data are on a relative scale. RESULTS: Here, we propose a novel hierarchical approach combining (i) the efficient analytic evaluation of optimal scaling, offset and error model parameters with (ii) the scalable evaluation of objective function gradients using adjoint sensitivity analysis. We evaluate the properties of the methods by parameterizing a pan-cancer ordinary differential equation model (\textbackslash textgreater1000 state variables, \textbackslash textgreater4000 parameters) using relative protein, phosphoprotein and viability measurements. The hierarchical formulation improves optimizer performance considerably. Furthermore, we show that this approach allows estimating error model parameters with negligible computational overhead when no experimental estimates are available, providing an unbiased way to weight heterogeneous data. Overall, our hierarchical formulation is applicable to a wide range of models, and allows for the efficient parameterization of large-scale models based on heterogeneous relative measurements. AVAILABILITY AND IMPLEMENTATION: Supplementary code and data are available online at http://doi.org/10.5281/zenodo.3254429 and http://doi.org/10.5281/zenodo.3254441. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
  file = {/home/harrisonpl/Documents/PDFs/Schmiester et al. - 2020 - Efficient parameterization of large-scale dynamic models based on relative.pdf},
  journal = {Bioinformatics (Oxford, England)},
  number = {2}
}

@article{schrimlDiseaseOntologyBackbone2012,
  ids = {Schriml2012},
  title = {Disease Ontology: {{A}} Backbone for Disease Semantic Integration},
  shorttitle = {Disease {{Ontology}}},
  author = {Schriml, Lynn Marie and Arze, Cesar and Nadendla, Suvarna and Chang, Yu Wei Wayne and Mazaitis, Mark and Felix, Victor and Feng, Gang and Kibbe, Warren Alden},
  year = {2012},
  volume = {40},
  pages = {D940--D946},
  issn = {03051048},
  doi = {10.1093/nar/gkr972},
  abstract = {The Disease Ontology (DO) database (http:// disease-ontology.org) represents a comprehensive knowledge base of 8043 inherited, developmental and acquired human diseases (DO version 3, revision 2510). The DO web browser has been designed for speed, efficiency and robustness through the use of a graph database. Full-text contextual searching functionality using Lucene allows the querying of name, synonym, definition, DOID and cross-reference (xrefs) with complex Boolean search strings. The DO semantically integrates disease and medical vocabularies through extensive cross mapping and integration of MeSH, ICD, NCI\^a\texteuro\texttrademark s thesaurus, SNOMED CT and OMIM disease-specific terms and identifiers. The DO is utilized for disease annotation by major biomedical databases (e.g. Array Express, NIF, IEDB), as a standard representation of human disease in biomedical ontologies (e.g. IDO, Cell line ontology, NIFSTD ontology, Experimental Factor Ontology, Influenza Ontology), and as an ontological cross mappings resource between DO, MeSH and OMIM (e.g. GeneWiki). The DO project (http://diseaseontology.sf.net) has been incorporated into open source tools (e.g. Gene Answers, FunDO) to connect gene and disease biomedical data through the lens of human disease. The next iteration of the DO web browser will integrate DO\^a\texteuro\texttrademark s extended relations and logical definition representation along with these biomedical resource cross-mappings. \textcopyright{} The Author(s) 2011.},
  file = {/home/harrisonpl/Documents/PDFs/Schriml et al. - 2012 - Disease ontology.pdf},
  journal = {Nucleic Acids Research},
  keywords = {ontology},
  mendeley-tags = {ontology},
  number = {D1}
}

@article{sejnowskiUnreasonableEffectivenessDeep2020,
  ids = {Sejnowski2020},
  title = {The Unreasonable Effectiveness of Deep Learning in Artificial Intelligence},
  author = {Sejnowski, Terrence J.},
  year = {2020},
  pages = {201907373},
  issn = {0027-8424},
  doi = {10.1073/pnas.1907373117},
  abstract = {Deep learning networks have been trained to recognize speech, caption photographs, and translate text between languages at high levels of performance. Although applications of deep learning networks to real-world problems have become ubiquitous, our understanding of why they are so effective is lacking. These empirical results should not be possible according to sample complexity in statistics and nonconvex optimization theory. However, paradoxes in the training and effectiveness of deep learning networks are being investigated and insights are being found in the geometry of high-dimensional spaces. A mathematical theory of deep learning would illuminate how they function, allow us to assess the strengths and weaknesses of different network architectures, and lead to major improvements. Deep learning has provided natural ways for humans to communicate with digital devices and is foundational for building artificial general intelligence. Deep learning was inspired by the architecture of the cerebral cortex and insights into autonomy and general intelligence may be found in other brain regions that are essential for planning and survival, but major breakthroughs will be needed to achieve these goals.},
  file = {/home/harrisonpl/Documents/PDFs/Sejnowski - 2020 - The unreasonable effectiveness of deep learning in artificial intelligence.pdf},
  journal = {Proceedings of the National Academy of Sciences}
}

@article{selkovFunctionalAnalysisGapped2000,
  ids = {Selkov2000},
  title = {Functional Analysis of Gapped Microbial Genomes: {{Amino}} Acid Metabolism of {{Thiobacillus}} Ferrooxidans},
  shorttitle = {Functional Analysis of Gapped Microbial Genomes},
  author = {Selkov, Evgeni and Overbeek, Ross and Kogan, Yakov and Chu, Lien and Vonstein, Veronika and Holmes, David and Silver, Simon and Haselkorn, Robert and Fonstein, Michael},
  year = {2000},
  volume = {97},
  pages = {3509--3514},
  issn = {00278424},
  doi = {10.1073/pnas.97.7.3509},
  abstract = {A gapped genome sequence of the biomining bacterium Thiobacillus ferrooxidans strain ATCC23270 was assembled from sheared DNA fragments (3.2- times coverage) into 1,912 contigs. A total of 2,712 potential genes (ORFs) were identified in 2.6 Mbp (megabase pairs) of Thiobacillus genomic sequence. Of these genes, 2,159 could be assigned functions by using the WIT-Pro/EMP genome analysis system, most with a high degree of certainty. Nine hundred of the genes have been assigned roles in metabolic pathways, producing an overview of cellular biosynthesis, bioenergetics, and catabolism. Sequence similarities, relative gene positions on the chromosome, and metabolic reconstruction (placement of gene products in metabolic pathways) were all used to aid gene assignments and for development of a functional overview. Amino acid biosynthesis was chosen to demonstrate the analytical capabilities of this approach. Only 10 expected enzymatic activities, of the nearly 150 involved in the biosynthesis of all 20 amino acids, are currently unassigned in the Thiobacillus genome. This result compares favorably with 10 missing genes for amino acid biosynthesis in the complete Escherichia coil genome. Gapped genome analysis can therefore give a decent picture of the central metabolism of a microorganism, equivalent to that of a complete sequence, at significantly lower cost.},
  file = {/home/harrisonpl/Documents/PDFs/Selkov et al. - 2000 - Functional analysis of gapped microbial genomes.pdf},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  keywords = {Gapped genomes,Genome analysis,Metabolic reconstruction},
  number = {7},
  pmid = {10737802}
}

@article{sennrichNeuralMachineTranslation2016,
  ids = {Sennrich2016},
  title = {Neural Machine Translation of Rare Words with Subword Units},
  author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  year = {2016},
  volume = {3},
  pages = {1715--1725},
  doi = {10.18653/v1/p16-1162},
  abstract = {Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English\textrightarrow German and English\textrightarrow Russian by up to 1.1 and 1.3 Bleu, respectively.},
  annotation = {\_eprint: 1508.07909},
  archivePrefix = {arXiv},
  arxivid = {1508.07909},
  eprint = {1508.07909},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Sennrich et al. - 2016 - Neural machine translation of rare words with subword units.pdf},
  isbn = {9781510827585},
  journal = {54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers},
  keywords = {byte pair encoding,nlp,openai,translation}
}

@article{sennStatisticsPracticeAdvisory2002,
  title = {Statistics in Practice Advisory Editor},
  author = {Senn, Stephen},
  year = {2002},
  doi = {10.1002/sim.1602},
  file = {/home/harrisonpl/Documents/PDFs/Senn - 2002 - Statistics in practice advisory editor.pdf},
  isbn = {0471489867}
}

@article{serbanExploringModularityBiological2020,
  ids = {Serban2020},
  title = {Exploring Modularity in Biological Networks},
  author = {Serban, Maria},
  year = {2020},
  volume = {375},
  pages = {20190316},
  issn = {0962-8436},
  doi = {10.1098/rstb.2019.0316},
  abstract = {Network theoretical approaches have shaped our understanding of many different kinds of biological modularity. This essay makes the case that to capture these contributions, it is useful to think about the role of network models in exploratory research. The overall point is that it is possible to provide a systematic analysis of the exploratory functions of network models in bioscientific research. Using two examples from molecular and developmental biology, I argue that often the same modelling approach can perform one or more exploratory functions, such as introducing new directions of research, offering a complementary set of concepts, methods and algorithms for individuating important features of natural phenomena, generating proofs of principle demonstrations and potential explanations for phenomena of interest and enlarging the scope of certain research agendas.},
  file = {/home/harrisonpl/Documents/PDFs/Serban - 2020 - Exploring modularity in biological networks.pdf},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  keywords = {exploratory,HPL comprehensive exam,models,modularity,networks},
  mendeley-tags = {HPL comprehensive exam},
  number = {1796}
}

@article{shahMetabolomicsDataAnalysis2015,
  ids = {Shah2015},
  title = {Metabolomics Data Analysis and Missing Value Issues with Application to Infarcted Mouse Hearts},
  author = {Shah, Jasmit S. and Brock, Guy N. and Rai, Shesh N.},
  year = {2015},
  volume = {16},
  pages = {P16},
  issn = {14712105},
  doi = {10.1186/1471-2105-16-S15-P16},
  file = {/home/harrisonpl/Documents/PDFs/Shah et al. - 2015 - Metabolomics data analysis and missing value issues with application to.pdf},
  journal = {BMC Bioinformatics},
  number = {15}
}

@article{shameerNetworkBiologyInformedComputational2018,
  ids = {Shameer2018,Shameer2018a},
  title = {A {{Network}}-{{Biology Informed Computational Drug Repositioning Strategy}} to {{Target Disease Risk Trajectories}} and {{Comorbidities}} of {{Peripheral Artery Disease}}.},
  author = {Shameer, Khader and Dow, Garrett and Glicksberg, Benjamin S and Johnson, Kipp W and Ze, Yi and Tomlinson, Max S and Readhead, Ben and Dudley, Joel T and Kullo, Iftikhar J},
  year = {2018},
  volume = {2017},
  pages = {108--117},
  issn = {2153-4063},
  abstract = {Currently, drug discovery approaches focus on the design of therapies that alleviate an index symptom by reengineering the underlying biological mechanism in agonistic or antagonistic fashion. For example, medicines are routinely developed to target an essential gene that drives the disease mechanism. Therapeutic overloading where patients get multiple medications to reduce the primary and secondary side effect burden is standard practice. This single-symptom based approach may not be scalable, as we understand that diseases are more connected than random and molecular interactions drive disease comorbidities. In this work, we present a proof-of-concept drug discovery strategy by combining network biology, disease comorbidity estimates, and computational drug repositioning, by targeting the risk factors and comorbidities of peripheral artery disease, a vascular disease associated with high morbidity and mortality. Individualized risk estimation and recommending disease sequelae based therapies may help to lower the mortality and morbidity of peripheral artery disease.},
  file = {/home/harrisonpl/Documents/PDFs/Shameer et al. - 2018 - A Network-Biology Informed Computational Drug Repositioning Strategy to Target.pdf},
  journal = {AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science},
  keywords = {mechanism},
  mendeley-tags = {mechanism},
  number = {6},
  pmid = {29888052}
}

@misc{shamsAmLearningMicro,
  title = {I Am {{Learning}}: {{Micro}}- and {{Macro}}-Average of {{Precision}}, {{Recall}} and {{F}}-{{Score}}},
  shorttitle = {I Am {{Learning}}},
  author = {Shams, Rushdi},
  howpublished = {http://rushdishams.blogspot.com/2011/08/micro-and-macro-average-of-precision.html},
  keywords = {\#nosource,macro-average f-score,macro-average precision,macro-average recall,micro-average f-score,micro-average precision,micro-average recall,Precision,Recall}
}

@article{shangAreNoisySentences2019,
  ids = {Shang2019},
  title = {Are {{Noisy Sentences Useless}} for {{Distant Supervised Relation Extraction}}?},
  author = {Shang, Yuming},
  year = {2019},
  abstract = {The noisy labeling problem has been one of the major obstacles for distant supervised relation extraction. Existing approaches usually consider that the noisy sentences are useless and will harm the model's performance. Therefore, they mainly alleviate this problem by reducing the influence of noisy sentences, such as applying bag-level selective attention or removing noisy sentences from sentence-bags. However, the underlying cause of the noisy labeling problem is not the lack of useful information, but the missing relation labels. Intuitively, if we can allocate credible labels for noisy sentences, they will be transformed into useful training data and benefit the model's performance. Thus, in this paper, we propose a novel method for distant supervised relation extraction, which employs unsupervised deep clustering to generate reliable labels for noisy sentences. Specifically, our model contains three modules: a sentence encoder, a noise detector and a label generator. The sentence encoder is used to obtain feature representations. The noise detector detects noisy sentences from sentence-bags, and the label generator produces high-confidence relation labels for noisy sentences. Extensive experimental results demonstrate that our model outperforms the state-of-the-art baselines on a popular benchmark dataset, and can indeed alleviate the noisy labeling problem.},
  annotation = {\_eprint: 1911.09788},
  archivePrefix = {arXiv},
  arxivid = {1911.09788},
  eprint = {1911.09788},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Shang - 2019 - Are Noisy Sentences Useless for Distant Supervised Relation Extraction.pdf}
}

@inproceedings{sharmaUnsupervisedApproachCauseeffect2018,
  title = {An Unsupervised Approach for Cause-Effect Relation Extraction from Biomedical Text},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Sharma, Raksha and Palshikar, Girish and Pawar, Sachin},
  year = {2018},
  month = jun,
  volume = {10859 LNCS},
  pages = {419--427},
  publisher = {{Springer Verlag}},
  doi = {10.1007/978-3-319-91947-8_43},
  abstract = {Identification of Cause-effect (CE) relation mentions, along with the arguments, are crucial for creating a scientific knowledge-base. Linguistically complex constructs are used to express CE relations in text, mainly using generic causative (causal) verbs (cause, lead, result etc). We observe that some generic verbs have a domain-specific causative sense (inhibit, express) and some domains have altogether new causative verbs (down-regulate). Not every mention of a generic causative verb (e.g., lead) indicates a CE relation mention. We propose a linguistically-oriented unsupervised iterative co-discovery approach to identify domain-specific causative verbs, starting from a small set of seed causative verbs and an unlabeled corpus. We use known causative verbs to extract CE arguments, and use known CE arguments to discover causative verbs (hence co-discovery). Since causes and effects are typically agents, events, actions, or conditions, we use WordNet hypernym categories to identify suitable CE arguments. PMI is used to measure linguistic associations between a causative verb and its argument. Once we have a list of domain-specific causative verbs, we use it to extract CE relation mentions from a given corpus in an unsupervised manner, filtering out non-causative use of a causative verb using WordNet hypernym check of its arguments. Our approach extracts 256 domain-specific causative verbs from 10,\^A 000 PubMed abstracts of Leukemia papers, and outperforms several baselines for extracting intra-sentence CE relation mentions.},
  isbn = {978-3-319-91946-1},
  keywords = {\#nosource,Biomedical domain,Causative verbs,Cause-effect relation,Hypernyms,Leukemia,PMI,Relation extraction}
}

@article{sharptonIntroductionAnalysisShotgun2014,
  ids = {Sharpton2014},
  title = {An Introduction to the Analysis of Shotgun Metagenomic Data},
  author = {Sharpton, Thomas J.},
  year = {2014},
  volume = {5},
  issn = {1664462X},
  doi = {10.3389/fpls.2014.00209},
  abstract = {Environmental DNA sequencing has revealed the expansive biodiversity of microorganisms and clarified the relationship between host-associated microbial communities and host phenotype. Shotgun metagenomic DNA sequencing is a relatively new and powerful environmental sequencing approach that provides insight into community biodiversity and function. But, the analysis of metagenomic sequences is complicated due to the complex structure of the data. Fortunately, new tools and data resources have been developed to circumvent these complexities and allow researchers to determine which microbes are present in the community and what they might be doing. This review describes the analytical strategies and specific tools that can be applied to metagenomic data and the considerations and caveats associated with their use. Specifically, it documents how metagenomes can be analyzed to quantify community structure and diversity, assemble novel genomes, identify new taxa and genes, and determine which metabolic pathways are encoded in the community. It also discusses several methods that can be used compare metagenomes to identify taxa and functions that differentiate communities.},
  file = {/home/harrisonpl/Documents/PDFs/Sharpton - 2014 - An introduction to the analysis of shotgun metagenomic data.pdf},
  journal = {Frontiers in Plant Science},
  keywords = {Bioinformatics,Host-microbe interactions,Metagenome,Microbial diversity,Microbiome,Microbiota,Review},
  number = {JUN},
  pmid = {24982662}
}

@article{shearerHermiTHighlyeficientOWL2009,
  title = {{{HermiT}}: {{A}} Highly-Eficient {{OWL}} Reasoner},
  author = {Shearer, Rob and Motik, Boris and Horrocks, Ian},
  year = {2009},
  volume = {432},
  pages = {10},
  issn = {16130073},
  abstract = {HermiT is a new OWL reasoner based on a novel "hypertableau" calculus. The new calculus addresses performance problems due to nondeterminism and model size|the primary sources of complexity in state-of-the-art OWL reasoners. The latter is particularly important in practice, and it is achieved in HermiT with an improved blocking strategy and and an optimization that tries to reuse existing individuals rather than generating new ones. HermiT also incorporates a number of other novel optimizations, such as a more eficient approach to handling nominals, and various techniques for optimizing ontology classification. Our tests show that HermiT is usually much faster than other reasoners when classifying complex ontologies, and it is already able to classify a number of ontologies which no other reasoner has been able to handle.},
  journal = {CEUR Workshop Proceedings},
  keywords = {\#nosource,HermiT,ontology,owl,reasoner}
}

@article{shenMetaanalysisPathwayEnrichment2010,
  ids = {Shen2010},
  title = {Meta-Analysis for Pathway Enrichment Analysis When Combining Multiple Genomic Studies},
  author = {Shen, Kui and Tseng, George C.},
  year = {2010},
  month = may,
  volume = {26},
  pages = {1316--1323},
  issn = {13674803},
  doi = {10.1093/bioinformatics/btq148},
  abstract = {Motivation: Many pathway analysis (or gene set enrichment analysis) methods have been developed to identify enriched pathways under different biological states within a genomic study. As more and more microarray datasets accumulate, meta-analysis methods have also been developed to integrate information among multiple studies. Currently, most meta-analysis methods for combining genomic studies focus on biomarker detection and meta-analysis for pathway analysis has not been systematically pursued. Results: We investigated two approaches of meta-analysis for pathway enrichment (MAPE) by combining statistical significance across studies at the gene level (MAPE\_G) or at the pathway level (MAPE\_P). Simulation results showed increased statistical power of meta-analysis approaches compared to a single study analysis and showed complementary advantages of MAPE\_G and MAPE\_P under different scenarios. We also developed an integrated method (MAPE\_I) that incorporates advantages of both approaches. Comprehensive simulations and applications to real data on drug response of breast cancer cell lines and lung cancer tissues were evaluated to compare the performance of three MAPE variations. MAPE\_P has the advantage of not requiring gene matching across studies. When MAPE\_G and MAPE\_P show complementary advantages, the hybrid version of MAPE\_I is generally recommended. Availability: http://www.biostat.pitt.edu/bioinfo/. Contact: ctseng@pitt.edu. Supplementary information: Supplementary data are available at Bioinformatics online. \textcopyright{} The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org.},
  file = {/home/harrisonpl/Documents/PDFs/Shen, Tseng - 2010 - Meta-analysis for pathway enrichment analysis when combining multiple genomic.pdf},
  journal = {Bioinformatics},
  language = {en},
  number = {10}
}

@incollection{shiBriefSurveyRelation2019a,
  ids = {shiBriefSurveyRelation2019},
  title = {A {{Brief Survey}} of {{Relation Extraction Based}} on {{Distant Supervision}}},
  booktitle = {Computational {{Science}} \textendash{} {{ICCS}} 2019},
  author = {Shi, Yong and Xiao, Yang and Niu, Lingfeng},
  editor = {Rodrigues, Jo{\~a}o M. F. and Cardoso, Pedro J. S. and Monteiro, J{\^a}nio and Lam, Roberto and Krzhizhanovskaya, Valeria V. and Lees, Michael H. and Dongarra, Jack J. and Sloot, Peter M.A.},
  year = {2019},
  volume = {11538},
  pages = {293--303},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-22744-9_23},
  abstract = {As a core task and important part of Information Extraction\dbend Entity Relation Extraction can realize the identification of the semantic relation between entity pairs. And it plays an important role in semantic understanding of sentences and the construction of entity knowledge base. It has the potential of employing distant supervision method, end-to-end model and other deep learning model with the creation of large datasets. In this review, we compare the contributions and defect of the various models that have been used for the task, to help guide the path ahead.},
  file = {/home/harrisonpl/Documents/PDFs/Shi et al. - 2019 - A Brief Survey of Relation Extraction Based on Distant Supervision.pdf},
  isbn = {978-3-030-22743-2 978-3-030-22744-9},
  keywords = {\#nosource,Deep learning,Distant supervision,Relation extraction},
  language = {en}
}

@article{shmelkovHistoreceptomicFingerprintsDruglike2015,
  ids = {Shmelkov2015},
  title = {Historeceptomic Fingerprints for Drug-like Compounds},
  author = {Shmelkov, Evgeny and Grigoryan, Arsen and Swetnam, James and Xin, Junyang and Tivon, Doreen and Shmelkov, Sergey V. and Cardozo, Timothy},
  year = {2015},
  volume = {6},
  issn = {1664042X},
  doi = {10.3389/fphys.2015.00371},
  abstract = {Most drugs exert their beneficial and adverse effects through their combined action on several different molecular targets (polypharmacology). The true molecular fingerprint of the direct action of a drug has two components: the ensemble of all the receptors upon which a drug acts and their level of expression in organs/tissues. Conversely, the fingerprint of the adverse effects of a drug may derive from its action in bystander tissues. The ensemble of targets is almost always only partially known. Here we describe an approach improving upon and integrating both components: in silico identification of a more comprehensive ensemble of targets for any drug weighted by the expression of those receptors in relevant tissues. Our system combines more than 300,000 experimentally determined bioactivity values from the ChEMBL database and 4.2 billion molecular docking scores. We integrated these scores with gene expression data for human receptors across a panel of human tissues to produce drug-specific tissue-receptor (historeceptomics) scores. A statistical model was designed to identify significant scores, which define an improved fingerprint representing the unique activity of any drug. These multi-dimensional historeceptomic fingerprints describe, in a novel, intuitive, and easy to interpret style, the holistic, in vivo picture of the mechanism of any drug's action. Valuable applications in drug discovery and personalized medicine, including the identification of molecular signatures for drugs with polypharmacologic modes of action, detection of tissue-specific adverse effects of drugs, matching molecular signatures of a disease to drugs, target identification for bioactive compounds with unknown receptors, and hypothesis generation for drug/compound phenotypes may be enabled by this approach. The system has been deployed at drugable.org for access through a user-friendly web site.},
  file = {/home/harrisonpl/Documents/PDFs/Shmelkov et al. - 2015 - Historeceptomic fingerprints for drug-like compounds.pdf},
  journal = {Frontiers in Physiology},
  keywords = {Drug target,Gene expression,Lawrence Hunter,Mechanism of drug action,Molecular docking simulation,Polypharmacology,Recommendation},
  mendeley-tags = {Lawrence Hunter,Recommendation},
  number = {DEC}
}

@article{silverbergCancerStatistics19701970,
  title = {Cancer Statistics, 1970},
  author = {Silverberg, Edwin and Grant, Roald N.},
  year = {1970},
  volume = {20},
  pages = {10--23},
  issn = {1542-4863},
  doi = {10.3322/canjclin.20.1.10},
  annotation = {\_eprint: https://acsjournals.onlinelibrary.wiley.com/doi/pdf/10.3322/canjclin.20.1.10},
  copyright = {Copyright \textcopyright{} 1970 American Cancer Society},
  file = {/home/harrisonpl/Documents/PDFs/Silverberg, Grant - 1970 - Cancer statistics, 1970.pdf;/home/harrisonpl/Zotero/storage/G3VTXGUA/canjclin.20.1.html},
  journal = {CA: A Cancer Journal for Clinicians},
  language = {en},
  number = {1}
}

@article{silverbergCancerStatistics19731973,
  title = {Cancer Statistics 1973},
  author = {Silverberg, Edwin and Holleb, Arthur I.},
  year = {1973},
  volume = {23},
  pages = {2--27},
  issn = {1542-4863},
  doi = {10.3322/canjclin.23.1.2},
  annotation = {\_eprint: https://acsjournals.onlinelibrary.wiley.com/doi/pdf/10.3322/canjclin.23.1.2},
  copyright = {Copyright \textcopyright{} 1973 American Cancer Society},
  file = {/home/harrisonpl/Documents/PDFs/Silverberg, Holleb - 1973 - Cancer statistics 1973.pdf;/home/harrisonpl/Zotero/storage/RN73TA7P/canjclin.23.1.html},
  journal = {CA: A Cancer Journal for Clinicians},
  language = {en},
  number = {1}
}

@article{simpson-lavyCrossTalkCarbonMetabolism2015,
  ids = {Simpson-Lavy2015},
  title = {Cross-{{Talk}} between {{Carbon Metabolism}} and the {{DNA Damage Response}} in {{S}}. Cerevisiae},
  author = {{Simpson-Lavy}, Kobi J. and Bronstein, Alex and Kupiec, Martin and Johnston, Mark},
  year = {2015},
  month = sep,
  volume = {12},
  pages = {1865--1875},
  publisher = {{Elsevier B.V.}},
  issn = {22111247},
  doi = {10.1016/j.celrep.2015.08.025},
  abstract = {Yeast cells with DNA damage avoid respiration, presumably because products of oxidative metabolism can be harmful to DNA. We show that DNA damage inhibits the activity of the Snf1 (AMP-activated) protein kinase (AMPK), which activates expression of genes required for respiration. Glucose and DNA damage upregulate SUMOylation of Snf1, catalyzed by the SUMO E3 ligase Mms21, which inhibits SNF1 activity. The DNA damage checkpoint kinases Mec1/ATR and Tel1/ATM, as well as the nutrient-sensing protein kinase A (PKA), regulate Mms21 activity toward Snf1. Mec1 and Tel1 are required for two SNF1-regulated processes-glucose sensing and ADH2 gene expression-even without exogenous genotoxic stress. Our results imply that inhibition of Snf1 by SUMOylation is a mechanism by which cells lower their respiration in response to DNA damage. This raises the possibility that activation of DNA damage checkpoint mechanisms could contribute to aerobic fermentation (Warburg effect), a hallmark of cancer cells.},
  file = {/home/harrisonpl/Documents/PDFs/Simpson-Lavy et al. - 2015 - Cross-Talk between Carbon Metabolism and the DNA Damage Response in S.pdf},
  journal = {Cell Reports},
  number = {11}
}

@article{simpsonDetectingNovelEmerging2018,
  title = {Detecting Novel and Emerging Drug Terms Using Natural Language Processing:A Social Media Corpus Study},
  author = {Simpson, Sean S. and Adams, Nikki and Brugman, Claudia M. and Conners, Thomas J.},
  year = {2018},
  month = jan,
  volume = {20},
  issn = {14388871},
  doi = {10.2196/publichealth.7726},
  abstract = {Background: With the rapid development of new psychoactive substances (NPS) and changes in the use of more traditional drugs, it is increasingly difficult for researchers and public health practitioners to keep up with emerging drugs and drug terms. Substance use surveys and diagnostic tools need to be able to ask about substances using the terms that drug users themselves are likely to be using. Analyses of social media may offer new ways for researchers to uncover and track changes in drug terms in near real time. This study describes the initial results from an innovative collaboration between substance use epidemiologists and linguistic scientists employing techniques from the field of natural language processing to examine drug-related terms in a sample of tweets from the United States. Objective: The objective of this study was to assess the feasibility of using distributed word-vector embeddings trained on social media data to uncover previously unknown (to researchers) drug terms. Methods: In this pilot study, we trained a continuous bag of words (CBOW) model of distributed word-vector embeddings on a Twitter dataset collected during July 2016 (roughly 884.2 million tokens). We queried the trained word embeddings for terms with high cosine similarity (a proxy for semantic relatedness) to well-known slang terms for marijuana to produce a list of candidate terms likely to function as slang terms for this substance. This candidate list was then compared with an expert-generated list of marijuana terms to assess the accuracy and efficacy of using word-vector embeddings to search for novel drug terminology. Results: The method described here produced a list of 200 candidate terms for the target substance (marijuana). Of these 200 candidates, 115 were determined to in fact relate to marijuana (65 terms for the substance itself, 50 terms related to paraphernalia). This included 30 terms which were used to refer to the target substance in the corpus yet did not appear on the expert-generated list and were therefore considered to be successful cases of uncovering novel drug terminology. Several of these novel terms appear to have been introduced as recently as 1 or 2 months before the corpus time slice used to train the word embeddings. Conclusions: Though the precision of the method described here is low enough as to still necessitate human review of any candidate term lists generated in such a manner, the fact that this process was able to detect 30 novel terms for the target substance based only on one month's worth of Twitter data is highly promising. We see this pilot study as an important proof of concept and a first step toward producing a fully automated drug term discovery system capable of tracking emerging NPS terms in real time.},
  journal = {Journal of Medical Internet Research},
  keywords = {\#nosource,Natural language processing,Social media,Street drugs,Vocabulary},
  number = {1}
}

@book{sinclairLifespanWhyWe2019,
  title = {Lifespan: Why We Age--and Why We Don't Have To},
  shorttitle = {Lifespan},
  author = {Sinclair, David A. and LaPlante, Matthew D.},
  year = {2019},
  edition = {First Atria Books hardcover edition},
  publisher = {{Atria Books}},
  address = {{New York}},
  isbn = {978-1-5011-9197-8},
  keywords = {\#nosource,HEALTH \& FITNESS / Diseases / Genetic,Life spans (Biology),Longevity,SCIENCE / Life Sciences / Genetics \& Genomics},
  lccn = {QH528.5 .S56 2019}
}

@article{sirinPelletPracticalOWLDL2007,
  ids = {sirinPelletPracticalOWLDL2007a},
  title = {Pellet: {{A}} Practical {{OWL}}-{{DL}} Reasoner},
  shorttitle = {Pellet},
  author = {Sirin, Evren and Parsia, Bijan and Grau, Bernardo Cuenca and Kalyanpur, Aditya and Katz, Yarden},
  year = {2007},
  volume = {5},
  pages = {51--53},
  issn = {15708268},
  doi = {10.1016/j.websem.2007.03.004},
  abstract = {In this paper, we present a brief overview of Pellet: a complete OWL-DL reasoner with acceptable to very good performance, extensive middleware, and a number of unique features. Pellet is the first sound and complete OWL-DL reasoner with extensive support for reasoning with individuals (including nominal support and conjunctive query), user-defined datatypes, and debugging support for ontologies. It implements several extensions to OWL-DL including a combination formalism for OWL-DL ontologies, a non-monotonic operator, and preliminary support for OWL/Rule hybrid reasoning. Pellet is written in Java and is open source. \textcopyright{} 2007.},
  file = {/home/harrisonpl/Documents/PDFs/Sirin et al. - 2007 - Pellet.pdf;/home/harrisonpl/Zotero/storage/RYC7RTYK/S1570826807000169.html},
  journal = {Web Semantics},
  keywords = {\#nosource,Description logics,ontology,Tableau Theorem Proving,Web Ontology Language},
  number = {2},
  series = {Software {{Engineering}} and the {{Semantic Web}}}
}

@article{skorstadAbstractionProcessesConcept1988,
  ids = {skorstadABSTRACTIONPROCESSESCONCEPT},
  title = {Abstraction {{Processes During Concept Learning}}: {{A Structural View}}},
  shorttitle = {Abstraction Processes during Concept Learning},
  author = {Skorstad, Janice and Gentner, Dedre and Medin, Doug},
  year = {1988},
  month = jan,
  pages = {419--425},
  file = {/home/harrisonpl/Documents/PDFs/Skorstad et al. - 1988 - Abstraction Processes During Concept Learning.pdf},
  journal = {Proceedings of the Tenth Annual Conference of the Cognitive Science Society},
  keywords = {\#nosource,HPL comprehensive exam}
}

@article{smalheiserUsingARROWSMITHComputerassisted1998,
  title = {Using {{ARROWSMITH}}: A Computer-Assisted Approach to Formulating and Assessing Scientific Hypotheses},
  shorttitle = {Using {{ARROWSMITH}}},
  author = {Smalheiser, Neil R and Swanson, Don R},
  year = {1998},
  month = nov,
  volume = {57},
  pages = {149--153},
  issn = {0169-2607},
  doi = {10.1016/S0169-2607(98)00033-9},
  abstract = {Conventional computer searches of the biomedical literature (e.g. MEDLINE) allow investigators to retrieve much of the information that has already been published on a given topic. However, these searches are of limited utility at the frontier of scientific discovery, when one wishes to identify and assess new, untested scientific hypotheses, or to uncover biologically significant relations between two previously disparate fields of inquiry. We have designed a set of interactive software and database search strategies, collectively called ARROWSMITH, that facilitate the discovery of plausible hypotheses linking findings across specialties (Artif. Intell. 91 (1997) 183\textendash 203). In the simplest implementation of ARROWSMITH, the user begins with an experimental finding or hypothesis that two items A and C are related in some way. The titles of papers indexed in MEDLINE which contain the word `A' (or synonyms) are downloaded into a file A, and similarly a file C is created. The software constructs a list of words and phrases B common to files A and C; automatic and manual editing are used to filter out uninteresting B-terms. For each B-term, the software generates an AB file of titles containing both `A' and `B', and a BC file of titles containing both `B' and `C'; these titles are juxtaposed to facilitate the user judging whether there is likely to be a biologically significant relation among A, B and C. ARROWSMITH has been employed to analyze research problems relating to oxidative stress, brain damage, Alzheimer's disease and schizophrenia. Applications of ARROWSMITH include: anticipating adverse drug reactions, identifying mechanisms by which agents modulate cellular or organismal responses, suggesting new therapeutic approaches, identifying possible risk factors for diseases, and identifying potential animal models for human conditions. A simplified experimental version of ARROWSMITH is now freely accessible on the World Wide Web (http://kiwi.uchicago.edu).},
  file = {/home/harrisonpl/Documents/PDFs/Smalheiser, Swanson - 1998 - Using ARROWSMITH.pdf;/home/harrisonpl/Zotero/storage/2E6BZ63V/S0169260798000339.html},
  journal = {Computer Methods and Programs in Biomedicine},
  keywords = {Alzheimer's disease,Drug discovery,Information retrieval,Oxidative stress,Scientific discovery},
  language = {en},
  number = {3}
}

@incollection{smithAboutnessFoundationsInformation2015,
  title = {Aboutness: {{Towards}} Foundations for the Information Artifact Ontology},
  shorttitle = {Aboutness},
  booktitle = {{{CEUR Workshop Proceedings}}},
  author = {Smith, Barry and Ceusters, Werner},
  year = {2015},
  volume = {1515},
  pages = {1--5},
  publisher = {{CEUR vol. 1515}},
  abstract = {The Information Artifact Ontology (IAO) was created to serve as a domain-neutral resource for the representation of types of information content entities (ICEs) such as documents, data-bases, and digital images. We identify a series of problems with the current version of the IAO and suggest solutions designed to advance our understanding of the relations between ICEs and associated cognitive representations in the minds of human subjects. This requires embedding IAO in a larger framework of ontologies, including most importantly the Mental Functioning Ontology (MFO). It also requires a careful treatment of the aboutness relations between ICEs and associated cognitive representations and their targets in reality.},
  keywords = {\#nosource,IAO,Identifiers,ontology,Ontology,Ontology (information science),Web Ontology Language}
}

@article{smithMammalianPhenotypeOntology2009,
  ids = {Smith2009},
  title = {The Mammalian Phenotype Ontology: {{Enabling}} Robust Annotation and Comparative Analysis},
  shorttitle = {The {{Mammalian Phenotype Ontology}}},
  author = {Smith, Cynthia L. and Eppig, Janan T.},
  year = {2009},
  volume = {1},
  pages = {390--399},
  issn = {19395094},
  doi = {10.1002/wsbm.44},
  abstract = {The mouse has long been an important model for the study of human genetic disease. Through the application of genetic engineering and mutagenesis techniques, the number of unique mutant mouse models and the amount of phenotypic data describing them are growing exponentially. Describing phenotypes of mutant mice in a computationally useful manner that will facilitate data mining is a major challenge for bioinformatics. Here we describe a tool, the Mammalian Phenotype (MP) ontology, for classifying and organizing phenotypic information related to the mouse and other mammalian species. The MP ontology has been applied to mouse phenotype descriptions in the Mouse Genome Informatics (MGI) Database (http://www.informatics.jax.org/), the Rat Genome Database (RGD, http://rgd.mcw.edu), the Online Mendelian Inheritance in Animals (OMIA, http://omia.angis.org.au/), and elsewhere. Use of this ontology allows comparisons of data from diverse sources, facilitates comparisons across mammalian species, assists in identifying appropriate experimental disease models, and aids in the discovery of candidate disease genes and molecular signaling pathways. \textcopyright{} 2009 John Wiley \& Sons, Inc.},
  file = {/home/harrisonpl/Documents/PDFs/Smith, Eppig - 2009 - The mammalian phenotype ontology.pdf},
  journal = {Wiley Interdisciplinary Reviews: Systems Biology and Medicine},
  keywords = {hunter lab,ontology},
  mendeley-tags = {hunter lab,ontology},
  number = {3},
  pmid = {20052305}
}

@article{smithRelationsBiomedicalOntologies2005,
  ids = {Smith2005},
  title = {Relations in Biomedical Ontologies.},
  author = {Smith, Barry and Ceusters, Werner and Klagges, Bert and K{\"o}hler, Jacob and Kumar, Anand and Lomax, Jane and Mungall, Chris and Neuhaus, Fabian and Rector, Alan L. and Rosse, Cornelius},
  year = {2005},
  volume = {6},
  pages = {R46},
  issn = {14656914},
  doi = {10.1186/gb-2005-6-5-r46},
  abstract = {To enhance the treatment of relations in biomedical ontologies we advance a methodology for providing consistent and unambiguous formal definitions of the relational expressions used in such ontologies in a way designed to assist developers and users in avoiding errors in coding and annotation. The resulting Relation Ontology can promote interoperability of ontologies and support new types of automated reasoning about the spatial and temporal dimensions of biological and medical phenomena.},
  file = {/home/harrisonpl/Documents/PDFs/Smith et al. - 2005 - Relations in biomedical ontologies.pdf},
  journal = {Genome biology},
  keywords = {Gene Ontology,ontology,Process Instance,Relation Ontology,Spatial Region,Unify Medical Language System},
  mendeley-tags = {ontology},
  number = {5},
  pmid = {15892874}
}

@article{sondergaardRoleCardiacRyanodine2019,
  ids = {S\o ndergaard2019,S\o ndergaard2019a},
  title = {Role of Cardiac Ryanodine Receptor Calmodulin-Binding Domains in Mediating the Action of Arrhythmogenic Calmodulin {{N}}-Domain Mutation {{N54I}}},
  author = {S{\o}ndergaard, Mads T. and Liu, Yingjie and Guo, Wenting and Wei, Jinhong and Wang, Ruiwu and Brohus, Malene and Overgaard, Michael T. and Chen, S. R.Wayne},
  year = {2019},
  month = dec,
  pages = {1--25},
  publisher = {{Blackwell Publishing Ltd}},
  issn = {17424658},
  doi = {10.1111/febs.15147},
  abstract = {The Ca2+-sensing protein calmodulin (CaM) inhibits cardiac ryanodine receptor (RyR2)-mediated Ca2+ release. CaM mutations associated with arrhythmias and sudden cardiac death have been shown to diminish CaM-dependent inhibition of RyR2, but the underlying mechanisms are not well understood. Nearly all arrhythmogenic CaM mutations identified are located in the C-domain of CaM and exert marked effects on Ca2+ binding to CaM and on the CaM C-domain interaction with the CaM-binding domain 2 (CaMBD2) in RyR2. Interestingly, the arrhythmogenic N-domain mutation CaM-N54I has little or no effect on Ca2+ binding to CaM or the CaM C-domain-RyR2 CaMBD2 interaction, unlike all CaM C-domain mutations. This suggests that CaM-N54I may diminish CaM-dependent RyR2 inhibition by affecting CaM N-domain interactions with RyR2 CaMBDs other than CaMBD2. To explore this possibility, we assessed the effects of deleting each of the four known CaMBDs in RyR2 (CaMBD1a, -1b, -2, or -3) on the CaM-dependent inhibition of RyR2-mediated Ca2+ release in HEK293 cells. We found that removing CaMBD1a, CaMBD1b, or CaMBD3 did not alter the effects of CaM-N54I or CaM-WT on RyR2 inhibition. On the other hand, deleting RyR2-CaMBD2 abolished the effects of both CaM-N54I and CaM-WT. Our results support that CaM-N54I causes aberrant RyR2 regulation via an uncharacterized CaMBD or less likely CaMBD2, and that RyR2 CaMBD2 is required for the actions of both N- and C-domain CaM mutations. Moreover, our results show that CaMBD1a is central to RyR2 regulation, but CaMBD1a, CaMBD1b, and CaMBD3 are not required for CaM-dependent inhibition of RyR2 in HEK293 cells.},
  file = {/home/harrisonpl/Documents/PDFs/Søndergaard et al. - 2019 - Role of cardiac ryanodine receptor calmodulin-binding domains in mediating the.pdf},
  journal = {FEBS Journal},
  keywords = {arrhythmia,calmodulin,intracellular Ca2+ signalling,ion channel regulation,ryanodine receptor}
}

@techreport{sousaBiOntDeepLearning2020,
  ids = {Sousa2020},
  title = {{{BiOnt}}: {{Deep Learning}} Using {{Multiple Biomedical Ontologies}} for {{Relation Extraction}}},
  author = {Sousa, Diana and Couto, Francisco M.},
  year = {2020},
  abstract = {Successful biomedical relation extraction can provide evidence to researchers and clinicians about possible unknown associations between biomedical entities, advancing the current knowledge we have about those entities and their inherent mechanisms. Most biomedical relation extraction systems do not resort to external sources of knowledge, such as domain-specific ontologies. However, using deep learning methods, along with biomedical ontologies, has been recently shown to effectively advance the biomedical relation extraction field. To perform relation extraction, our deep learning system, BiOnt, employs four types of biomedical ontologies, namely, the Gene Ontology, the Human Phenotype Ontology, the Human Disease Ontology, and the Chemical Entities of Biological Interest, regarding gene-products, phenotypes, diseases, and chemical compounds, respectively. We tested our system with three data sets that represent three different types of relations of biomedical entities. BiOnt achieved, in F-score, an improvement of 4.93 percentage points for drug-drug interactions (DDI corpus), 4.99 percentage points for phenotype-gene relations (PGR corpus), and 2.21 percentage points for chemical-induced disease relations (BC5CDR corpus), relatively to the state-of-the-art. The code supporting this system is available at https://github.com/lasigeBioTM/BiONT.},
  annotation = {\_eprint: 2001.07139},
  archivePrefix = {arXiv},
  arxivid = {2001.07139},
  eprint = {2001.07139},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Sousa, Couto - 2020 - BiOnt.pdf}
}

@article{sousaUsingNeuralNetworks2019,
  ids = {Sousa2019},
  title = {Using {{Neural Networks}} for {{Relation Extraction}} from {{Biomedical Literature}}},
  author = {Sousa, Diana and Lamurias, Andre and Couto, Francisco M.},
  year = {2019},
  abstract = {Using different sources of information to support automated extracting of relations between biomedical concepts contributes to the development of our understanding of biological systems. The primary comprehensive source of these relations is biomedical literature. Several relation extraction approaches have been proposed to identify relations between concepts in biomedical literature, namely using neural networks algorithms. The use of multichannel architectures composed of multiple data representations, as in deep neural networks, is leading to state-of-the-art results. The right combination of data representations can eventually lead us to even higher evaluation scores in relation extraction tasks. Thus, biomedical ontologies play a fundamental role by providing semantic and ancestry information about an entity. The incorporation of biomedical ontologies has already been proved to enhance previous state-of-the-art results.},
  annotation = {\_eprint: 1905.11391},
  archivePrefix = {arXiv},
  arxivid = {1905.11391},
  eprint = {1905.11391},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Sousa et al. - 2019 - Using Neural Networks for Relation Extraction from Biomedical Literature.pdf},
  keywords = {biomedical literature,deep learning,external sources of knowledge,neural net-,ontologies,relation extraction,works}
}

@inproceedings{spanglerAutomatedHypothesisGeneration2014,
  title = {Automated Hypothesis Generation Based on Mining Scientific Literature},
  booktitle = {Proceedings of the {{ACM SIGKDD}} International Conference on Knowledge Discovery and Data Mining},
  author = {Spangler, Scott and Wilkins, Angela D. and Bachman, Benjamin J. and Nagarajan, Meena and Dayaram, Tajhal and Haas, Peter and Regenbogen, Sam and Pickering, Curtis R. and Comer, Austin and Myers, Jeffrey N. and Stanoi, Ioana and Kato, Linda and Lelescu, Ana and Labrie, Jacques J. and Parikh, Neha and Lisewski, Andreas Martin and Donehower, Lawrence and Chen, Ying and Lichtarge, Olivier},
  year = {2014},
  month = aug,
  pages = {1877--1886},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, New York, USA}},
  doi = {10.1145/2623330.2623667},
  abstract = {Keeping up with the ever-expanding flow of data and publications is untenable and poses a fundamental bottleneck to scientific progress. Current search technologies typically find many relevant documents, but they do not extract and organize the information content of these documents or suggest new scientific hypotheses based on this organized content. We present an initial case study on KnIT, a prototype system that mines the information contained in the scientific literature, represents it explicitly in a queriable network, and then further reasons upon these data to generate novel and experimentally testable hypotheses. KnIT combines entity detection with neighbor-text feature analysis and with graph-based diffusion of information to identify potential new properties of entities that are strongly implied by existing relationships. We discuss a successful application of our approach that mines the published literature to identify new protein kinases that phosphorylate the protein tumor suppressor p53. Retrospective analysis demonstrates the accuracy of this approach and ongoing laboratory experiments suggest that kinases identified by our system may indeed phosphorylate p53. These results establish proof of principle for automated hypothesis generation and discovery based on text mining of the scientific literature. \textcopyright{} 2014 ACM.},
  file = {/home/harrisonpl/Documents/PDFs/Spangler et al. - 2014 - Automated hypothesis generation based on mining scientific literature.pdf},
  isbn = {978-1-4503-2956-9},
  keywords = {comp-exam,hypothesis generation,scientific discovery,text mining},
  mendeley-tags = {comp-exam,hypothesis generation,scientific discovery,text mining},
  series = {{{KDD}} '14}
}

@article{sparkesRobotScientistsAutonomous2010,
  ids = {Sparkes2010,Sparkes2010a},
  title = {Towards {{Robot Scientists}} for Autonomous Scientific Discovery},
  booktitle = {Automated Experimentation},
  author = {Sparkes, Andrew and Aubrey, Wayne and Byrne, Emma and Clare, Amanda and Khan, Muhammed N. and Liakata, Maria and Markham, Magdalena and Rowland, Jem and Soldatova, Larisa N. and Whelan, Kenneth E. and Young, Michael and King, Ross D.},
  year = {2010},
  month = jan,
  volume = {2},
  pages = {1},
  publisher = {{BioMed Central}},
  issn = {17594499},
  doi = {10.1186/1759-4499-2-1},
  abstract = {We review the main components of autonomous scientific discovery, and how they lead to the concept of a Robot Scientist. This is a system which uses techniques from artificial intelligence to automate all aspects of the scientific discovery process: it generates hypotheses from a computer model of the domain, designs experiments to test these hypotheses, runs the physical experiments using robotic systems, analyses and interprets the resulting data, and repeats the cycle. We describe our two prototype Robot Scientists: Adam and Eve. Adam has recently proven the potential of such systems by identifying twelve genes responsible for catalysing specific reactions in the metabolic pathways of the yeast Saccharomyces cerevisiae. This work has been formally recorded in great detail using logic. We argue that the reporting of science needs to become fully formalised and that Robot Scientists can help achieve this. This will make scientific information more reproducible and reusable, and promote the integration of computers in scientific reasoning. We believe the greater automation of both the physical and intellectual aspects of scientific investigations to be essential to the future of science. Greater automation improves the accuracy and reliability of experiments, increases the pace of discovery and, in common with conventional laboratory automation, removes tedious and repetitive tasks from the human scientist. \textcopyright{} 2010 Sparkes et al; licensee BioMed Central Ltd.},
  file = {/home/harrisonpl/Documents/PDFs/Sparkes et al. - 2010 - Towards Robot Scientists for autonomous scientific discovery.pdf},
  journal = {Automated Experimentation},
  keywords = {Computational Biology/Bioinformatics,Computer Appl. in Life Sciences,Laboratory Medicine},
  number = {1}
}

@article{stancuStatinsMechanismAction2001,
  ids = {Stancu2001},
  title = {Statins: {{Mechanism}} of Action and Effects},
  shorttitle = {Statins},
  author = {Stancu, Cam{\'e}lia and Sima, Anca},
  year = {2001},
  volume = {5},
  pages = {378--387},
  issn = {15821838},
  doi = {10.1111/j.1582-4934.2001.tb00172.x},
  abstract = {The beneficial effects of statins are the result of their capacity to reduce cholesterol biosyntesis, mainly in the liver, where they are selectively distributed, as well as to the modulation of lipid metabolism, derived from their effect of inhibition upon HMG-CoA reductase. Statins have antiatherosclerotic effects, that positively correlate with the percent decrease in LDL cholesterol. In addition, they can exert antiatherosclerotic effects independently of their hypolipidemic action. Because the mevalonate metabolism generates a series of isoprenoids vital for different cellular functions, from cholesterol synthesis to the control of cell growth and differentiation, HMG-CoA reductase inhibition has beneficial pleiotropic effects. Consequently, statins reduce significantly the incidence of coronary events, both in primary and secondary prevention, being the most efficient hypolipidemic compounds that have reduced the rate of mortality in coronary patients. Independent of their hypolipidemic properties, statins interfere with events involved in bone formation and impede tumor cell growth.},
  file = {/home/harrisonpl/Documents/PDFs/Stancu, Sima - 2001 - Statins.pdf},
  journal = {Journal of Cellular and Molecular Medicine},
  keywords = {Atherosclerosis,Cancer osteoporosis,Cell signaling,Endothelial dysfunction,Hmg coa reductase,Ldl oxidation,Macrophage,mechanism,Smooth muscle cell},
  mendeley-tags = {mechanism},
  number = {4},
  pmid = {12067471}
}

@article{stebbingCOVID19CombiningAntiviral2020,
  title = {{{COVID}}-19: Combining Antiviral and Anti-Inflammatory Treatments},
  shorttitle = {{{COVID}}-19},
  author = {Stebbing, Justin and Phelan, Anne and Griffin, Ivan and Tucker, Catherine and Oechsle, Olly and Smith, Dan and Richardson, Peter},
  year = {2020},
  month = feb,
  volume = {0},
  publisher = {{Elsevier}},
  issn = {1473-3099, 1474-4457},
  doi = {10.1016/S1473-3099(20)30132-8},
  abstract = {Both coronavirus disease 2019 (COVID-19) and severe acute respiratory syndrome (SARS) are characterised by an overexuberant inflammatory response and, for SARS, viral load is not correlated with the worsening of symptoms.1,2 In our previous Correspondence to The Lancet,3 we described how BenevolentAI's proprietary artificial intelligence (AI)-derived knowledge graph,4 queried by a suite of algorithms, enabled identification of a target and a potential therapeutic against SARS coronavirus 2 (SARS-CoV-2; the causative organism in COVID-19).},
  annotation = {ZSCC: 0000001},
  file = {/home/harrisonpl/Documents/PDFs/Stebbing et al. - 2020 - COVID-19.pdf;/home/harrisonpl/Zotero/storage/CUN5FA4J/fulltext.html},
  journal = {The Lancet Infectious Diseases},
  language = {English},
  number = {0},
  pmid = {32113509; http://web.archive.org/web/20200318211713/https://www.thelancet.com/journals/laninf/article/PIIS1473-3099(20)30132-8/fulltext}
}

@inproceedings{stenetorpBratWebbasedTool2012,
  title = {Brat: A {{Web}}-Based {{Tool}} for {{NLP}}-{{Assisted Text Annotation}}},
  booktitle = {{{EACL}} 2012 - {{Proceedings}} of the {{Demonstrations}} at the 13th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Stenetorp, Pontus and Pyysalo, Sampo and Topic, Goran and Ohta, Tomoko and Ananiadou, Sophia and Tsujii, Jun'ichi},
  year = {2012},
  pages = {6},
  abstract = {We introduce the brat rapid annotation tool (BRAT), an intuitive web-based tool for text annotation supported by Natural Language Processing (NLP) technology. BRAT has been developed for rich structured annotation for a variety of NLP tasks and aims to support manual curation efforts and increase annotator productivity using NLP techniques. We discuss several case studies of real-world annotation projects using pre-release versions of BRAT and present an evaluation of annotation assisted by semantic class disambiguation on a multicategory entity mention annotation task, showing a 15\% decrease in total annotation time. BRAT is available under an opensource license from: http://brat.nlplab.org.},
  isbn = {978-1-937284-19-0},
  keywords = {\#nosource,Annotation}
}

@article{strobeltGuidelinesEffectiveUsage2016,
  title = {Guidelines for {{Effective Usage}} of {{Text Highlighting Techniques}}},
  author = {Strobelt, Hendrik and Oelke, Daniela and Kwon, Bum Chul and Schreck, Tobias and Pfister, Hanspeter},
  year = {2016},
  volume = {22},
  pages = {489--498},
  issn = {10772626},
  doi = {10.1109/TVCG.2015.2467759},
  abstract = {Semi-automatic text analysis involves manual inspection of text. Often, different text annotations (like part-of-speech or named entities) are indicated by using distinctive text highlighting techniques. In typesetting there exist well-known formatting conventions, such as bold typeface, italics, or background coloring, that are useful for highlighting certain parts of a given text. Also, many advanced techniques for visualization and highlighting of text exist; yet, standard typesetting is common, and the effects of standard typesetting on the perception of text are not fully understood. As such, we surveyed and tested the effectiveness of common text highlighting techniques, both individually and in combination, to discover how to maximize pop-out effects while minimizing visual interference between techniques. To validate our findings, we conducted a series of crowdsourced experiments to determine: i) a ranking of nine commonly-used text highlighting techniques; ii) the degree of visual interference between pairs of text highlighting techniques; iii) the effectiveness of techniques for visual conjunctive search. Our results show that increasing font size works best as a single highlighting technique, and that there are significant visual interferences between some pairs of highlighting techniques. We discuss the pros and cons of different combinations as a design guideline to choose text highlighting techniques for text viewers.},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  keywords = {\#nosource,Color,Data visualization,hunter lab,Image color analysis,Interference,Natural language processing,Text analysis,Visualization},
  number = {1}
}

@article{styczynskiBLOSUM62MiscalculationsImprove2008,
  ids = {Styczynski2008,Styczynski2008b},
  title = {{{BLOSUM62}} Miscalculations Improve Search Performance},
  author = {Styczynski, Mark P. and Jensen, Kyle L. and Rigoutsos, Isidore and Stephanopoulos, Gregory},
  year = {2008},
  volume = {26},
  pages = {274--275},
  issn = {10870156},
  doi = {10.1038/nbt0308-274},
  abstract = {Nature Biotechnology journal featuring biotechnology articles and science research papers of commercial interest in pharmaceutical, medical, and environmental sciences.},
  file = {/home/harrisonpl/Documents/PDFs/Styczynski et al. - 2008 - BLOSUM62 miscalculations improve search performance.pdf},
  journal = {Nature Biotechnology},
  number = {3}
}

@article{subramanianExtractiveAbstractiveNeural2019,
  ids = {Subramanian2019},
  title = {On {{Extractive}} and {{Abstractive Neural Document Summarization}} with {{Transformer Language Models}}},
  author = {Subramanian, Sandeep and Li, Raymond and Pilault, Jonathan and Pal, Christopher},
  year = {2019},
  abstract = {We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We show that this extractive step significantly improves summarization results. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher rouge scores. Note: The abstract above was not written by the authors, it was generated by one of the models presented in this paper.},
  annotation = {\_eprint: 1909.03186},
  archivePrefix = {arXiv},
  arxivid = {1909.03186},
  eprint = {1909.03186},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Subramanian et al. - 2019 - On Extractive and Abstractive Neural Document Summarization with Transformer.pdf}
}

@article{subramanianGeneSetEnrichment2005,
  ids = {Subramanian2005},
  title = {Gene Set Enrichment Analysis: {{A}} Knowledge-Based Approach for Interpreting Genome-Wide Expression Profiles},
  shorttitle = {Gene Set Enrichment Analysis},
  author = {Subramanian, Aravind and Tamayo, Pablo and Mootha, Vamsi K. and Mukherjee, Sayan and Ebert, Benjamin L. and Gillette, Michael A. and Paulovich, Amanda and Pomeroy, Scott L. and Golub, Todd R. and Lander, Eric S. and Mesirov, Jill P.},
  year = {2005},
  volume = {102},
  pages = {15545--15550},
  issn = {00278424},
  doi = {10.1073/pnas.0506580102},
  abstract = {Although genomewide RNA expression analysis has become a routine tool in biomedical research, extracting biological insight from such information remains a major challenge. Here, we describe a powerful analytical method called Gene Set Enrichment Analysis (GSEA) for interpreting gene expression data. The method derives its power by focusing on gene sets, that is, groups of genes that share common biological function, chromosomal location, or regulation. We demonstrate how GSEA yields insights into several cancer-related data sets, including leukemia and lung cancer. Notably, where single-gene analysis finds little similarity between two independent studies of patient survival in lung cancer, GSEA reveals many biological pathways in common. The GSEA method is embodied in a freely available software package, together with an initial database of 1,325 biologically defined gene sets. \textcopyright{} 2005 by The National Academy of Sciences of the USA.},
  file = {/home/harrisonpl/Documents/PDFs/Subramanian et al. - 2005 - Gene set enrichment analysis.pdf},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  keywords = {Microarray},
  number = {43},
  pmid = {16199517}
}

@article{sucharovMyocardialMicroRNAsAssociated2017,
  ids = {Sucharov2017,Sucharov2017b},
  title = {Myocardial {{microRNAs}} Associated with Reverse Remodeling in Human Heart Failure},
  author = {Sucharov, Carmen C. and Kao, David P. and Port, J. David and {Karimpour-Fard}, Anis and Quaife, Robert A. and Minobe, Wayne and Nunley, Karin and Lowes, Brian D. and Gilbert, Edward M. and Bristow, Michael R.},
  year = {2017},
  month = jan,
  volume = {2},
  pages = {e89169},
  publisher = {{American Society for Clinical Investigation}},
  issn = {2379-3708},
  doi = {10.1172/jci.insight.89169},
  abstract = {BACKGROUND In dilated cardiomyopathies (DCMs) changes in expression of protein-coding genes are associated with reverse remodeling, and these changes can be regulated by microRNAs (miRs). We tested the general hypothesis that dynamic changes in myocardial miR expression are predictive of {$\beta$}-blocker-associated reverse remodeling. METHODS Forty-three idiopathic DCM patients (mean left ventricular ejection fraction 0.24 {$\pm$} 0.09) were treated with {$\beta$}-blockers. Serial ventriculography and endomyocardial biopsies were performed at baseline, and after 3 and 12 months of treatment. Changes in RT-PCR (candidate miRs) or array-measured miRs were compared based on the presence (R) or absence (NR) of a reverse-remodeling response, and a miR-mRNA-function pathway analysis (PA) was performed. RESULTS At 3 months, 2 candidate miRs were selectively changed in Rs, decreases in miR-208a-3p and miR-591. PA revealed changes in miR-mRNA interactions predictive of decreased apoptosis and myocardial cell death. At 12 months, 5 miRs exhibited selective changes in Rs (decreases in miR-208a-3p, -208b-3p, 21-5p, and 199a-5p; increase in miR-1-3p). PA predicted decreases in apoptosis, cardiac myocyte cell death, hypertrophy, and heart failure, with increases in contractile and overall cardiac functions. CONCLUSIONS In DCMs, myocardial miRs predict the time-dependent reverse-remodeling response to {$\beta$}-blocker treatment, and likely regulate the expression of remodeling-associated miRs. TRIAL REGISTRATION ClinicalTrials.gov NCT01798992. FUNDING NIH 2R01 HL48013, 1R01 HL71118 (Bristow, PI); sponsored research agreements from Glaxo-SmithKline and AstraZeneca (Bristow, PI); NIH P20 HL101435 (Lowes, Port multi-PD/PI); sponsored research agreement from Miragen Therapeutics (Port, PI).},
  file = {/home/harrisonpl/Documents/PDFs/Sucharov et al. - 2017 - Myocardial microRNAs associated with reverse remodeling in human heart failure.pdf},
  journal = {JCI Insight},
  number = {2},
  pmid = {28138556}
}

@article{suhComprehensiveGeneExpression2019,
  ids = {Suh2019,Suh2019a},
  title = {Comprehensive Gene Expression Analysis for Exploring the Association between Glucose Metabolism and Differentiation of Thyroid Cancer},
  author = {Suh, Hoon Young and Choi, Hongyoon and Paeng, Jin Chul and Cheon, Gi Jeong and Chung, June Key and Kang, Keon Wook},
  year = {2019},
  month = dec,
  volume = {19},
  pages = {1--9},
  publisher = {{BioMed Central Ltd.}},
  issn = {14712407},
  doi = {10.1186/s12885-019-6482-7},
  abstract = {Background: The principle of loss of iodine uptake and increased glucose metabolism according to dedifferentiation of thyroid cancer is clinically assessed by imaging. Though these biological properties are widely applied to appropriate iodine therapy, the understanding of the genomic background of this principle is still lacking. We investigated the association between glucose metabolism and differentiation in advanced thyroid cancer as well as papillary thyroid cancer (PTC). Methods: We used RNA sequencing of 505 patients with PTC obtained from the Cancer Genome Archives and microarray data of poorly-differentiated and anaplastic thyroid cancer (PDTC/ATC). The signatures of GLUT and glycolysis were estimated to assess glucose metabolic profiles. The glucose metabolic profiles were associated with tumor differentiation score (TDS) and BRAFV600E mutation status. In addition, survival analysis of glucose metabolic profiles was performed for predicting recurrence-free survival. Results: In PTC, the glycolysis signature was positively correlated with TDS, while the GLUT signature was inversely correlated with TDS. These correlations were significantly stronger in the BRAFV600E negative group than the positive group. Meanwhile, both GLUT and glycolysis signatures were negatively correlated with TDS in advanced thyroid cancer. The high glycolysis signature was significantly associated with poor prognosis in PTC in spite of high TDS. The glucose metabolic profiles are intricately associated with tumor differentiation in PTC and PDTC/ATC. Conclusions: As glycolysis was an independent prognostic marker, we suggest that the glucose metabolism features of thyroid cancer could be another biological progression marker different from differentiation and provide clinical implications for risk stratification. Trial registration: Not applicable.},
  file = {/home/harrisonpl/Documents/PDFs/Suh et al. - 2019 - Comprehensive gene expression analysis for exploring the association between.pdf},
  journal = {BMC Cancer},
  keywords = {Glucose metabolism,GLUT,Glycolysis,Thyroid cancer,Tumor differentiation},
  number = {1}
}

@article{sukthankerAnaphoraCoreferenceResolution2018,
  title = {Anaphora and Coreference Resolution: {{A}} Review},
  booktitle = {Artificial Intelligence Review},
  author = {Sukthanker, Rhea and Poria, Soujanya and Cambria, Erik and Thirunavukarasu, Ramkumar},
  year = {2018},
  abstract = {Entity resolution aims at resolving repeated references to an entity in a document and forms a core component of natural language processing (NLP) research. This field possesses immense potential to improve the performance of other NLP fields like machine translation, sentiment analysis, paraphrase detection, summarization, etc. The area of entity resolution in NLP has seen proliferation of research in two separate sub-areas namely: anaphora resolution and coreference resolution. Through this review article, we aim at clarifying the scope of these two tasks in entity resolution. We also carry out a detailed analysis of the datasets, evaluation metrics and research methods that have been adopted to tackle this NLP problem. This survey is motivated with the aim of providing the reader with a clear understanding of what constitutes this NLP problem and the issues that require attention. Keywords},
  file = {/home/harrisonpl/Documents/PDFs/Sukthanker et al. - 2018 - Anaphora and coreference resolution.pdf},
  keywords = {Anaphora Resolution,Coreference Resolution,Deep Learning,Entity Resolution,Natural Language Processing,Sentiment Analysis}
}

@article{sukthankerAnaphoraCoreferenceResolution2020,
  title = {Anaphora and Coreference Resolution: {{A}} Review},
  shorttitle = {Anaphora and Coreference Resolution},
  booktitle = {Information Fusion},
  author = {Sukthanker, Rhea and Poria, Soujanya and Cambria, Erik and Thirunavukarasu, Ramkumar},
  year = {2020},
  issn = {15662535},
  doi = {10.1016/j.inffus.2020.01.010},
  abstract = {Entity resolution aims at resolving repeated references to an entity in a document and forms a core component of natural language processing (NLP) research. This field possesses immense potential to improve the performance of other NLP fields like machine translation, sentiment analysis, paraphrase detection, summarization, etc. The area of entity resolution in NLP has seen proliferation of research in two separate sub-areas namely: anaphora resolution and coreference resolution. Through this review article, we aim at clarifying the scope of these two tasks in entity resolution. We also carry out a detailed analysis of the datasets, evaluation metrics and research methods that have been adopted to tackle this NLP problem. This survey is motivated with the aim of providing the reader with a clear understanding of what constitutes this NLP problem and the issues that require attention.},
  archivePrefix = {arXiv},
  arxivid = {1805.11824},
  eprint = {1805.11824},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Sukthanker et al. - 2020 - Anaphora and coreference resolution.pdf},
  keywords = {Natural language processing},
  mendeley-tags = {Natural language processing}
}

@article{sunJointSelfAttentionBased2019,
  ids = {Sun2019},
  title = {Joint {{Self}}-{{Attention Based Neural Networks}} for {{Semantic Relation Extraction}}},
  author = {Sun, Jun and Li, Yan and Shen, Yatian and Ding, Wenke and Shi, Xianjin and Zhang, Lei and Shen, Xiajiong and He, Jing},
  year = {2019},
  volume = {1},
  pages = {69--75},
  issn = {2637-4226},
  doi = {10.32604/jihpp.2019.06357},
  file = {/home/harrisonpl/Documents/PDFs/Sun et al. - 2019 - Joint Self-Attention Based Neural Networks for Semantic Relation Extraction.pdf},
  journal = {Journal of Information Hiding and Privacy Protection},
  keywords = {neural networks,relation extraction,self-attention},
  number = {2}
}

@article{tanenblattConceptMapperApproachNamed2010,
  title = {The {{ConceptMapper}} Approach to Named Entity Recognition},
  author = {Tanenblatt, Michael and Coden, Anni and Sominsky, Igor},
  year = {2010},
  pages = {546--551},
  abstract = {ConceptMapper is an open source tool we created for classifying mentions in an unstructured text document based on concept terminologies and yielding named entities as output. It is implemented as a UIMA1 (Unstructured Information Management Architecture (IBM, 2004)) annotator, and concepts come from standardised or proprietary terminologies. ConceptMapper can be easily configured, for instance, to use different search strategies or syntactic concepts. In this paper we will describe ConceptMapper, its configuration parameters and their trade-offs, in terms of precision and recall in identifying concepts in a collection of clinical reports written in English. ConceptMapper is available from the Apache UIMA Sandbox, using the Apache Open Source license.},
  journal = {Proceedings of the 7th International Conference on Language Resources and Evaluation, LREC 2010},
  keywords = {\#nosource,Entity recognition,hunter lab,Natural language processing}
}

@article{taoEnhancingRelationExtraction2020,
  ids = {Tao2020},
  title = {Enhancing {{Relation Extraction Using Syntactic Indicators}} and {{Sentential Contexts}}},
  author = {Tao, Qiongxing and Luo, Xiangfeng and Wang, Hao and Xu, Richard},
  year = {2020},
  pages = {1574--1580},
  doi = {10.1109/ictai.2019.00227},
  abstract = {State-of-the-art methods for relation extraction consider the sentential context by modeling the entire sentence. However, syntactic indicators, certain phrases or words like prepositions that are more informative than other words and may be beneficial for identifying semantic relations. Other approaches using fixed text triggers capture such information but ignore the lexical diversity. To leverage both syntactic indicators and sentential contexts, we propose an indicator-aware approach for relation extraction. Firstly, we extract syntactic indicators under the guidance of syntactic knowledge. Then we construct a neural network to incorporate both syntactic indicators and the entire sentences into better relation representations. By this way, the proposed model alleviates the impact of noisy information from entire sentences and breaks the limit of text triggers. Experiments on the SemEval-2010 Task 8 benchmark dataset show that our model significantly outperforms the state-of-the-art methods.},
  annotation = {\_eprint: 1912.01858},
  archivePrefix = {arXiv},
  arxivid = {1912.01858},
  eprint = {1912.01858},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Tao et al. - 2020 - Enhancing Relation Extraction Using Syntactic Indicators and Sentential Contexts.pdf}
}

@article{taroniMultiPLIERTransferLearning2019,
  title = {{{MultiPLIER}}: A Transfer Learning Framework for Transcriptomics Reveals Systemic Features of Rare Disease},
  shorttitle = {{{MultiPLIER}}},
  author = {Taroni, Jaclyn N. and Grayson, Peter C. and Hu, Qiwen and Eddy, Sean and Kretzler, Matthias and Merkel, Peter A. and Greene, Casey S.},
  year = {2019},
  month = jan,
  pages = {395947},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/395947},
  abstract = {{$<$}h3{$>$}SUMMARY{$<$}/h3{$>$} {$<$}p{$>$}Unsupervised machine learning methods provide a promising means to analyze and interpret large datasets. However, most gene expression datasets generated by individual researchers remain too small to fully benefit from these methods. In the case of rare diseases, there may be too few cases available, even when multiple studies are combined. We trained a Pathway Level Information ExtractoR (PLIER) model using on a large public data compendium comprised of multiple experiments, tissues, and biological conditions. We then transferred the model to small rare disease datasets in an approach we term MultiPLIER. Models constructed from large, diverse public data i) included features that aligned well to important biological factors; ii) were more comprehensive than those constructed from individual datasets or conditions; iii) transferred to rare disease datasets where the models describe biological processes related to disease severity more effectively than models trained on specifically those datasets.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  file = {/home/harrisonpl/Documents/PDFs/Taroni et al. - 2019 - MultiPLIER.pdf;/home/harrisonpl/Zotero/storage/UUS2U2U8/395947v2.html;/home/harrisonpl/Zotero/storage/ZEFYI9QM/395947v2.html},
  journal = {bioRxiv},
  language = {en}
}

@article{taylorAccountingUndetectedCompounds2013,
  ids = {Taylor2013},
  title = {Accounting for Undetected Compounds in Statistical Analyses of Mass Spectrometry 'omic Studies},
  author = {Taylor, Sandra L. and Leiserowitz, Gary S. and Kim, Kyoungmi},
  year = {2013},
  volume = {12},
  pages = {703--722},
  issn = {15446115},
  doi = {10.1515/sagmb-2013-0021},
  abstract = {Mass spectrometry is an important high-throughput technique for profiling small molecular compounds in biological samples and is widely used to identify potential diagnostic and prognostic compounds associated with disease. Commonly, this data generated by mass spectrometry has many missing values resulting when a compound is absent from a sample or is present but at a concentration below the detection limit. Several strategies are available for statistically analyzing data with missing values. The accelerated failure time (AFT) model assumes all missing values result from censoring below a detection limit. Under a mixture model, missing values can result from a combination of censoring and the absence of a compound. We compare power and estimation of a mixture model to an AFT model. Based on simulated data, we found the AFT model to have greater power to detect differences in means and point mass proportions between groups. However, the AFT model yielded biased estimates with the bias increasing as the proportion of observations in the point mass increased while estimates were unbiased with the mixture model except if all missing observations came from censoring. These findings suggest using the AFT model for hypothesis testing and mixture model for estimation. We demonstrated this approach through application to glycomics data of serum samples from women with ovarian cancer and matched controls.},
  file = {/home/harrisonpl/Documents/PDFs/Taylor et al. - 2013 - Accounting for undetected compounds in statistical analyses of mass.pdf},
  journal = {Statistical Applications in Genetics and Molecular Biology},
  keywords = {Accelerated failure time model,Glycomics,Mass spectrometry,Metabolomics,Missing values,Point-mass mixture},
  number = {6},
  pmid = {24246290}
}

@article{tenenbaumHowGrowMind2011,
  ids = {Tenenbaum2011},
  title = {How to Grow a Mind: {{Statistics}}, Structure, and Abstraction},
  author = {Tenenbaum, Joshua B. and Kemp, Charles and Griffiths, Thomas L. and Goodman, Noah D.},
  year = {2011},
  volume = {331},
  pages = {1279--1285},
  issn = {00368075},
  doi = {10.1126/science.1192788},
  abstract = {In coming to understand the world - in learning concepts, acquiring language, and grasping causal relations - our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?},
  file = {/home/harrisonpl/Documents/PDFs/Tenenbaum et al. - 2011 - How to grow a mind.pdf},
  journal = {Science},
  keywords = {Abstraction,Computational cognition},
  mendeley-tags = {Abstraction,Computational cognition},
  number = {6022},
  pmid = {21393536}
}

@article{thomasGeneOntologyCausal2019,
  ids = {Thomas2019,Thomas2019b},
  title = {Gene {{Ontology Causal Activity Modeling}} ({{GO}}-{{CAM}}) Moves beyond {{GO}} Annotations to Structured Descriptions of Biological Functions and Systems},
  author = {Thomas, Paul D. and Hill, David P. and Mi, Huaiyu and {Osumi-Sutherland}, David and Van Auken, Kimberly and Carbon, Seth and Balhoff, James P. and Albou, Laurent Philippe and Good, Benjamin and Gaudet, Pascale and Lewis, Suzanna E. and Mungall, Christopher J.},
  year = {2019},
  month = oct,
  volume = {51},
  pages = {1429--1433},
  publisher = {{Springer US}},
  issn = {15461718},
  doi = {10.1038/s41588-019-0500-1},
  abstract = {To increase the utility of Gene Ontology (GO) annotations for interpretation of genome-wide experimental data, we have developed GO-CAM, a structured framework for linking multiple GO annotations into an integrated model of a biological system. We expect that GO-CAM will enable new applications in pathway and network analysis, as well as improve standard GO annotations for traditional GO-based applications.},
  file = {/home/harrisonpl/Documents/PDFs/Thomas et al. - 2019 - Gene Ontology Causal Activity Modeling (GO-CAM) moves beyond GO annotations to.pdf},
  journal = {Nature Genetics},
  keywords = {comp-exam,HPL comprehensive exam},
  language = {en},
  mendeley-tags = {HPL comprehensive exam},
  number = {10}
}

@article{topjianUnixBeginningMage,
  ids = {Topjian},
  title = {Unix for the {{Beginning Mage}}},
  author = {Topjian, Joe},
  file = {/home/harrisonpl/Documents/PDFs/Topjian - Unix for the Beginning Mage.pdf}
}

@article{tripodiApplyingKnowledgedrivenMechanistic2020,
  title = {Applying Knowledge-Driven Mechanistic Inference to Toxicogenomics},
  author = {Tripodi, Ignacio J. and Callahan, Tiffany J. and Westfall, Jessica T. and Meitzer, Nayland S. and Dowell, Robin D. and Hunter, Lawrence E.},
  year = {2020},
  month = aug,
  volume = {66},
  pages = {104877},
  issn = {0887-2333},
  doi = {10.1016/j.tiv.2020.104877},
  abstract = {When considering toxic chemicals in the environment, a mechanistic, causal explanation of toxicity may be preferred over a statistical or machine learning-based prediction by itself. Elucidating a mechanism of toxicity is, however, a costly and time-consuming process that requires the participation of specialists from a variety of fields, often relying on animal models. We present an innovative mechanistic inference framework (MechSpy), which can be used as a hypothesis generation aid to narrow the scope of mechanistic toxicology analysis. MechSpy generates hypotheses of the most likely mechanisms of toxicity, by combining a semantically-interconnected knowledge representation of human biology, toxicology and biochemistry with gene expression time series on human tissue. Using vector representations of biological entities, MechSpy seeks enrichment in a manually curated list of high-level mechanisms of toxicity, represented as biochemically- and causally-linked ontology concepts. Besides predicting the canonical mechanism of toxicity for many well-studied compounds, we experimentally validated some of our predictions for other chemicals without an established mechanism of toxicity. This mechanistic inference framework is an advantageous tool for predictive toxicology, and the first of its kind to produce a mechanistic explanation for each prediction. MechSpy can be modified to include additional mechanisms of toxicity, and is generalizable to other types of mechanisms of human biology.},
  file = {/home/harrisonpl/Documents/PDFs/Tripodi et al. - 2020 - Applying knowledge-driven mechanistic inference to toxicogenomics.pdf;/home/harrisonpl/Zotero/storage/4DMPTSGE/S0887233320300576.html},
  journal = {Toxicology in Vitro},
  keywords = {Adverse outcome pathways,Artificial intelligence,Computational toxicology,Mechanistic inference,Mechanistic toxicology},
  language = {en}
}

@article{tripodiDetectingDifferentialTranscription2018,
  ids = {Tripodi2018,Tripodi2018a},
  title = {Detecting Differential Transcription Factor Activity from {{ATAC}}-{{Seq}} Data},
  author = {Tripodi, Ignacio J. and Allen, Mary A. and Dowell, Robin D.},
  year = {2018},
  volume = {23},
  pages = {1--11},
  publisher = {{MDPI AG}},
  issn = {14203049},
  doi = {10.3390/molecules23051136},
  abstract = {Transcription factors are managers of the cellular factory, and key components to many diseases. Many non-coding single nucleotide polymorphisms affect transcription factors, either by directly altering the protein or its functional activity at individual binding sites. Here we first briefly summarize high-throughput approaches to studying transcription factor activity. We then demonstrate, using published chromatin accessibility data (specifically ATAC-seq), that the genome-wide profile of TF recognition motifs relative to regions of open chromatin can determine the key transcription factor altered by a perturbation. Our method of determining which TFs are altered by a perturbation is simple, is quick to implement, and can be used when biological samples are limited. In the future, we envision that this method could be applied to determine which TFs show altered activity in response to a wide variety of drugs and diseases.},
  file = {/home/harrisonpl/Documents/PDFs/Tripodi et al. - 2018 - Detecting differential transcription factor activity from ATAC-Seq data.pdf},
  journal = {Molecules},
  keywords = {ATAC-seq,DAStk,DNase I cleavage,Motif,Open chromatin,Perturbation,RNA-seq,Transcription factor},
  number = {5},
  pmid = {29748466}
}

@article{tseytlinNOBLEFlexibleConcept2016,
  ids = {Tseytlin2016},
  title = {{{NOBLE}} - {{Flexible}} Concept Recognition for Large-Scale Biomedical Natural Language Processing},
  author = {Tseytlin, Eugene and Mitchell, Kevin and Legowski, Elizabeth and Corrigan, Julia and Chavan, Girish and Jacobson, Rebecca S.},
  year = {2016},
  volume = {17},
  pages = {32},
  issn = {14712105},
  doi = {10.1186/s12859-015-0871-y},
  abstract = {Background: Natural language processing (NLP) applications are increasingly important in biomedical data analysis, knowledge engineering, and decision support. Concept recognition is an important component task for NLP pipelines, and can be either general-purpose or domain-specific. We describe a novel, flexible, and general-purpose concept recognition component for NLP pipelines, and compare its speed and accuracy against five commonly used alternatives on both a biological and clinical corpus. NOBLE Coder implements a general algorithm for matching terms to concepts from an arbitrary vocabulary set. The system's matching options can be configured individually or in combination to yield specific system behavior for a variety of NLP tasks. The software is open source, freely available, and easily integrated into UIMA or GATE. We benchmarked speed and accuracy of the system against the CRAFT and ShARe corpora as reference standards and compared it to MMTx, MGrep, Concept Mapper, cTAKES Dictionary Lookup Annotator, and cTAKES Fast Dictionary Lookup Annotator. Results: We describe key advantages of the NOBLE Coder system and associated tools, including its greedy algorithm, configurable matching strategies, and multiple terminology input formats. These features provide unique functionality when compared with existing alternatives, including state-of-the-art systems. On two benchmarking tasks, NOBLE's performance exceeded commonly used alternatives, performing almost as well as the most advanced systems. Error analysis revealed differences in error profiles among systems. Conclusion: NOBLE Coder is comparable to other widely used concept recognition systems in terms of accuracy and speed. Advantages of NOBLE Coder include its interactive terminology builder tool, ease of configuration, and adaptability to various domains and tasks. NOBLE provides a term-to-concept matching system suitable for general concept recognition in biomedical NLP pipelines.},
  file = {/home/harrisonpl/Documents/PDFs/Tseytlin et al. - 2016 - NOBLE - Flexible concept recognition for large-scale biomedical natural.pdf},
  journal = {BMC Bioinformatics},
  keywords = {Auto-coding,Biomedical terminologies,Concept recognition,Named Entity Recognition,Natural language processing,System evaluation,Text-processing},
  number = {1}
}

@article{turneyLatentRelationMapping2008,
  ids = {Turney2008},
  title = {The Latent Relation Mapping Engine: {{Algorithm}} and Experiments},
  shorttitle = {The {{Latent Relation Mapping Engine}}},
  booktitle = {Journal of Artificial Intelligence Research},
  author = {Turney, Peter D.},
  year = {2008},
  month = dec,
  volume = {33},
  issn = {10769757},
  doi = {10.1613/jair.2693},
  abstract = {Many AI researchers and cognitive scientists have argued that analogy is the core of cognition. The most influential work on computational modeling of analogy-making is Structure Mapping Theory (SMT) and its implementation in the Structure Mapping Engine (SME). A limitation of SME is the requirement for complex hand-coded representations. We introduce the Latent Relation Mapping Engine (LRME), which combines ideas from SME and Latent Relational Analysis (LRA) in order to remove the requirement for handcoded representations. LRME builds analogical mappings between lists of words, using a large corpus of raw text to automatically discover the semantic relations among the words. We evaluate LRME on a set of twenty analogical mapping problems, ten based on scientific analogies and ten based on common metaphors. LRME achieves human-level performance on the twenty problems. We compare LRME with a variety of alternative approaches and find that they are not able to reach the same level of performance. \textcopyright{} 2008 AI Access Foundation. All rights reserved.},
  annotation = {\_eprint: 0812.4446},
  archivePrefix = {arXiv},
  arxivid = {0812.4446},
  eprint = {0812.4446},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Turney - 2008 - The latent relation mapping engine.pdf},
  journal = {Journal of Artificial Intelligence Research},
  keywords = {HPL comprehensive exam},
  mendeley-tags = {HPL comprehensive exam}
}

@article{uzuner2010I2b2VA2011,
  title = {2010 I2b2/{{VA}} Challenge on Concepts, Assertions, and Relations in Clinical Text},
  author = {Uzuner, {\"O}zlem and South, Brett R. and Shen, Shuying and DuVall, Scott L.},
  year = {2011},
  month = sep,
  volume = {18},
  pages = {552--556},
  issn = {10675027},
  doi = {10.1136/amiajnl-2011-000203},
  abstract = {The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for Clinical Records presented three tasks: a concept extraction task focused on the extraction of medical concepts from patient reports; an assertion classification task focused on assigning assertion types for medical problem concepts; and a relation classification task focused on assigning relation types that hold between medical problems, tests, and treatments. i2b2 and the VA provided an annotated reference standard corpus for the three tasks. Using this reference standard, 22 systems were developed for concept extraction, 21 for assertion classification, and 16 for relation classification. These systems showed that machine learning approaches could be augmented with rule-based systems to determine concepts, assertions, and relations. Depending on the task, the rule-based systems can either provide input for machine learning or post-process the output of machine learning. Ensembles of classifiers, information from unlabeled data, and external knowledge sources can help when the training data are inadequate.},
  file = {/home/harrisonpl/Documents/PDFs/Uzuner et al. - 2011 - 2010 i2b2-VA challenge on concepts, assertions, and relations in clinical text.pdf},
  journal = {Journal of the American Medical Informatics Association},
  language = {en},
  number = {5}
}

@article{vanderheijdenUnseenMajoritySoil2008,
  ids = {VanDerHeijden2008},
  title = {The Unseen Majority: {{Soil}} Microbes as Drivers of Plant Diversity and Productivity in Terrestrial Ecosystems},
  shorttitle = {The Unseen Majority},
  author = {Van Der Heijden, Marcel G.A. and Bardgett, Richard D. and Van Straalen, Nico M.},
  year = {2008},
  volume = {11},
  pages = {296--310},
  issn = {1461023X},
  doi = {10.1111/j.1461-0248.2007.01139.x},
  abstract = {Microbes are the unseen majority in soil and comprise a large portion of life's genetic diversity. Despite their abundance, the impact of soil microbes on ecosystem processes is still poorly understood. Here we explore the various roles that soil microbes play in terrestrial ecosystems with special emphasis on their contribution to plant productivity and diversity. Soil microbes are important regulators of plant productivity, especially in nutrient poor ecosystems where plant symbionts are responsible for the acquisition of limiting nutrients. Mycorrhizal fungi and nitrogen-fixing bacteria are responsible for c. 5-20\% (grassland and savannah) to 80\% (temperate and boreal forests) of all nitrogen, and up to 75\% of phosphorus, that is acquired by plants annually. Free-living microbes also strongly regulate plant productivity, through the mineralization of, and competition for, nutrients that sustain plant productivity. Soil microbes, including microbial pathogens, are also important regulators of plant community dynamics and plant diversity, determining plant abundance and, in some cases, facilitating invasion by exotic plants. Conservative estimates suggest that c. 20 000 plant species are completely dependent on microbial symbionts for growth and survival pointing to the importance of soil microbes as regulators of plant species richness on Earth. Overall, this review shows that soil microbes must be considered as important drivers of plant diversity and productivity in terrestrial ecosystems. \textcopyright{} 2007 Blackwell Publishing Ltd/CNRS.},
  file = {/home/harrisonpl/Documents/PDFs/Van Der Heijden et al. - 2008 - The unseen majority.pdf},
  journal = {Ecology Letters},
  keywords = {Biological diversity and ecosystem functioning,Microbial consortia,Microbial diversity,Mycorrhizal fungi,Nitrogen,Nitrogen fixation,Phosphorus,Soil},
  number = {3},
  pmid = {18047587}
}

@article{vandesompelReminiscing15Years2015,
  title = {Reminiscing {{About}} 15 {{Years}} of {{Interoperability Efforts}}},
  author = {{Van de Sompel}, Herbert and Nelson, Michael L.},
  year = {2015},
  month = nov,
  volume = {21},
  issn = {1082-9873},
  doi = {10.1045/november2015-vandesompel},
  file = {/home/harrisonpl/Documents/PDFs/Van de Sompel, Nelson - 2015 - Reminiscing About 15 Years of Interoperability Efforts.pdf},
  journal = {D-Lib Magazine},
  language = {en},
  number = {11/12}
}

@article{vanlaarhovenGaussianInteractionProfile2011,
  ids = {VanLaarhoven,VanLaarhoven2011a},
  title = {Gaussian Interaction Profile Kernels for Predicting Drug-Target Interaction},
  author = {{van Laarhoven}, Twan and Nabuurs, Sander B. and Marchiori, Elena},
  year = {2011},
  volume = {27},
  pages = {3036--3043},
  issn = {13674803},
  doi = {10.1093/bioinformatics/btr500},
  abstract = {Motivation: The in silico prediction of potential interactions between drugs and target proteins is of core importance for the identification of new drugs or novel targets for existing drugs. However, only a tiny portion of all drug-target pairs in current datasets are experimentally validated interactions. This motivates the need for developing computational methods that predict true interaction pairs with high accuracy. Results: We show that a simple machine learning method that uses the drug-target network as the only source of information is capable of predicting true interaction pairs with high accuracy. Specifically, we introduce interaction profiles of drugs (and of targets) in a network, which are binary vectors specifying the presence or absence of interaction with every target (drug) in that network. We define a kernel on these profiles, called the Gaussian Interaction Profile (GIP) kernel, and use a simple classifier, (kernel) Regularized Least Squares (RLS), for prediction drug-target interactions. We test comparatively the effectiveness of RLS with the GIP kernel on four drug-target interaction networks used in previous studies. The proposed algorithm achieves area under the precision-recall curve (AUPR) up to 92.7, significantly improving over results of state-of-theart methods. Moreover, we show that using also kernels based on chemical and genomic information further increases accuracy, with a neat improvement on small datasets. These results substantiate the relevance of the network topology (in the form of interaction profiles) as source of information for predicting drug-target interactions. \textcopyright{} The Author 2011. Published by Oxford University Press. All rights reserved.},
  file = {/home/harrisonpl/Documents/PDFs/van Laarhoven et al. - 2011 - Gaussian interaction profile kernels for predicting drug-target interaction.pdf},
  journal = {Bioinformatics},
  keywords = {Algorithms,Artificial Intelligence,Drug Delivery Systems,Drug Discovery,Genomics},
  number = {21},
  pmid = {21893517}
}

@article{vaswaniAttentionAllYou2017,
  ids = {Vaswani2017},
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = jun,
  volume = {2017-Decem},
  pages = {5999--6009},
  issn = {10495258},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  annotation = {\_eprint: 1706.03762},
  archivePrefix = {arXiv},
  arxivid = {1706.03762},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Vaswani et al. - 2017 - Attention Is All You Need.pdf},
  journal = {Advances in Neural Information Processing Systems},
  keywords = {attention,autoencoder,Favorite,neural net,openai,transformer}
}

@article{vera-ramosExampleMultimodalBiological2019,
  ids = {Vera-Ramos2019},
  title = {An Example of Multimodal Biological Knowledge Representation},
  author = {{Vera-Ramos}, Jos{\'e} Antonio and {Juanes-Cort{\'e}s}, Bel{\'e}n and {Fern{\'a}ndez-Breis}, Jesualdo Tom{\'a}s and Gaudet, Pascale and Kuiper, Martin and L{\ae}greid, Astrid and Logie, Colin and {Rold{\'a}n-Garc{\'i}a}, Mar{\'i}a del Mar and Schulz, Stefan},
  year = {2019},
  volume = {2518},
  pages = {0--3},
  issn = {16130073},
  abstract = {Biological knowledge evolves in a quick way whereas more people with other backgrounds (e.g., bioinformaticians, computer scientists, etc.) that help in biological research require detailed knowledge on biomolecular processes in order to understand the data they need to analyse. To solve this problem, in this paper we propose a multimodal knowledge representation using graphical diagrams representing biological knowledge, a natural language elucidation of the content of the graphical diagrams, linking the graphical elements to ontology instances; and a graph for visualising the ontology.},
  file = {/home/harrisonpl/Documents/PDFs/Vera-Ramos et al. - 2019 - An example of multimodal biological knowledge representation.pdf},
  journal = {CEUR Workshop Proceedings},
  keywords = {Graphical diagrams,Knowledge representation,Methodology,Ontology,OWL}
}

@techreport{vergantiDesignAgeArtificial2020,
  ids = {Verganti2020},
  title = {Design in the {{Age}} of {{Artificial Intelligence}}},
  author = {Verganti, Roberto and Vendraminelli, Luca and Iansiti, Marco},
  year = {2020},
  file = {/home/harrisonpl/Documents/PDFs/Verganti et al. - 2020 - Design in the Age of Artificial Intelligence.pdf}
}

@article{vermaFamilialHypercholesteremiaWhen2018,
  title = {Familial {{Hypercholesteremia}}: {{When Statins}}, {{Ezetimibe}} \& {{Pcsk9 Inhibitors Are Not Enough}}},
  shorttitle = {{{FAMILIAL HYPERCHOLESTEREMIA}}},
  author = {Verma, Isha and Fernandez, Antonio and Thompson, Paul},
  year = {2018},
  volume = {71},
  pages = {A2439},
  issn = {07351097},
  doi = {10.1016/s0735-1097(18)32980-2},
  abstract = {Background: Patients with familial hypercholesteremia (FH) require lipoprotein aphaeresis (LA), mipomersen or lomitapide to control their LDL-C. Proprotein convertase subtisilin kexin type 9 (PCSK9) inhibitors have reduced the use of these treatment strategies but some FH patients still require additional LDL-C reduction. Case: A 34 year old man with FH and Non-St elevation myocardial infarction was referred for lipid management. His LDL-C was 252mg/dl despite being on 80mg of atorvastatin. We subsequently started ezetimibe 10mg daily and eventually initiated evolocumab 140mg bimonthly. Decision-making: His LDL-C remained high even on atorvastatin, ezetimibe and evolocumab (Table). This LDL-C is greater than our goal of less than 70MG/DL but remains below most payers coverage criteria for LA. It posed a therapeutic conundrum and resulted in the consideration of mipomersen for further treatment. Conclusion: PCSK9 inhibitor therapy has emerged as a safe and effective therapy for most FH patients. Some FH patients exhibit suboptimal LDL-C reductions despite co-administration with statins and ezetimibe. This subgroup of FH patients might beneft from adjuvant therapy with mipomersen and lomitapide. The combination of either of these drugs with PCSK9 inhibitors has not been studied in randomized clinical trials. Case reports and case series might contribute to the medical literature in this subject and guide medical therapy, until clinical trials are devised in this patient population. [Figure Presented].},
  file = {/home/harrisonpl/Documents/PDFs/Verma et al. - 2018 - Familial Hypercholesteremia.pdf},
  journal = {Journal of the American College of Cardiology},
  keywords = {\#nosource,mechanism},
  number = {11}
}

@inproceedings{vermaLearningHierarchicalSimilarity2012,
  title = {Learning Hierarchical Similarity Metrics},
  booktitle = {2012 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Verma, Nakul and Mahajan, Dhruv and Sellamanickam, Sundararajan and Nair, Vinod},
  year = {2012},
  month = jun,
  pages = {2280--2287},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2012.6247938},
  abstract = {Categories in multi-class data are often part of an underlying semantic taxonomy. Recent work in object classification has found interesting ways to use this taxonomy structure to develop better recognition algorithms. Here we propose a novel framework to learn similarity metrics using the class taxonomy. We show that a nearest neighbor classifier using the learned metrics gets improved performance over the best discriminative methods. Moreover, by incorporating the taxonomy, our learned metrics can also help in some taxonomy specific applications. We show that the metrics can help determine the correct placement of a new category that was not part of the original taxonomy, and can provide effective classification amongst categories local to specific subtrees of the taxonomy.},
  file = {/home/harrisonpl/Documents/PDFs/Verma et al. - 2012 - Learning hierarchical similarity metrics.pdf;/home/harrisonpl/Zotero/storage/ZC5EWXIV/6247938.html},
  keywords = {Accuracy,class taxonomy,discriminative methods,hierarchical similarity metric learning,image classification,learning (artificial intelligence),Measurement,multiclass data,nearest neighbor classifier,object classification,object recognition,object recognition task,Optimization,Prototypes,recognition algorithms,semantic taxonomy,support vector machines,Support vector machines,Taxonomy,Training}
}

@inproceedings{vermaLiRMELocallyInterpretable2019,
  ids = {Verma2019},
  title = {{{LiRME}}: {{Locally}} Interpretable Ranking Model Explanation},
  booktitle = {{{SIGIR}} 2019 - {{Proceedings}} of the 42nd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Verma, Manisha and Ganguly, Debasis},
  year = {2019},
  month = apr,
  volume = {Part F1271},
  pages = {1281--1284},
  publisher = {{Association for Computing Machinery}},
  issn = {01635964},
  doi = {10.1145/nnnnnnn.nnnnnnn},
  abstract = {Information retrieval (IR) models often employ complex variations in term weights to compute an aggregated similarity score of a query-document pair. Treating IR models as black-boxes makes it difficult to understand or explain why certain documents are retrieved at top-ranks for a given query. Local explanation models have emerged as a popular means to understand individual predictions of classification models. However, there is no systematic investigation that learns to interpret IR models, which is in fact the core contribution of our work in this paper. We explore three sampling methods to train an explanation model and propose two metrics to evaluate explanations generated for an IR model. Our experiments reveal some interesting observations, namely that a) diversity in samples is important for training local explanation models, and b) the stability of a model is inversely proportional to the number of parameters used to explain the model.},
  file = {/home/harrisonpl/Documents/PDFs/Verma, Ganguly - 2019 - LiRME.pdf},
  isbn = {978-1-4503-6172-9},
  keywords = {Interpretability,Point-wise explanations,Ranking}
}

@article{verspoorCategorizationApproachAutomated2006,
  ids = {Verspoor2006},
  title = {A Categorization Approach to Automated Ontological Function Annotation},
  author = {Verspoor, Karin and Cohn, Judith and Mniszewski, Susan and Joslyn, Cliff},
  year = {2006},
  volume = {15},
  pages = {1544--1549},
  issn = {09618368},
  doi = {10.1110/ps.062184006},
  abstract = {Automated function prediction (AFP) methods increasingly use knowledge discovery algorithms to map sequence, structure, literature, and/or pathway information about proteins whose functions are unknown into functional ontologies, typically (a portion of) the Gene Ontology (GO). While there are a growing number of methods within this paradigm, the general problem of assessing the accuracy of such prediction algorithms has not been seriously addressed. We present first an application for function prediction from protein sequences using the POSet Ontology Categorizer (POSOC) to produce new annotations by analyzing collections of GO nodes derived from annotations of protein BLAST neighborhoods. We then also present hierarchical precision and hierarchical recall as new evaluation metrics for assessing the accuracy of any predictions in hierarchical ontologies, and discuss results on a test set of protein sequences. We show that our method provides substantially improved hierarchical precision (measure of predictions made that are correct) when applied to the nearest BLAST neighbors of target proteins, as compared with simply imputing that neighborhood's annotations to the target. Moreover, when our method is applied to a broader BLAST neighborhood, hierarchical precision is enhanced even further. In all cases, such increased hierarchical precision performance is purchased at a modest expense of hierarchical recall (measure of all annotations that get predicted at all).},
  file = {/home/harrisonpl/Documents/PDFs/Verspoor et al. - 2006 - A categorization approach to automated ontological function annotation.pdf},
  journal = {Protein Science},
  keywords = {Gene Ontology,GO,HPL comprehensive exam,prediction evaluation metrics,protein function prediction},
  language = {en},
  mendeley-tags = {GO,Gene Ontology,HPL comprehensive exam,prediction evaluation metrics,protein function prediction},
  number = {6}
}

@article{vicensTenSimpleRules2007,
  ids = {Vicens2007},
  title = {Ten Simple Rules for a Successful Collaboration},
  author = {Vicens, Quentin and Bourne, Philip E.},
  year = {2007},
  volume = {3},
  pages = {0335--0336},
  issn = {15537358},
  doi = {10.1371/journal.pcbi.0030044},
  file = {/home/harrisonpl/Documents/PDFs/Vicens, Bourne - 2007 - Ten simple rules for a successful collaboration.pdf},
  journal = {PLoS Computational Biology},
  keywords = {Biologists,Communications,Complex systems,Drag,Medicine and health sciences,Personality,Social communication,Telephones},
  number = {3}
}

@inproceedings{vickreySentenceSimplificationSemantic2008,
  title = {Sentence {{Simplification}} for {{Semantic Role Labeling}}},
  booktitle = {{{ACL}}-08: {{HLT}} - 46th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Proceedings}} of the {{Conference}}},
  author = {Vickrey, David and Koller, Daphne},
  year = {2008},
  pages = {344--352},
  abstract = {Parse-tree paths are commonly used to incorporate information from syntactic parses into NLP systems. These systems typically treat the paths as atomic (or nearly atomic) features; these features are quite sparse due to the immense variety of syntactic expression. In this paper, we propose a general method for learning how to iteratively simplify a sentence, thus decomposing complicated syntax into small, easy-to-process pieces. Our method applies a series of hand-written transformation rules corresponding to basic syntactic patterns \textemdash for example, one rule ``depassivizes'' a sentence. The model is parameterized by learned weights specifying preferences for some rules over others. After applying all possible transformations to a sentence, we are left with a set of candidate simplified sentences. We apply our simplification system to semantic role labeling (SRL). As we do not have labeled examples of correct simplifications, we use labeled training data for the SRL task to jointly learn both the weights of the simplification model and of an SRL model, treating the simplification as a hidden variable. By extracting and labeling simplified sentences, this combined simplification/SRL system better generalizes across syntactic variation. It achieves a statistically significant 1.2\% F1 measure increase over a strong baseline on the Conll2005 SRL task, attaining near-state-of-the-art performance.},
  isbn = {978-1-932432-04-6},
  keywords = {\#nosource,Natural language processing,nlp}
}

@article{vuCombiningRecurrentConvolutional2016,
  ids = {Vu2016},
  title = {Combining Recurrent and Convolutional Neural Networks for Relation Classification},
  author = {Vu, Ngoc Thang and Adel, Heike and Gupta, Pankaj and Sch{\"u}tze, Hinrich},
  year = {2016},
  pages = {534--539},
  publisher = {{IEEE}},
  doi = {10.18653/v1/n16-1065},
  abstract = {This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. Finally, we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results. Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task.},
  annotation = {\_eprint: 1605.07333},
  archivePrefix = {arXiv},
  arxivid = {1605.07333},
  eprint = {1605.07333},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Vu et al. - 2016 - Combining recurrent and convolutional neural networks for relation.pdf},
  isbn = {9781941643914},
  journal = {2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference},
  keywords = {multi-task learning,neural networks,relation},
  number = {2016}
}

@article{wadiImpactOutdatedGene2016,
  ids = {Wadi2016},
  title = {Impact of Outdated Gene Annotations on Pathway Enrichment Analysis},
  booktitle = {Nature Methods},
  author = {Wadi, Lina and Meyer, Mona and Weiser, Joel and Stein, Lincoln D. and Reimand, J{\"u}ri},
  year = {2016},
  month = sep,
  volume = {13},
  issn = {15487105},
  doi = {10.1038/nmeth.3963},
  file = {/home/harrisonpl/Documents/PDFs/Wadi et al. - 2016 - Impact of outdated gene annotations on pathway enrichment analysis.pdf},
  journal = {Nature Methods},
  language = {en}
}

@article{wagnerApplicationTwopartStatistics2011,
  ids = {Wagner2011},
  title = {Application of Two-Part Statistics for Comparison of Sequence Variant Counts},
  author = {Wagner, Brandie D. and Robertson, Charles E. and Harris, J. Kirk},
  year = {2011},
  volume = {6},
  pages = {e20296},
  issn = {19326203},
  doi = {10.1371/journal.pone.0020296},
  abstract = {Investigation of microbial communities, particularly human associated communities, is significantly enhanced by the vast amounts of sequence data produced by high throughput sequencing technologies. However, these data create high-dimensional complex data sets that consist of a large proportion of zeros, non-negative skewed counts, and frequently, limited number of samples. These features distinguish sequence data from other forms of high-dimensional data, and are not adequately addressed by statistical approaches in common use. Ultimately, medical studies may identify targeted interventions or treatments, but lack of analytic tools for feature selection and identification of taxa responsible for differences between groups, is hindering advancement. The objective of this paper is to examine the application of a two-part statistic to identify taxa that differ between two groups. The advantages of the two-part statistic over common statistical tests applied to sequence count datasets are discussed. Results from the t-test, the Wilcoxon test, and the two-part test are compared using sequence counts from microbial ecology studies in cystic fibrosis and from cenote samples. We show superior performance of the two-part statistic for analysis of sequence data. The improved performance in microbial ecology studies was independent of study type and sequence technology used. \textcopyright{} 2011 Wagner et al.},
  file = {/home/harrisonpl/Documents/PDFs/Wagner et al. - 2011 - Application of two-part statistics for comparison of sequence variant counts.pdf},
  journal = {PLoS ONE},
  keywords = {Community ecology,Comparative sequence analysis,Microbial ecology,Microbiome,RNA sequence analysis,Sequence analysis,Statistical distributions,Test statistics},
  number = {5}
}

@article{wallukatVadrenergicReceptors2002,
  title = {The {{$\beta$}}-Adrenergic Receptors},
  booktitle = {Herz},
  author = {Wallukat, Gerd},
  year = {2002},
  volume = {27},
  pages = {683--690},
  publisher = {{Springer}},
  issn = {03409937},
  doi = {10.1007/s00059-002-2434-z},
  abstract = {Background: The {$\beta$}-adrenergic receptors of the myocardium play an important role in the regulation of heart function. The {$\beta$}-adrenergic receptors belong to the family of G-protein coupled receptors. Three subtypes have been distinguished ({$\beta$}1-, {$\beta$}2-, and {$\beta$}3-adrenoceptors). The receptors consist of seven membrane-spanning domains, three intra- and three extracellular loops, one extracellular N-terminal domain, and one intracellular C-terminal tail. Pathophysiology: Stimulation of {$\beta$}-adrenergic receptors by catecholamines is realized via the {$\beta$}-adrenoceptor-adenylylcyclase-protein kinase A cascade. The second messenger is the cyclic AMP (cAMP). Stimulation of the cascade caused an accumulation of the second messenger cAMP and activated via the cAMP the cAMP dependent protein kinase A (PKA) The PKA phosphorylated, beside other cell proteins, the {$\beta$}-adrenergic receptors. A phosphorylation of the {$\beta$}-adrenergic receptors caused - with exception of the {$\beta$}3-adrenoceptor - an uncoupling and desensitisation of the receptors. Phosphorylation via the G-protein receptor kinase (GRK or {$\beta$}ARK) also caused uncoupling and reduced the {$\beta$}-adrenergic responsiveness. The uncoupling of the receptor is the prerequisite for receptor internalisation. In the process of internalisation the receptor shifted from the sarcolemma membrane into cytosolic compartments. Chronic {$\beta$}-adrenergic stimulation caused a down-regulation of the receptors. During this process of desensitisation the expression of the receptor on mRNA and protein level is reduced. Changing of the Receptors in the Failing Heart: In patients with dilated cardiomyopathy the {$\beta$}-adrenergic responsiveness of the myocard is diminished. It was shown that in these patients the expression of the {$\beta$}1-adrenergic receptor is reduced on the mRNA and protein level. In these patients the expression of the inhibitory G-protein Gi is increased. Furthermore, the expression of the G-protein receptor kinase is elevated. This kinase induces the uncoupling of the {$\beta$}-adrenergic receptors. These alterations of the {$\beta$}-adrenoceptor signal cascade may be induced by an elevated catecholamine release or by agonist-like autoantibodies directed against the {$\beta$}1-adrenergic receptor found in patients with dilated cardiomyopathy. Both, permanent stimulation with catecholamines and chronic treatment with agonistic anti-{$\beta$}1-adrenoceptor autoantibodies cause a reduction of the expression of the {$\beta$}1-adrenoceptor on mRNA and protein level in "in vitro" experiments. Moreover, an over-expression of the {$\beta$}1-adrenoceptor, the stimulatory Gs protein, and the protein kinase A induce detrimental alterations of the cardiac function and morphology in transgenic animals. These animals developed heart failure accompanied by an increased mortality rate.},
  file = {/home/harrisonpl/Documents/PDFs/Wallukat - 2002 - The span class=nocaseβ-span-adrenergic receptors.pdf},
  journal = {Herz},
  keywords = {Desensitisation,Dilatative cardiomyopathy,Embryonic stem cells,Overexpression,β-adrenoceptor},
  number = {7}
}

@book{wallukatVadrenergicReceptors2002a,
  title = {The {$\beta$}-Adrenergic Receptors},
  author = {Wallukat, Gerd},
  year = {2002},
  month = nov,
  volume = {27},
  publisher = {{Springer}},
  issn = {03409937},
  doi = {10.1007/s00059-002-2434-z},
  abstract = {Background: The {$\beta$}-adrenergic receptors of the myocardium play an important role in the regulation of heart function. The {$\beta$}-adrenergic receptors belong to the family of G-protein coupled receptors. Three subtypes have been distinguished ({$\beta$}1-, {$\beta$}2-, and {$\beta$}3-adrenoceptors). The receptors consist of seven membrane-spanning domains, three intra- and three extracellular loops, one extracellular N-terminal domain, and one intracellular C-terminal tail. Pathophysiology: Stimulation of {$\beta$}-adrenergic receptors by catecholamines is realized via the {$\beta$}-adrenoceptor-adenylylcyclase-protein kinase A cascade. The second messenger is the cyclic AMP (cAMP). Stimulation of the cascade caused an accumulation of the second messenger cAMP and activated via the cAMP the cAMP dependent protein kinase A (PKA) The PKA phosphorylated, beside other cell proteins, the {$\beta$}-adrenergic receptors. A phosphorylation of the {$\beta$}-adrenergic receptors caused - with exception of the {$\beta$}3-adrenoceptor - an uncoupling and desensitisation of the receptors. Phosphorylation via the G-protein receptor kinase (GRK or {$\beta$}ARK) also caused uncoupling and reduced the {$\beta$}-adrenergic responsiveness. The uncoupling of the receptor is the prerequisite for receptor internalisation. In the process of internalisation the receptor shifted from the sarcolemma membrane into cytosolic compartments. Chronic {$\beta$}-adrenergic stimulation caused a down-regulation of the receptors. During this process of desensitisation the expression of the receptor on mRNA and protein level is reduced. Changing of the Receptors in the Failing Heart: In patients with dilated cardiomyopathy the {$\beta$}-adrenergic responsiveness of the myocard is diminished. It was shown that in these patients the expression of the {$\beta$}1-adrenergic receptor is reduced on the mRNA and protein level. In these patients the expression of the inhibitory G-protein Gi is increased. Furthermore, the expression of the G-protein receptor kinase is elevated. This kinase induces the uncoupling of the {$\beta$}-adrenergic receptors. These alterations of the {$\beta$}-adrenoceptor signal cascade may be induced by an elevated catecholamine release or by agonist-like autoantibodies directed against the {$\beta$}1-adrenergic receptor found in patients with dilated cardiomyopathy. Both, permanent stimulation with catecholamines and chronic treatment with agonistic anti-{$\beta$}1-adrenoceptor autoantibodies cause a reduction of the expression of the {$\beta$}1-adrenoceptor on mRNA and protein level in "in vitro" experiments. Moreover, an over-expression of the {$\beta$}1-adrenoceptor, the stimulatory Gs protein, and the protein kinase A induce detrimental alterations of the cardiac function and morphology in transgenic animals. These animals developed heart failure accompanied by an increased mortality rate.},
  file = {/home/harrisonpl/Documents/PDFs/Wallukat - 2002 - The β-adrenergic receptors.pdf},
  journal = {Herz},
  keywords = {Desensitisation,Dilatative cardiomyopathy,Embryonic stem cells,Overexpression,β-adrenoceptor}
}

@article{wangAutomaticDiagnosisEfficient2018,
  ids = {Wang2018},
  title = {Automatic Diagnosis with Efficient Medical Case Searching Based on Evolving Graphs},
  author = {Wang, Xiaoli and Wang, Yuan and Gao, Chuchu and Lin, Kunhui and Li, Yadi},
  year = {2018},
  volume = {6},
  pages = {53307--53318},
  publisher = {{IEEE}},
  issn = {21693536},
  doi = {10.1109/ACCESS.2018.2871769},
  abstract = {The clinical data are often multimodal and consist of both structured data and unstructured data. The modeling of clinical data has become a very important and challenging problem in healthcare big data analytics. Most existing systems focus on only one type of data. In this paper, we propose a knowledge graph-based method to build the linkage between various types of multimodal data. First, we build a semantic-rich knowledge base using both medical dictionaries and practical clinical data collected from hospitals. Second, we propose a graph modeling method to bridge the gap between different types of data, and the multimodal clinical data of each patient are fused and modeled as one unified profile graph. To capture the temporal evolution of the patient's clinical case, the profile graph is represented as a sequence of evolving graphs. Third, we develop a lazy learning algorithm for automatic diagnosis based on graph similarity search. To evaluate our method, we conduct experimental studies on ICU patient diagnosis and Orthopaedics patient classification. The results show that our method could outperform the baseline algorithms. We also implement a real automatic diagnosis system for clinical use. The results obtained from the hospital demonstrate high precision.},
  file = {/home/harrisonpl/Documents/PDFs/Wang et al. - 2018 - Automatic diagnosis with efficient medical case searching based on evolving.pdf},
  journal = {IEEE Access},
  keywords = {Automatic diagnosis,evolving graphs,graph similarity search,multimodal medical data}
}

@article{wangBiasNoveltyScience2017,
  ids = {Wang2017},
  title = {Bias against Novelty in Science: {{A}} Cautionary Tale for Users of Bibliometric Indicators},
  author = {Wang, Jian and Veugelers, Reinhilde and Stephan, Paula},
  year = {2017},
  volume = {46},
  pages = {1416--1436},
  issn = {00487333},
  doi = {10.1016/j.respol.2017.06.006},
  abstract = {Research which explores unchartered waters has a high potential for major impact but also carries a higher uncertainty of having impact. Such explorative research is often described as taking a novel approach. This study examines the complex relationship between pursuing a novel approach and impact. Viewing scientific research as a combinatorial process, we measure novelty in science by examining whether a published paper makes first-time-ever combinations of referenced journals, taking into account the difficulty of making such combinations. We apply this newly developed measure of novelty to all Web of Science research articles published in 2001 across all scientific disciplines. We find that highly novel papers, defined to be those that make more (distant) new combinations, deliver high gains to science: they are more likely to be a top 1\% highly cited paper in the long run, to inspire follow-on highly cited research, and to be cited in a broader set of disciplines and in disciplines that are more distant from their ``home'' field. At the same time, novel research is also more risky, reflected by a higher variance in its citation performance. We also find strong evidence of delayed recognition of novel papers as novel papers are less likely to be top cited when using short time-windows. In addition, we find that novel research is significantly more highly cited in ``foreign'' fields but not in their ``home'' field. Finally, novel papers are published in journals with a lower Impact Factor, compared with non-novel papers, ceteris paribus. These findings suggest that science policy, in particular funding decisions which rely on bibliometric indicators based on short-term citation counts and Journal Impact Factors, may be biased against ``high risk/high gain'' novel research. The findings also caution against a mono-disciplinary approach in peer review to assess the true value of novel research.},
  file = {/home/harrisonpl/Documents/PDFs/Wang et al. - 2017 - Bias against novelty in science.pdf},
  journal = {Research Policy},
  keywords = {Bibliometrics,Breakthrough research,Evaluation,Impact,Novelty},
  number = {8}
}

@article{wangCallingCardsDNAbinding2012,
  ids = {Wang2012,Wang2012a,wangCallingCardsDNAbinding2012a},
  title = {"{{Calling Cards}}" for {{DNA}}-Binding {{Proteins}} in Mammalian {{Cells}}},
  author = {Wang, Haoyi and Mayhew, David and Chen, Xuhua and Johnston, Mark and Mitra, Robi David},
  year = {2012},
  volume = {190},
  pages = {941--949},
  issn = {00166731},
  doi = {10.1534/genetics.111.137315},
  abstract = {The ability to chronicle transcription-factor binding events throughout the development of an organism would facilitate mapping of transcriptional networks that control cell-fate decisions. We describe a method for permanently recording protein-DNA interactions in mammalian cells. We endow transcription factors with the ability to deposit a transposon into the genome near to where they bind. The transposon becomes a "calling card" that the transcription factor leaves behind to record its visit to the genome. The locations of the calling cards can be determined by massively parallel DNA sequencing. We show that the transcription factor SP1 fused to the piggyBac transposase directs insertion of the piggyBac transposon near SP1 binding sites. The locations of transposon insertions are highly reproducible and agree with sites of SP1-binding determined by ChIP-seq. Genes bound by SP1 are more likely to be expressed in the HCT116 cell line we used, and SP1-bound CpG islands show a strong preference to be unmethylated. This method has the potential to trace transcription-factor binding throughout cellular and organismal development in a way that has heretofore not been possible. \textcopyright{} 2012 by the Genetics Society of America.},
  file = {/home/harrisonpl/Documents/PDFs/Wang et al. - 2012 - Calling Cards for DNA-binding Proteins in mammalian Cells.pdf},
  journal = {Genetics},
  number = {3},
  pmid = {22214611}
}

@article{wangFastIdentificationPossible2020,
  title = {Fast {{Identification}} of {{Possible Drug Treatment}} of {{Coronavirus Disease}} -19 ({{COVID}}-19) {{Through Computational Drug Repurposing Study}}},
  author = {Wang, Junmei},
  year = {2020},
  month = feb,
  publisher = {{ChemRxiv; http://web.archive.org/web/20200318211727/https://chemrxiv.org/articles/Fast\_Identification\_of\_Possible\_Drug\_Treatment\_of\_Coronavirus\_Disease\_-19\_COVID-19\_Through\_Computational\_Drug\_Repurposing\_Study/11875446}},
  doi = {10.26434/chemrxiv.11875446.v1},
  abstract = {The recent outbreak of novel coronavirus disease -19 (COVID-19) calls for and welcomes possible treatment strategies using drugs on the market. It is very efficient to apply computer-aided drug design techniques to quickly identify promising drug repurposing candidates, especially after the detailed 3D-structures of key virous proteins are resolved. Taking the advantage of a recently released crystal structure of COVID-19 protease in complex with a covalently-bonded inhibitor, N3,1 I conducted virtual docking screening of approved drugs and drug candidates in clinical trials. For the top docking hits, I then performed molecular dynamics simulations followed by binding free energy calculations using an endpoint method called MM-PBSA-WSAS.2-4 Several promising known drugs stand out as potential inhibitors of COVID-19 protease, including Carfilzomib, Eravacycline, Valrubicin, Lopinavir and Elbasvir. Carfilzomib, an approved anti-cancer drug acting as a proteasome inhibitor, has the best MM-PBSA-WSAS binding free energy, -13.82 kcal/mol. Streptomycin, an antibiotic and a charged molecule, also demonstrates some inhibitory effect, even though the predicted binding free energy of the charged form (-3.82 kcal/mol) is not nearly as low as that of the neutral form (-7.92 kcal/mol). One bioactive, PubChem 23727975, has a binding free energy of -12.86 kcal/mol. Detailed receptor-ligand interactions were analyzed and hot spots for the receptor-ligand binding were identified. I found that one hotspot residue HIS41, is a conserved residue across many viruses including COVID-19, SARS, MERS, and HCV. The findings of this study can facilitate rational drug design targeting the COVID-19 protease.},
  file = {/home/harrisonpl/Documents/PDFs/Wang - 2020 - Fast Identification of Possible Drug Treatment of Coronavirus Disease -19.pdf},
  language = {en}
}

@article{wangNewMethodMeasure2007,
  title = {A New Method to Measure the Semantic Similarity of {{GO}} Terms},
  author = {Wang, James Z. and Du, Zhidian and Payattakool, Rapeeporn and Yu, Philip S. and Chen, Chin-Fu},
  year = {2007},
  month = may,
  volume = {23},
  pages = {1274--1281},
  publisher = {{Oxford Academic; http://web.archive.org/web/20200410050944/https://academic.oup.com/bioinformatics/article/23/10/1274/197095}},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btm087},
  abstract = {Abstract.  Motivation: Although controlled biochemical or biological vocabularies, such as Gene Ontology (GO) (http://www.geneontology.org), address the need fo},
  annotation = {ZSCC: 0000824},
  file = {/home/harrisonpl/Documents/PDFs/Wang et al. - 2007 - A new method to measure the semantic similarity of GO terms.pdf},
  journal = {Bioinformatics},
  language = {en},
  number = {10}
}

@article{wangQuantificationProteinsMetabolites2003,
  ids = {Wang2003},
  title = {Quantification of Proteins and Metabolites by Mass Spectrometry without Isotopic Labeling or Spiked Standards},
  author = {Wang, Weixun and Becker, Christopher H. and Zhou, Haihong and Lin, Hua and Roy, Sushmita and Shaler, Thomas A. and Hill, Lander R. and Norton, Scott and Kumar, Praveen and Anderle, Markus},
  year = {2003},
  volume = {75},
  pages = {4818--4826},
  issn = {00032700},
  doi = {10.1021/ac026468x},
  abstract = {A new method is presented for quantifying proteomic and metabolomic profile data by liquid chromatography- mass spectrometry (LC-MS) with electrospray ionization. This biotechnology provides differential expression measurements and enables the discovery of biological markers (biomarkers). Work presented here uses human serum but is applicable to any fluid or tissue. The approach relies on linearity of signal versus molecular concentration and reproducibility of sample processing. There is no use of isotopic labeling or chemically similar standard materials. Linear standard curves are reported for a variety of compounds introduced into human serum. As a measure of analytical reproducibility for proteome and metabolome sampling, median coefficients of variation of 25.7 and 23.8\%, respectively, were determined for {$\sim$}3400 molecular ions (not counting their numerous isotopes) from 25 independently processed human serum samples, corresponding to a total of 85 000 individual molecular ion measurements.},
  file = {/home/harrisonpl/Documents/PDFs/Wang et al. - 2003 - Quantification of proteins and metabolites by mass spectrometry without.pdf},
  journal = {Analytical Chemistry},
  number = {18}
}

@article{webb-robertsonReviewEvaluationDiscussion2015,
  ids = {Webb-Robertson,Webb-Robertson2015,Webb-Robertson2015a},
  title = {Review, Evaluation, and Discussion of the Challenges of Missing Value Imputation for Mass Spectrometry-Based Label-Free Global Proteomics},
  author = {{Webb-Robertson}, Bobbie Jo M. and Wiberg, Holli K. and Matzke, Melissa M. and Brown, Joseph N. and Wang, Jing and McDermott, Jason E. and Smith, Richard D. and Rodland, Karin D. and Metz, Thomas O. and Pounds, Joel G. and Waters, Katrina M.},
  year = {2015},
  volume = {14},
  pages = {1993--2001},
  issn = {15353907},
  doi = {10.1021/pr501138h},
  abstract = {In this review, we apply selected imputation strategies to label-free liquid chromatography-mass spectrometry (LC-MS) proteomics datasets to evaluate the accuracy with respect to metrics of variance and classification. We evaluate several commonly used imputation approaches for individual merits and discuss the caveats of each approach with respect to the example LC-MS proteomics data. In general, local similarity-based approaches, such as the regularized expectation maximization and least-squares adaptive algorithms, yield the best overall performances with respect to metrics of accuracy and robustness. However, no single algorithm consistently outperforms the remaining approaches, and in some cases, performing classification without imputation sometimes yielded the most accurate classification. Thus, because of the complex mechanisms of missing data in proteomics, which also vary from peptide to protein, no individual method is a single solution for imputation. On the basis of the observations in this review, the goal for imputation in the field of computational proteomics should be to develop new approaches that work generically for this data type and new strategies to guide users in the selection of the best imputation for their dataset and analysis objectives.},
  file = {/home/harrisonpl/Documents/PDFs/Webb-Robertson et al. - 2015 - Review, evaluation, and discussion of the challenges of missing value.pdf},
  journal = {Journal of Proteome Research},
  keywords = {accuracy,classification,Imputation,label free,mean-square error,peak intensity},
  number = {5},
  pmid = {25855118}
}

@article{weiAssessingStateArt2016,
  ids = {Wei2016,Wei2016c},
  title = {Assessing the State of the Art in Biomedical Relation Extraction: {{Overview}} of the {{BioCreative V}} Chemical-Disease Relation ({{CDR}}) Task},
  shorttitle = {Assessing the State of the Art in Biomedical Relat},
  author = {Wei, Chih Hsuan and Peng, Yifan and Leaman, Robert and Davis, Allan Peter and Mattingly, Carolyn J. and Li, Jiao and Wiegers, Thomas C. and Lu, Zhiyong},
  year = {2016},
  volume = {2016},
  pages = {1--8},
  issn = {17580463},
  doi = {10.1093/database/baw032},
  abstract = {Manually curating chemicals, diseases and their relationships is significantly important to biomedical research, but it is plagued by its high cost and the rapid growth of the biomedical literature. In recent years, there has been a growing interest in developing computational approaches for automatic chemical-disease relation (CDR) extraction. Despite these attempts, the lack of a comprehensive benchmarking dataset has limited the comparison of different techniques in order to assess and advance the current state-of-the-art. To this end, we organized a challenge task through BioCreative V to automatically extract CDRs from the literature. We designed two challenge tasks: disease named entity recognition (DNER) and chemicalinduced disease (CID) relation extraction. To assist system development and assessment, we created a large annotated text corpus that consisted of human annotations of chemicals, diseases and their interactions from 1500 PubMed articles. 34 teams worldwide participated in the CDR task: 16 (DNER) and 18 (CID). The best systems achieved an F-score of 86.46\% for the DNER task-a result that approaches the human inter-annotator agreement (0.8875)- and an F-score of 57.03\% for the CID task, the highest results ever reported for such tasks. When combining team results via machine learning, the ensemble system was able to further improve over the best team results by achieving 88.89\% and 62.80\% in F-score for the DNER and CID task, respectively. Additionally, another novel aspect of our evaluation is to test each participating system's ability to return real-time results: the average response time for each team's DNER and CID web service systems were 5.6 and 9.3 s, respectively. Most teams used hybrid systems for their submissions based on machining learning. Given the level of participation and results, we found our task to be successful in engaging the text-mining research community, producing a large annotated corpus and improving the results of automatic disease recognition and CDR extraction.},
  file = {/home/harrisonpl/Documents/PDFs/Wei et al. - 2016 - Assessing the state of the art in biomedical relation extraction.pdf},
  journal = {Database},
  keywords = {Biomedical Research,Data Mining,Databases- Factual,Disease,Humans,Statistics as Topic,Time Factors},
  language = {eng},
  mendeley-tags = {Biomedical Research,Data Mining,Databases- Factual,Disease,Humans,Statistics as Topic,Time Factors},
  pmid = {26994911}
}

@article{wenFewShotLearningRelation2019,
  ids = {Wen2019},
  title = {Few-{{Shot Learning}} for {{Relation Extraction}}},
  author = {Wen, Ruichen},
  year = {2019},
  file = {/home/harrisonpl/Documents/PDFs/Wen - 2019 - Few-Shot Learning for Relation Extraction.pdf},
  number = {May}
}

@article{wengMedicalSubdomainClassification2017,
  ids = {Weng2017},
  title = {Medical Subdomain Classification of Clinical Notes Using a Machine Learning-Based Natural Language Processing Approach},
  author = {Weng, Wei Hung and Wagholikar, Kavishwar B. and McCray, Alexa T. and Szolovits, Peter and Chueh, Henry C.},
  year = {2017},
  month = dec,
  volume = {17},
  pages = {155},
  publisher = {{BioMed Central Ltd.}},
  issn = {14726947},
  doi = {10.1186/s12911-017-0556-8},
  abstract = {Background: The medical subdomain of a clinical note, such as cardiology or neurology, is useful content-derived metadata for developing machine learning downstream applications. To classify the medical subdomain of a note accurately, we have constructed a machine learning-based natural language processing (NLP) pipeline and developed medical subdomain classifiers based on the content of the note. Methods: We constructed the pipeline using the clinical NLP system, clinical Text Analysis and Knowledge Extraction System (cTAKES), the Unified Medical Language System (UMLS) Metathesaurus, Semantic Network, and learning algorithms to extract features from two datasets - clinical notes from Integrating Data for Analysis, Anonymization, and Sharing (iDASH) data repository (n = 431) and Massachusetts General Hospital (MGH) (n = 91,237), and built medical subdomain classifiers with different combinations of data representation methods and supervised learning algorithms. We evaluated the performance of classifiers and their portability across the two datasets. Results: The convolutional recurrent neural network with neural word embeddings trained-medical subdomain classifier yielded the best performance measurement on iDASH and MGH datasets with area under receiver operating characteristic curve (AUC) of 0.975 and 0.991, and F1 scores of 0.845 and 0.870, respectively. Considering better clinical interpretability, linear support vector machine-trained medical subdomain classifier using hybrid bag-of-words and clinically relevant UMLS concepts as the feature representation, with term frequency-inverse document frequency (tf-idf)-weighting, outperformed other shallow learning classifiers on iDASH and MGH datasets with AUC of 0.957 and 0.964, and F1 scores of 0.932 and 0.934 respectively. We trained classifiers on one dataset, applied to the other dataset and yielded the threshold of F1 score of 0.7 in classifiers for half of the medical subdomains we studied. Conclusion: Our study shows that a supervised learning-based NLP approach is useful to develop medical subdomain classifiers. The deep learning algorithm with distributed word representation yields better performance yet shallow learning algorithms with the word and concept representation achieves comparable performance with better clinical interpretability. Portable classifiers may also be used across datasets from different institutions.},
  file = {/home/harrisonpl/Documents/PDFs/Weng et al. - 2017 - Medical subdomain classification of clinical notes using a machine.pdf},
  journal = {BMC Medical Informatics and Decision Making},
  keywords = {Computer-assisted,Deep Learning,Distributed Representation,Machine Learning,Medical Decision Making,Natural Language Processing,Unified Medical Language System},
  number = {1}
}

@article{westonMemoryNetworks2015,
  ids = {Weston,Weston2015,Weston2015a},
  title = {Memory Networks},
  author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
  year = {2015},
  pages = {1--15},
  abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
  annotation = {\_eprint: 1410.3916},
  archivePrefix = {arXiv},
  arxivid = {1410.3916},
  eprint = {1410.3916},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Weston et al. - 2015 - Memory networks.pdf},
  journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,hunter lab,Statistics - Machine Learning},
  mendeley-tags = {hunter lab}
}

@article{wildSystemsChemicalBiology2012,
  title = {Systems Chemical Biology and the {{Semantic Web}}: {{What}} They Mean for the Future of Drug Discovery Research},
  shorttitle = {Systems Chemical Biology and the {{Semantic Web}}},
  author = {Wild, David J. and Ding, Ying and Sheth, Amit P. and Harland, Lee and Gifford, Eric M. and Lajiness, Michael S.},
  year = {2012},
  volume = {17},
  pages = {469--474},
  issn = {13596446},
  doi = {10.1016/j.drudis.2011.12.019},
  abstract = {Systems chemical biology, the integration of chemistry, biology and computation to generate understanding about the way small molecules affect biological systems as a whole, as well as related fields such as chemogenomics, are central to emerging new paradigms of drug discovery such as drug repurposing and personalized medicine. Recent Semantic Web technologies such as RDF and SPARQL are technical enablers of systems chemical biology, facilitating the deployment of advanced algorithms for searching and mining large integrated datasets. In this paper, we aim to demonstrate how these technologies together can change the way that drug discovery is accomplished. \textcopyright{} 2011 Elsevier Ltd. All rights reserved.},
  journal = {Drug Discovery Today},
  keywords = {\#nosource,hunter lab},
  number = {9-10}
}

@article{wishartChapterSmallMolecules2012,
  ids = {Wishart2012},
  title = {Chapter 3: {{Small Molecules}} and {{Disease}}},
  shorttitle = {Chapter 3},
  author = {Wishart, David S.},
  year = {2012},
  volume = {8},
  issn = {1553734X},
  doi = {10.1371/journal.pcbi.1002805},
  abstract = {"Big" molecules such as proteins and genes still continue to capture the imagination of most biologists, biochemists and bioinformaticians. "Small" molecules, on the other hand, are the molecules that most biologists, biochemists and bioinformaticians prefer to ignore. However, it is becoming increasingly apparent that small molecules such as amino acids, lipids and sugars play a far more important role in all aspects of disease etiology and disease treatment than we realized. This particular chapter focuses on an emerging field of bioinformatics called "chemical bioinformatics" - a discipline that has evolved to help address the blended chemical and molecular biological needs of toxicogenomics, pharmacogenomics, metabolomics and systems biology. In the following pages we will cover several topics related to chemical bioinformatics. First, a brief overview of some of the most important or useful chemical bioinformatic resources will be given. Second, a more detailed overview will be given on those particular resources that allow researchers to connect small molecules to diseases. This section will focus on describing a number of recently developed databases or knowledgebases that explicitly relate small molecules - either as the treatment, symptom or cause - to disease. Finally a short discussion will be provided on newly emerging software tools that exploit these databases as a means to discover new biomarkers or even new treatments for disease. \textcopyright{} 2012 David S.},
  file = {/home/harrisonpl/Documents/PDFs/Wishart - 2012 - Chapter 3.pdf},
  journal = {PLoS Computational Biology},
  keywords = {mechanism},
  mendeley-tags = {mechanism},
  number = {12},
  pmid = {23300405}
}

@article{wishartDrugBankMajorUpdate2018,
  ids = {Wishart2018},
  title = {{{DrugBank}} 5.0: {{A}} Major Update to the {{DrugBank}} Database for 2018},
  shorttitle = {{{DrugBank}} 5.0},
  author = {Wishart, David S. and Feunang, Yannick D. and Guo, An C. and Lo, Elvis J. and Marcu, Ana and Grant, Jason R. and Sajed, Tanvir and Johnson, Daniel and Li, Carin and Sayeeda, Zinat and Assempour, Nazanin and Iynkkaran, Ithayavani and Liu, Yifeng and MacIejewski, Adam and Gale, Nicola and Wilson, Alex and Chin, Lucy and Cummings, Ryan and Le, DIana and Pon, Allison and Knox, Craig and Wilson, Michael},
  year = {2018},
  volume = {46},
  pages = {D1074--D1082},
  issn = {13624962},
  doi = {10.1093/nar/gkx1037},
  abstract = {DrugBank (www.drugbank.ca) is a web-enabled database containing comprehensive molecular information about drugs, their mechanisms, their interactions and their targets. First described in 2006, DrugBank has continued to evolve over the past 12 years in response to marked improvements to web standards and changing needs for drug research and development. This year's update, DrugBank 5.0, represents the most significant upgrade to the database in more than 10 years. In many cases, existing data content has grown by 100\% or more over the last update. For instance, the total number of investigational drugs in the database has grown by almost 300\%, the number of drug-drug interactions has grown by nearly 600\% and the number of SNP-associated drug effects has grown more than 3000\%. Significant improvements have been made to the quantity, quality and consistency of drug indications, drug binding data as well as drug-drug and drug-food interactions. A great deal of brand new data have also been added to DrugBank 5.0. This includes information on the influence of hundreds of drugs on metabolite levels (pharmacometabolomics), gene expression levels (pharmacotranscriptomics) and protein expression levels (pharmacoprotoemics). New data have also been added on the status of hundreds of new drug clinical trials and existing drug repurposing trials. Many other important improvements in the content, interface and performance of the DrugBank website have been made and these should greatly enhance its ease of use, utility and potential applications in many areas of pharmacological research, pharmaceutical science and drug education.},
  file = {/home/harrisonpl/Documents/PDFs/Wishart et al. - 2018 - DrugBank 5.pdf},
  journal = {Nucleic Acids Research},
  keywords = {mechanism},
  mendeley-tags = {mechanism},
  number = {D1},
  pmid = {29126136}
}

@article{wolffSystematicReviewEconomic2019,
  ids = {Wolff2019},
  title = {A {{Systematic Review}} of {{Economic Impact Studies}} of {{Artificial Intelligence}} in {{Healthcare}} ({{Preprint}})},
  author = {Wolff, Justus and Baumbach, Jan and Pauling, Josch and Keck, Andreas},
  year = {2019},
  month = feb,
  volume = {22},
  pages = {1--8},
  publisher = {{JMIR Publications Inc.}},
  issn = {1438-8871},
  doi = {10.2196/16866},
  abstract = {BACKGROUND Positive economic impact is a key decision factor in making the case for or against investing in an artificial intelligence (AI) solution in the health care industry. It is most relevant for the care provider and insurer as well as for the pharmaceutical and medical technology sectors. Although the broad economic impact of digital health solutions in general has been assessed many times in literature and the benefit for patients and society has also been analyzed, the specific economic impact of AI in health care has been addressed only sporadically. OBJECTIVE This study aimed to systematically review and summarize the cost-effectiveness studies dedicated to AI in health care and to assess whether they meet the established quality criteria. METHODS In a first step, the quality criteria for economic impact studies were defined based on the established and adapted criteria schemes for cost impact assessments. In a second step, a systematic literature review based on qualitative and quantitative inclusion and exclusion criteria was conducted to identify relevant publications for an in-depth analysis of the economic impact assessment. In a final step, the quality of the identified economic impact studies was evaluated based on the defined quality criteria for cost-effectiveness studies. RESULTS Very few publications have thoroughly addressed the economic impact assessment, and the economic assessment quality of the reviewed publications on AI shows severe methodological deficits. Only 6 out of 66 publications could be included in the second step of the analysis based on the inclusion criteria. Out of these 6 studies, none comprised a methodologically complete cost impact analysis. There are two areas for improvement in future studies. First, the initial investment and operational costs for the AI infrastructure and service need to be included. Second, alternatives to achieve similar impact must be evaluated to provide a comprehensive comparison. CONCLUSIONS This systematic literature analysis proved that the existing impact assessments show methodological deficits and that upcoming evaluations require more comprehensive economic analyses to enable economic decisions for or against implementing AI technology in health care.},
  journal = {Journal of Medical Internet Research},
  keywords = {\#nosource,artificial intelligence,cost-benefit analysis,machine learning,telemedicine},
  number = {2},
  pmid = {32130134}
}

@article{wonChoosingOptimalMethod2009,
  ids = {Won2009},
  title = {Choosing an Optimal Method to Combine {{P}}-Values},
  author = {Won, Sungho and Morris, Nathan and Lu, Qing and Elston, Robert C.},
  year = {2009},
  volume = {28},
  pages = {1537--1553},
  issn = {02776715},
  doi = {10.1002/sim.3569},
  abstract = {Fisher (1925) was the first to suggest a method of combining the p-values obtained from several statistics and many other methods have been proposed since then. However, there is no agreement about what is the best method. Motivated by a situation that now often arises in genetic epidemiology, we consider the problem when it is possible to define a simple alternative hypothesis of interest for which the expected effect size of each test statistic is known and we determine the most powerful test for this simple alternative hypothesis. Based on the proposed method, we show that information about the effect sizes can be used to obtain the best weights for Liptak's method of combining p-values. We present extensive simulation results comparing methods of combining p-values and illustrate for a real example in genetic epidemiology how information about effect sizes can be deduced. Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.},
  file = {/home/harrisonpl/Documents/PDFs/Won et al. - 2009 - Choosing an optimal method to combine P-values.pdf},
  journal = {Statistics in Medicine},
  keywords = {Effect size,Fisher,Liptak},
  number = {11},
  pmid = {19266501}
}

@article{woodSequenceMemoizer2011,
  ids = {Wood2011},
  title = {The Sequence Memoizer},
  author = {Wood, Frank and Gasthaus, Jan and Archambeau, C{\'e}dric and James, Lancelot and Teh, Yee Whye},
  year = {2011},
  volume = {54},
  pages = {91--98},
  issn = {00010782},
  doi = {10.1145/1897816.1897842},
  abstract = {Probabilistic models of sequences play a central role in most machine translation, automated speech recognition, lossless compression, spell-checking, and gene identification applications to name but a few. Unfortunately, realworld sequence data often exhibit long range dependencies which can only be captured by computationally challenging, complex models. Sequence data arising from natural processes also often exhibits power-law properties, yet common sequence models do not capture such properties. The sequence memoizer is a new hierarchical Bayesian model for discrete sequence data that captures long range dependencies and power-law characteristics, while remaining computationally attractive. Its utility as a language model and general purpose lossless compressor is demonstrated. \textcopyright{} 2011 ACM.},
  file = {/home/harrisonpl/Documents/PDFs/Wood et al. - 2011 - The sequence memoizer.pdf},
  journal = {Communications of the ACM},
  keywords = {hunter lab},
  mendeley-tags = {hunter lab},
  number = {2}
}

@inproceedings{wuAutomaticKnowledgeGraph2019,
  title = {Automatic {{Knowledge Graph Construction}}: {{A Report}} on the 2019 {{ICDM}}/{{ICBK Contest}}},
  shorttitle = {Automatic {{Knowledge Graph Construction}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Wu, Xindong and Wu, Jia and Fu, Xiaoyi and Li, Jiachen and Zhou, Peng and Jiang, Xu},
  year = {2019},
  month = nov,
  pages = {1540--1545},
  issn = {2374-8486},
  doi = {10.1109/ICDM.2019.00204},
  abstract = {Automatic knowledge graph construction seeks to build a knowledge graph from unstructured text in a specific domain or cross multiple domains, without human intervention. IEEE ICDM 2019 and ICBK 2019 invited teams from both degree-granting institutions and industrial labs to compete in the 2019 Knowledge Graph Contest by automatically constructing knowledge graphs in at least two different domains. This article reports the outcomes of the Contest. The participants were expected to build a model to extract knowledge represented as triplets from text data and develop a web application to visualize the triplets. Awards were given to five teams. Their models and key techniques used to construct knowledge graphs are summarized.},
  file = {/home/harrisonpl/Documents/PDFs/Wu et al. - 2019 - Automatic Knowledge Graph Construction.pdf;/home/harrisonpl/Zotero/storage/4TAXACRQ/8970862.html},
  keywords = {automatic knowledge graph construction,graph theory,ICBK 2019,IEEE ICDM 2019,Internet,knowledge acquisition,knowledge extraction,knowledge representation,Knowledge-Graph-Construction,text analysis,text data,unstructured text,Web application}
}

@article{wuClinicalNamedEntity2017,
  ids = {Wu2017,Wu2017a},
  title = {Clinical {{Named Entity Recognition Using Deep Learning Models}}},
  author = {Wu, Yonghui and Jiang, Min and Xu, Jun and Zhi, Degui and Xu, Hua},
  year = {2017},
  volume = {2017},
  pages = {1812--1819},
  publisher = {{NLM (Medline)}},
  issn = {1942597X},
  abstract = {Clinical Named Entity Recognition (NER) is a critical natural language processing (NLP) task to extract important concepts (named entities) from clinical narratives. Researchers have extensively investigated machine learning models for clinical NER. Recently, there have been increasing efforts to apply deep learning models to improve the performance of current clinical NER systems. This study examined two popular deep learning architectures, the Convolutional Neural Network (CNN) and the Recurrent Neural Network (RNN), to extract concepts from clinical texts. We compared the two deep neural network architectures with three baseline Conditional Random Fields (CRFs) models and two state-of-the-art clinical NER systems using the i2b2 2010 clinical concept extraction corpus. The evaluation results showed that the RNN model trained with the word embeddings achieved a new state-of-the- art performance (a strict F1 score of 85.94\%) for the defined clinical NER task, outperforming the best-reported system that used both manually defined and unsupervised learning features. This study demonstrates the advantage of using deep neural network architectures for clinical concept extraction, including distributed feature representation, automatic feature learning, and long-term dependencies capture. This is one of the first studies to compare the two widely used deep learning models and demonstrate the superior performance of the RNN model for clinical NER.},
  file = {/home/harrisonpl/Documents/PDFs/Wu et al. - 2017 - Clinical Named Entity Recognition Using Deep Learning Models.pdf},
  journal = {AMIA ... Annual Symposium proceedings. AMIA Symposium},
  pmid = {29854252}
}

@article{xieDevelopmentalBiologyInforms2003,
  ids = {xieDevelopmentalBiologyInforms2003a},
  title = {Developmental Biology Informs Cancer: {{The}} Emerging Role of the Hedgehog Signaling Pathway in Upper Gastrointestinal Cancers},
  shorttitle = {Developmental Biology Informs Cancer},
  author = {Xie, Keping and Abbruzzese, James L},
  year = {2003},
  month = oct,
  volume = {4},
  pages = {245--247},
  issn = {1535-6108},
  doi = {10.1016/S1535-6108(03)00246-0},
  abstract = {The hedgehog (Hh) signaling pathway plays many roles in invertebrate and vertebrate development. For example, specific inhibition of sonic Hh expression is critical during early stages of pancreas organogenesis, but an active Hh pathway appears to be required for maintenance of adult endocrine functions. Mutational inactivation of the Hh pathway has been demonstrated in human malignancies of the skin, cerebellum, and skeletal muscle. Now, two papers implicate aberrant Hh signaling in human upper gastrointestinal cancers including those developing from the esophagus, stomach, biliary tract, and pancreas.},
  file = {/home/harrisonpl/Documents/PDFs/Xie, Abbruzzese - 2003 - Developmental biology informs cancer.pdf;/home/harrisonpl/Documents/PDFs/Xie, Abbruzzese - 2003 - Developmental biology informs cancer2.pdf;/home/harrisonpl/Zotero/storage/ADKTML8W/S1535610803002460.html;/home/harrisonpl/Zotero/storage/CXEBL7GJ/S1535610803002460.html},
  journal = {Cancer Cell},
  language = {en},
  number = {4}
}

@article{yamanishiPredictionDrugtargetInteraction2008,
  ids = {Yamanishi2008},
  title = {Prediction of Drug-Target Interaction Networks from the Integration of Chemical and Genomic Spaces},
  author = {Yamanishi, Yoshihiro and Araki, Michihiro and Gutteridge, Alex and Honda, Wataru and Kanehisa, Minoru},
  year = {2008},
  volume = {24},
  pages = {i232--i240},
  issn = {13674803},
  doi = {10.1093/bioinformatics/btn162},
  abstract = {Motivation: The identification of interactions between drugs and target proteins is a key area in genomic drug discovery. Therefore, there is a strong incentive to develop new methods capable of detecting these potential drug-target interactions efficiently. Results: In this article, we characterize four classes of drug-target interaction networks in humans involving enzymes, ion channels, G-protein-coupled receptors (GPCRs) and nuclear receptors, and reveal significant correlations between drug structure similarity, target sequence similarity and the drug-target interaction network topology. We then develop new statistical methods to predict unknown drug-target interaction networks from chemical structure and genomic sequence information simultaneously on a large scale. The originality of the proposed method lies in the formalization of the drug-target interaction inference as a supervised learning problem for a bipartite graph, the lack of need for 3D structure information of the target proteins, and in the integration of chemical and genomic spaces into a unified space that we call 'pharmacological space'. In the results, we demonstrate the usefulness of our proposed method for the prediction of the four classes of drug-target interaction networks. Our comprehensively predicted drug-target interaction networks enable us to suggest many potential drug-target interactions and to increase research productivity toward genomic drug discovery. \textcopyright{} 2008 The Author(s).},
  file = {/home/harrisonpl/Documents/PDFs/Yamanishi et al. - 2008 - Prediction of drug-target interaction networks from the integration of chemical.pdf},
  journal = {Bioinformatics},
  keywords = {mechanism},
  mendeley-tags = {mechanism},
  number = {13},
  pmid = {18586719}
}

@article{yanContextAwarenessEmbedding2020,
  ids = {Yan},
  title = {Context Awareness and Embedding for Biomedical Event Extraction},
  author = {Yan, Shankai and Wong, Ka Chun},
  year = {2020},
  volume = {36},
  pages = {637--643},
  issn = {13674811},
  doi = {10.1093/bioinformatics/btz607},
  abstract = {MOTIVATION: Biomedical event extraction is fundamental for information extraction in molecular biology and biomedical research. The detected events form the central basis for comprehensive biomedical knowledge fusion, facilitating the digestion of massive information influx from the literature. Limited by the event context, the existing event detection models are mostly applicable for a single task. A general and scalable computational model is desiderated for biomedical knowledge management. RESULTS: We consider and propose a bottom-up detection framework to identify the events from recognized arguments. To capture the relations between the arguments, we trained a bidirectional long short-term memory network to model their context embedding. Leveraging the compositional attributes, we further derived the candidate samples for training event classifiers. We built our models on the datasets from BioNLP Shared Task for evaluations. Our method achieved the average F-scores of 0.81 and 0.92 on BioNLPST-BGI and BioNLPST-BB datasets, respectively. Comparing with seven state-of-the-art methods, our method nearly doubled the existing F-score performance (0.92 versus 0.56) on the BioNLPST-BB dataset. Case studies were conducted to reveal the underlying reasons. AVAILABILITY AND IMPLEMENTATION: https://github.com/cskyan/evntextrc. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
  annotation = {\_eprint: 1905.00982},
  archivePrefix = {arXiv},
  arxivid = {1905.00982},
  eprint = {1905.00982},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Yan, Wong - 2020 - Context awareness and embedding for biomedical event extraction.pdf},
  journal = {Bioinformatics (Oxford, England)},
  number = {2}
}

@article{yangWhiteBoxMachineLearning2019,
  ids = {WhiteboxMachineLearning2019,Yang2019,Yang2019a},
  title = {A {{White}}-{{Box Machine Learning Approach}} for {{Revealing Antibiotic Mechanisms}} of {{Action}}},
  author = {Yang, Jason H. and Wright, Sarah N. and Hamblin, Meagan and McCloskey, Douglas and Alcantar, Miguel A. and Schr{\"u}bbers, Lars and Lopatkin, Allison J. and Satish, Sangeeta and Nili, Amir and Palsson, Bernhard O. and Walker, Graham C. and Collins, James J.},
  year = {2019},
  month = may,
  volume = {177},
  pages = {1649--1661.e9},
  issn = {10974172},
  doi = {10.1016/j.cell.2019.04.016},
  abstract = {Current machine learning techniques enable robust association of biological signals with measured phenotypes, but these approaches are incapable of identifying causal relationships. Here, we develop an integrated ``white-box'' biochemical screening, network modeling, and machine learning approach for revealing causal mechanisms and apply this approach to understanding antibiotic efficacy. We counter-screen diverse metabolites against bactericidal antibiotics in Escherichia coli and simulate their corresponding metabolic states using a genome-scale metabolic network model. Regression of the measured screening data on model simulations reveals that purine biosynthesis participates in antibiotic lethality, which we validate experimentally. We show that antibiotic-induced adenine limitation increases ATP demand, which elevates central carbon metabolism activity and oxygen consumption, enhancing the killing effects of antibiotics. This work demonstrates how prospective network modeling can couple with machine learning to identify complex causal mechanisms underlying drug efficacy. Causal metabolic pathways underlying antibiotic lethality in bacteria are illuminated by a network model-driven machine learning approach, overcoming limitations of existing ``black-box'' approaches that cannot reveal causal relationships from large biological datasets.},
  file = {/home/harrisonpl/Documents/PDFs/Yang et al. - 2019 - A White-Box Machine Learning Approach for Revealing Antibiotic Mechanisms of.pdf},
  journal = {Cell},
  keywords = {adenylate energy charge,antibiotics,ATP,biochemical screen,LC-MS/MS,machine learning,metabolism,NADPH:NADP ratio,NADPH:NADP+ ratio,network modeling,purine biosynthesis},
  language = {en},
  mendeley-tags = {ATP,LC-MS/MS,NADPH:NADP ratio,adenylate energy charge,antibiotics,biochemical screen,machine learning,metabolism,network modeling,purine biosynthesis},
  number = {6}
}

@article{yeganovaDiscoveringThemesBiomedical2018,
  ids = {Yeganova2018},
  title = {Discovering Themes in Biomedical Literature Using a Projection-Based Algorithm},
  author = {Yeganova, Lana and Kim, Sun and Balasanov, Grigory and Wilbur, W. John},
  year = {2018},
  volume = {19},
  pages = {269},
  issn = {14712105},
  doi = {10.1186/s12859-018-2240-0},
  abstract = {Background: The need to organize any large document collection in a manner that facilitates human comprehension has become crucial with the increasing volume of information available. Two common approaches to provide a broad overview of the information space are document clustering and topic modeling. Clustering aims to group documents or terms into meaningful clusters. Topic modeling, on the other hand, focuses on finding coherent keywords for describing topics appearing in a set of documents. In addition, there have been efforts for clustering documents and finding keywords simultaneously. Results: We present an algorithm to analyze document collections that is based on a notion of a theme, defined as a dual representation based on a set of documents and key terms. In this work, a novel vector space mechanism is proposed for computing themes. Starting with a single document, the theme algorithm treats terms and documents as explicit components, and iteratively uses each representation to refine the other until the theme is detected. The method heavily relies on an optimization routine that we refer to as the projection algorithm which, under specific conditions, is guaranteed to converge to the first singular vector of a data matrix. We apply our algorithm to a collection of about sixty thousand PubMed documents examining the subject of Single Nucleotide Polymorphism, evaluate the results and show the effectiveness and scalability of the proposed method. Conclusions: This study presents a contribution on theoretical and algorithmic levels, as well as demonstrates the feasibility of the method for large scale applications. The evaluation of our system on benchmark datasets demonstrates that our method compares favorably with the current state-of-the-art methods in computing clusters of documents with coherent topic terms.},
  file = {/home/harrisonpl/Documents/PDFs/Yeganova et al. - 2018 - Discovering themes in biomedical literature using a projection-based algorithm.pdf},
  journal = {BMC Bioinformatics},
  keywords = {First singular vector,Natural language processing,Projection algorithm,Theme discovery},
  mendeley-tags = {Natural language processing},
  number = {1}
}

@article{yoonFunctionalGenomicMetagenomic2015,
  title = {Functional Genomic and Metagenomic Approaches to Understanding Gut Microbiota-Animal Mutualism},
  author = {Yoon, Sang Sun and Kim, Eun Kyoung and Lee, Won Jae},
  year = {2015},
  volume = {24},
  abstract = {Accumulating data sets of gut microbiome by next-generation sequencing allow us to gain a comprehensive view of the functional diversity of the gut-associated metagenome. However, many microbiome functions are unknown and/or have only been predicted, and may not necessarily reflect the in vivo function within a gut niche. Functional genomic and metagenomic approaches have been successfully applied to broaden the understanding of invertebrate and vertebrate gut microbiome involved in diverse functions, including colonization ability, nutritional processing, antibiotic resistance, microbial physiology and metabolism, and the modulation of the host physiology. In this review, we discuss the recent knowledge obtained from the study of functional genomics and metagenomics of the animal intestine and its potential values for understanding gut microbiota-animal mutualism.},
  keywords = {\#nosource}
}

@article{yordanovMethodIdentifyAnalyze2016,
  ids = {Yordanov2016},
  title = {A Method to Identify and Analyze Biological Programs through Automated Reasoning},
  author = {Yordanov, Boyan and Dunn, Sara Jane and Kugler, Hillel and Smith, Austin and Martello, Graziano and Emmott, Stephen},
  year = {2016},
  volume = {2},
  publisher = {{Nature Publishing Group}},
  issn = {20567189},
  doi = {10.1038/npjsba.2016.10},
  abstract = {Predictive biology is elusive because rigorous, data-constrained, mechanistic models of complex biological systems are difficult to derive and validate. Current approaches tend to construct and examine static interaction network models, which are descriptively rich, but often lack explanatory and predictive power, or dynamic models that can be simulated to reproduce known behavior. However, in such approaches implicit assumptions are introduced as typically only one mechanism is considered, and exhaustively investigating all scenarios is impractical using simulation. To address these limitations, we present a methodology based on automated formal reasoning, which permits the synthesis and analysis of the complete set of logical models consistent with experimental observations. We test hypotheses against all candidate models, and remove the need for simulation by characterizing and simultaneously analyzing all mechanistic explanations of observed behavior. Our methodology transforms knowledge of complex biological processes from sets of possible interactions and experimental observations to precise, predictive biological programs governing cell function.},
  file = {/home/harrisonpl/Documents/PDFs/Yordanov et al. - 2016 - A method to identify and analyze biological programs through automated reasoning.pdf},
  journal = {npj Systems Biology and Applications},
  number = {February}
}

@article{yuWordAttentionUsing2019,
  ids = {Yu2019},
  title = {Beyond Word Attention: {{Using}} Segment Attention in Neural Relation Extraction},
  author = {Yu, Bowen and Zhang, Zhenyu and Liu, Tingwen and Wang, Bin and Li, Sujian and Li, Quangang},
  year = {2019},
  volume = {2019-Augus},
  pages = {5401--5407},
  issn = {10450823},
  doi = {10.24963/ijcai.2019/750},
  abstract = {Relation extraction studies the issue of predicting semantic relations between pairs of entities in sentences. Attention mechanisms are often used in this task to alleviate the inner-sentence noise by performing soft selections of words independently. Based on the observation that information pertinent to relations is usually contained within segments (continuous words in a sentence), it is possible to make use of this phenomenon for better extraction. In this paper, we aim to incorporate such segment information into neural relation extractor. Our approach views the attention mechanism as linear-chain conditional random fields over a set of latent variables whose edges encode the desired structure, and regards attention weight as the marginal distribution of each word being selected as a part of the relational expression. Experimental results show that our method can attend to continuous relational expressions without explicit annotations, and achieve the state-of-the-art performance on the large-scale TACRED dataset.},
  file = {/home/harrisonpl/Documents/PDFs/Yu et al. - 2019 - Beyond word attention.pdf},
  isbn = {9780999241141},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  keywords = {Natural Language Processing: Information Extractio,Natural Language Processing: Natural Language Proc}
}

@misc{zaltaAnalogyAnalogicalReasoning,
  title = {Analogy and {{Analogical Reasoning}} ({{Stanford Encyclopedia}} of {{Philosophy}})},
  author = {Zalta, Edward},
  abstract = {An analogy is a comparison between two objects, or systems of objects, that highlights respects in which they are thought to be similar. Analogical reasoning is any type of thinking that relies upon an analogy. An analogical argument is an explicit representation of a form of analogical reasoning that cites accepted similarities between two systems to support the conclusion that some further similarity exists. In general (but not always), such arguments belong in the category of ampliative reasoning, since their conclusions do not follow with certainty but are only supported with varying degrees of strength. However, the proper characterization of analogical arguments is subject to debate (see \textsection 2.2). Analogical reasoning is fundamental to human thought and, arguably, to some nonhuman animals as well. Historically, analogical reasoning has played an important, but sometimes mysterious, role in a wide range of problem-solving contexts. The explicit use of analogical arguments, since antiquity, has been a distinctive feature of scientific, philosophical and legal reasoning. This article focuses primarily on the nature, evaluation and justification of analogical arguments. Related topics include metaphor, models in science, and precedent and analogy in legal reasoning.},
  file = {/home/harrisonpl/Documents/PDFs/Zalta - Analogy and Analogical Reasoning (Stanford Encyclopedia of Philosophy).pdf;/home/harrisonpl/Zotero/storage/DBGUNGKK/reasoning-analogy.html},
  howpublished = {https://plato.stanford.edu/entries/reasoning-analogy/},
  journal = {Stanford Encyclopedia of Philosophy}
}

@article{zhangBioWordVecImprovingBiomedical2019,
  ids = {Zhang2019f},
  title = {{{BioWordVec}}, Improving Biomedical Word Embeddings with Subword Information and {{MeSH}}},
  author = {Zhang, Yijia and Chen, Qingyu and Yang, Zhihao and Lin, Hongfei and Lu, Zhiyong},
  year = {2019},
  volume = {6},
  pages = {52},
  publisher = {{Springer US}},
  issn = {20524463},
  doi = {10.1038/s41597-019-0055-0},
  abstract = {Distributed word representations have become an essential foundation for biomedical natural language processing (BioNLP), text mining and information retrieval. Word embeddings are traditionally computed at the word level from a large corpus of unlabeled text, ignoring the information present in the internal structure of words or any information available in domain specific structured resources such as ontologies. However, such information holds potentials for greatly improving the quality of the word representation, as suggested in some recent studies in the general domain. Here we present BioWordVec: an open set of biomedical word vectors/embeddings that combines subword information from unlabeled biomedical text with a widely-used biomedical controlled vocabulary called Medical Subject Headings (MeSH). We assess both the validity and utility of our generated word embeddings over multiple NLP tasks in the biomedical domain. Our benchmarking results demonstrate that our word embeddings can result in significantly improved performance over the previous state of the art in those challenging tasks.},
  file = {/home/harrisonpl/Documents/PDFs/Zhang et al. - 2019 - BioWordVec, improving biomedical word embeddings with subword information and.pdf},
  journal = {Scientific data},
  keywords = {Embeddings,Favorite,HPL comprehensive exam,Relation extraction,Word embedding},
  mendeley-tags = {Embeddings,HPL comprehensive exam,Relation extraction,Word embedding},
  number = {1}
}

@article{zhangInteractiveLearningJoint2020,
  title = {Interactive Learning for Joint Event and Relation Extraction},
  author = {Zhang, Jingli and Hong, Yu and Zhou, Wenxuan and Yao, Jianmin and Zhang, Min},
  year = {2020},
  month = feb,
  volume = {11},
  pages = {449--461},
  issn = {1868808X},
  doi = {10.1007/s13042-019-00985-8},
  abstract = {We tackle the problems of both event and entity relation extraction, and come up with a novel method to implement joint extraction: iteratively interactive learning. This method is motivated by the empirical findings as below: the extracted event attributes (e.g., trigger and event type) can be used as the reliable features for the recognition of entity relation types, and vice versa. Accordingly, on one hand, we utilize the predicted event attributes (by a certain event extraction system) to remodel the distributed representations of features for entity relation extraction, and on the other hand, we use entity relations (recognized by a certain relation extraction system) to remodel the features for event extraction. This enables a double-channel task-independent joint model with an interactive learning: learning events for relation extraction, and meanwhile learning relations for event extraction. In practice, we perform the interactive learning in an iterative manner, so as to boost the joint model progressively. Methodologically, we take the neural network of bidirectional long short-term memory (Bi-LSTM) for learning event and relation respectively. And as usual, the attention mechanism is used. In our experiments, the automatic content extraction corpus is used for the evaluation of the proposed method. Such a corpus consists of event, entity and relation samples with gold-standard attribute tags. Experimental results show that our method outperforms the baselines (Bi-LSTMs with attention without interactive learning) in both event and relation extraction tasks, yielding performance gains of about 1.6\% and 1.8\% F-scores respectively, at the condition of low-resource setting.},
  journal = {International Journal of Machine Learning and Cybernetics},
  keywords = {\#nosource,Event detection,Interactive learning,Relation extraction},
  number = {2}
}

@article{zhangLongtailRelationExtraction2019,
  ids = {Zhang2019b},
  title = {Long-Tail {{Relation Extraction}} via {{Knowledge Graph Embeddings}} and {{Graph Convolution Networks}}},
  author = {Zhang, Ningyu and Deng, Shumin and Sun, Zhanlin and Wang, Guanying and Chen, Xi and Zhang, Wei and Chen, Huajun},
  year = {2019},
  pages = {3016--3025},
  doi = {10.18653/v1/n19-1306},
  abstract = {We propose a distance supervised relation extraction approach for long-tailed, imbalanced data which is prevalent in real-world settings. Here, the challenge is to learn accurate "few-shot" models for classes existing at the tail of the class distribution, for which little data is available. Inspired by the rich semantic correlations between classes at the long tail and those at the head, we take advantage of the knowledge from data-rich classes at the head of the distribution to boost the performance of the data-poor classes at the tail. First, we propose to leverage implicit relational knowledge among class labels from knowledge graph embeddings and learn explicit relational knowledge using graph convolution networks. Second, we integrate that relational knowledge into relation extraction model by coarse-to-fine knowledge-aware attention mechanism. We demonstrate our results for a large-scale benchmark dataset which show that our approach significantly outperforms other baselines, especially for long-tail relations.},
  annotation = {\_eprint: 1903.01306},
  archivePrefix = {arXiv},
  arxivid = {1903.01306},
  eprint = {1903.01306},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Zhang et al. - 2019 - Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph.pdf}
}

@article{zhangOpenKIIntegratingOpen2019,
  ids = {Zhang2019c},
  title = {{{OpenKI}}: {{Integrating Open Information Extraction}} and {{Knowledge Bases}} with {{Relation Inference}}},
  author = {Zhang, Dongxu and Mukherjee, Subhabrata and Lockard, Colin and Dong, Xin Luna and McCallum, Andrew},
  year = {2019},
  abstract = {In this paper, we consider advancing web-scale knowledge extraction and alignment by integrating OpenIE extractions in the form of (subject, predicate, object) triples with Knowledge Bases (KB). Traditional techniques from universal schema and from schema mapping fall in two extremes: either they perform instance-level inference relying on embedding for (subject, object) pairs, thus cannot handle pairs absent in any existing triples; or they perform predicate-level mapping and completely ignore background evidence from individual entities, thus cannot achieve satisfying quality. We propose OpenKI to handle sparsity of OpenIE extractions by performing instance-level inference: for each entity, we encode the rich information in its neighborhood in both KB and OpenIE extractions, and leverage this information in relation inference by exploring different methods of aggregation and attention. In order to handle unseen entities, our model is designed without creating entity-specific parameters. Extensive experiments show that this method not only significantly improves state-of-the-art for conventional OpenIE extractions like ReVerb, but also boosts the performance on OpenIE from semi-structured data, where new entity pairs are abundant and data are fairly sparse.},
  annotation = {\_eprint: 1904.12606},
  archivePrefix = {arXiv},
  arxivid = {1904.12606},
  eprint = {1904.12606},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Zhang et al. - 2019 - OpenKI.pdf}
}

@article{zhangPAGWASManuallyCurated2020,
  ids = {Zhang2020a},
  title = {{{PAGWAS}}: A Manually Curated Web-Based Knowledge Database of {{GWAS}} Pathway Analysis},
  author = {Zhang, Ningyi and Hu, Yang},
  year = {2020},
  pages = {1814--1821},
  doi = {10.1109/bibm47256.2019.8982970},
  file = {/home/harrisonpl/Documents/PDFs/Zhang, Hu - 2020 - PAGWAS.pdf},
  isbn = {9781728118673},
  keywords = {ȅ pathway}
}

@article{zhangProteinproteinInteractionInference2016,
  title = {Protein-Protein Interaction Inference Based on Semantic Similarity of {{Gene Ontology}} Terms},
  author = {Zhang, Shu Bo and Tang, Qiang Rong},
  year = {2016},
  volume = {401},
  abstract = {Identifying protein-protein interactions is important in molecular biology. Experimental methods to this issue have their limitations, and computational approaches have attracted more and more attentions from the biological community. The semantic similarity derived from the Gene Ontology (GO) annotation has been regarded as one of the most powerful indicators for protein interaction. However, conventional methods based on GO similarity fail to take advantage of the specificity of GO terms in the ontology graph. We proposed a GO-based method to predict protein-protein interaction by integrating different kinds of similarity measures derived from the intrinsic structure of GO graph. We extended five existing methods to derive the semantic similarity measures from the descending part of two GO terms in the GO graph, then adopted a feature integration strategy to combines both the ascending and the descending similarity scores derived from the three sub-ontologies to construct various kinds of features to characterize each protein pair. Support vector machines (SVM) were employed as discriminate classifiers, and five-fold cross validation experiments were conducted on both human and yeast protein-protein interaction datasets to evaluate the performance of different kinds of integrated features, the experimental results suggest the best performance of the feature that combines information from both the ascending and the descending parts of the three ontologies. Our method is appealing for effective prediction of protein-protein interaction.},
  keywords = {\#nosource,Ascending similarity,Descending similarity,Feature integration,Gene Ontology,hunter lab,Protein-protein interaction,Protein–protein interaction,Support vector machine}
}

@article{zhangSimpleFastAlgorithms1989,
  title = {Simple Fast Algorithms for the Editing Distance between Trees and Related Problems},
  author = {Zhang, Kaizhong and Shasha, Dennis},
  year = {1989},
  volume = {18},
  abstract = {Ordered labeled trees are trees in which the left-to-right order among siblings is. significant. The distance between two ordered trees is considered to be the weighted number of edit operations (insert, delete, and modify) to transform one tree to another. The problem of approximate tree matching is also considered. Specifically, algorithms are designed to answer the following kinds of questions: 1. What is the distance between two trees? 2. What is the minimum distance between T and T when zero or more subtrees can be removed from T2 3. Let the pruning of a tree at node n mean removing all the descendants of node n. The analogous question for prunings as for subtrees is answered. A dynamic programming algorithm is presented to solve the three questions in sequential time O(I Tll x IT2lxmin (depth ( Tt), leaves ( T)) x min (depth(T2), leaves(T2))) and space O(Ir, x lT21) compared with o(I T,I IT=I x(depth(T)): x (depth(T2))) for the best previous published algorithm due to Tai [J. Assoc. Comput. Mach., 26 (1979), pp. 422-433]. Further, the algorithm presented here can be parallelized to give},
  file = {/home/harrisonpl/Documents/PDFs/Zhang, Shasha - 1989 - Simple fast algorithms for the editing distance between trees and related.pdf;/home/harrisonpl/Zotero/storage/XSQ52LPF/summary.html},
  journal = {Siam J. Comput},
  number = {6}
}

@article{zhangSystemsPharmacologyInvestigation2019,
  ids = {Zhang2019a,Zhang2019d},
  title = {Systems {{Pharmacology}} for {{Investigation}} of the {{Mechanisms}} of {{Action}} of {{Traditional Chinese Medicine}} in {{Drug Discovery}}},
  author = {Zhang, Wenjuan and Huai, Ying and Miao, Zhiping and Qian, Airong and Wang, Yonghua},
  year = {2019},
  month = jul,
  volume = {10},
  publisher = {{Frontiers Media SA}},
  issn = {1663-9812},
  doi = {10.3389/fphar.2019.00743},
  abstract = {As a traditional medical intervention in Asia and a complementary and alternative medicine in western countries, traditional Chinese medicine (TCM) has attracted global attention in life science field. TCM provides extensive natural resources for medicinal compounds and these resources are generally regarded as effective and safe for use in drug discovery. However, owing to the complexity of compounds and their related multiple targets of TCM, it remains difficult to dissect the mechanisms of action of herbal medicines at a holistic level. To solve the issue, in the review, we proposed a novel approach of systems pharmacology to identify the bioactive compounds, predict their related targets as well as to illustrate the molecular mechanisms of action of TCM. With a predominant focus on the mechanisms of actions of TCM, we also highlighted the application of the systems pharmacology approach for the prediction of drug combination and dynamic analysis, the synergistic effects of TCMs, formula dissection and theory analysis. In summary, the systems pharmacology method contributes to understand the complex interactions among biological systems, drugs and complex diseases from a network perspective. Consequently, systems pharmacology provides a novel approach to promote drug discovery in a precise manner and a systematic level, thus facilitating the modernization of TCM.},
  file = {/home/harrisonpl/Documents/PDFs/Zhang et al. - 2019 - Systems Pharmacology for Investigation of the Mechanisms of Action of.pdf},
  journal = {Frontiers in Pharmacology},
  keywords = {Mechanism of action,Pharmacology},
  mendeley-tags = {Mechanism of action,Pharmacology}
}

@article{zhangUnderstandingDeepLearning2019,
  ids = {Zhang2019e},
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Recht, Benjamin and Bengio, Samy and Hardt, Moritz and Vinyals, Oriol},
  year = {2019},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
  annotation = {\_eprint: 1611.03530},
  archivePrefix = {arXiv},
  arxivid = {1611.03530},
  eprint = {1611.03530},
  eprinttype = {arxiv},
  file = {/home/harrisonpl/Documents/PDFs/Zhang et al. - 2019 - Understanding deep learning requires rethinking generalization.pdf},
  journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings}
}

@article{zhanKernelApproachesDifferential2015,
  ids = {Zhan2015},
  title = {Kernel Approaches for Differential Expression Analysis of Mass Spectrometry-Based Metabolomics Data},
  author = {Zhan, Xiang and Patterson, Andrew D. and Ghosh, Debashis},
  year = {2015},
  volume = {16},
  issn = {14712105},
  doi = {10.1186/s12859-015-0506-3},
  abstract = {Background: Data generated from metabolomics experiments are different from other types of "-omics" data. For example, a common phenomenon in mass spectrometry (MS)-based metabolomics data is that the data matrix frequently contains missing values, which complicates some quantitative analyses. One way to tackle this problem is to treat them as absent. Hence there are two types of information that are available in metabolomics data: presence/absence of a metabolite and a quantitative value of the abundance level of a metabolite if it is present. Combining these two layers of information poses challenges to the application of traditional statistical approaches in differential expression analysis. Results: In this article, we propose a novel kernel-based score test for the metabolomics differential expression analysis. In order to simultaneously capture both the continuous pattern and discrete pattern in metabolomics data, two new kinds of kernels are designed. One is the distance-based kernel and the other is the stratified kernel. While we initially describe the procedures in the case of single-metabolite analysis, we extend the methods to handle metabolite sets as well. Conclusions: Evaluation based on both simulated data and real data from a liver cancer metabolomics study indicates that our kernel method has a better performance than some existing alternatives. An implementation of the proposed kernel method in the R statistical computing environment is available at http://works.bepress.com/debashis\_ghosh/60/.},
  file = {/home/harrisonpl/Documents/PDFs/Zhan et al. - 2015 - Kernel approaches for differential expression analysis of mass.pdf},
  journal = {BMC Bioinformatics},
  keywords = {Differential expression analysis,Distance-based kernel,Metabolite,Stratified kernel},
  number = {1},
  pmid = {25887233}
}

@article{zhavoronkovPotentialCOVID20193Clike2020,
  title = {Potential {{COVID}}-2019 {{3C}}-like {{Protease Inhibitors Designed Using Generative Deep Learning Approaches}}},
  author = {Zhavoronkov, Alex and Aladinskiy, Vladimir and Zhebrak, Alexander and Zagribelnyy, Bogdan and Terentiev, Victor and Bezrukov, Dmitry S. and Polykovskiy, Daniil and Shayakhmetov, Rim and Filimonov, Andrey and Orekhov, Philipp and Yan, Yilin and Popova, Olga and Vanhaelen, Quentin and Aliper, Alex and Ivanenkov, Yan},
  year = {2020},
  month = feb,
  publisher = {{ChemRxiv; http://web.archive.org/web/20200318211737/https://chemrxiv.org/articles/Potential\_2019-nCoV\_3C-like\_Protease\_Inhibitors\_Designed\_Using\_Generative\_Deep\_Learning\_Approaches/11829102}},
  doi = {10.26434/chemrxiv.11829102.v2},
  abstract = {The emergence of the 2019 novel coronavirus (COVID-19), for which there is no vaccine or any known effective treatment created a sense of urgency for novel drug discovery approaches. One of the most important COVID-19 protein targets is the 3C-like protease for which the crystal structure is known. Most of the immediate efforts are focused on drug repurposing of known clinically-approved drugs and virtual screening for the molecules available from chemical libraries that may not work well. For example, the IC50 of lopinavir, an HIV protease inhibitor, against the 3C-like protease is approximately 50 micromolar, which is far from ideal. In an attempt to address this challenge, on January 28th, 2020 Insilico Medicine decided to utilize a part of its generative chemistry pipeline to design novel drug-like inhibitors of COVID-19 and started generation on January 30th. It utilized three of its previously validated generative chemistry approaches: crystal-derived pocked-based generator, homology modelling-based generation, and ligand-based generation. Novel druglike compounds generated using these approaches were published at www.insilico.com/ncov-sprint/. Several molecules will be synthesized and tested using the internal resources; however, the team is seeking collaborations to synthesize, test, and, if needed, optimize the published molecules.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/harrisonpl/Documents/PDFs/Zhavoronkov et al. - 2020 - Potential COVID-2019 3C-like Protease Inhibitors Designed Using Generative Deep.pdf},
  keywords = {COVID-19},
  language = {en}
}

@inproceedings{zhouAttentionbasedBidirectionalLong2016,
  ids = {Zhou2016},
  title = {Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification},
  booktitle = {54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}, {{ACL}} 2016 - {{Short Papers}}},
  author = {Zhou, Peng and Shi, Wei and Tian, Jun and Qi, Zhenyu and Li, Bingchen and Hao, Hongwei and Xu, Bo},
  year = {2016},
  pages = {207--212},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/p16-2034},
  abstract = {Relation classification is an important semantic processing task in the field of natural language processing (NLP). State-of-the-art systems still rely on lexical resources such as WordNet or NLP systems like dependency parser and named entity recognizers (NER) to get high-level features. Another challenge is that important information can appear at any position in the sentence. To tackle these problems, we propose Attention-Based Bidirectional Long Short-Term Memory Networks(Att-BLSTM) to capture the most important semantic information in a sentence. The experimental results on the SemEval-2010 relation classification task show that our method outperforms most of the existing methods, with only word vectors.},
  file = {/home/harrisonpl/Documents/PDFs/Zhou et al. - 2016 - Attention-based bidirectional long short-term memory networks for relation.pdf},
  isbn = {978-1-5108-2759-2},
  keywords = {Natural language processing},
  mendeley-tags = {Natural language processing}
}

@article{zhouEasyGOGeneOntologybased2007,
  title = {{{EasyGO}}: {{Gene Ontology}}-Based Annotation and Functional Enrichment Analysis Tool for Agronomical Species},
  shorttitle = {{{EasyGO}}},
  author = {Zhou, Xin and Su, Zhen},
  year = {2007},
  month = jul,
  volume = {8},
  pages = {246},
  issn = {1471-2164},
  doi = {10.1186/1471-2164-8-246},
  abstract = {It is always difficult to interpret microarray results. Recently, a handful of tools have been developed to meet this need, but almost none of them were designed to support agronomical species.},
  file = {/home/harrisonpl/Documents/PDFs/Zhou, Su - 2007 - EasyGO.pdf},
  journal = {BMC Genomics},
  language = {en},
  number = {1}
}

@article{zhuDynamicLabelCorrection2019,
  ids = {Zhu2019},
  title = {Dynamic {{Label Correction}} for {{Distant Supervision Relation Extraction}} via {{Semantic Similarity}}},
  author = {Zhu, Xinyu and Liu, Gongshen and Su, Bo and Nees, Jan Pan},
  year = {2019},
  volume = {11839 LNAI},
  pages = {16--27},
  issn = {16113349},
  doi = {10.1007/978-3-030-32236-6_2},
  abstract = {It was found that relation extraction (RE) suffered from the lack of data. A widely used solution is to use distant supervision, but it brings many wrong labeled sentences. Previous work performed bag-level training to reduce the effect of noisy data. However, these methods are suboptimal because they cannot handle the situation where all the sentences in a bag are wrong labeled. The best way to reduce noise is to recognize the wrong labels and correct them. In this paper, we propose a novel model focusing on dynamically correcting wrong labels, which can train models at sentence level and improve the quality of the dataset without reducing its quantity. A semantic similarity module and a new label correction algorithm are designed. We combined semantic similarity and classification probability to evaluate the original label, and correct it if it is wrong. The proposed method works as an additional module that can be applied to any classification models. Experiments show that the proposed method can accurately correct wrong labels, both false positive and false negative, and greatly improve the performance of relation classification comparing to state-of-the-art systems.},
  file = {/home/harrisonpl/Documents/PDFs/Zhu et al. - 2019 - Dynamic Label Correction for Distant Supervision Relation Extraction via.pdf},
  isbn = {9783030322359},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords = {Label correction,Relation extraction,Semantic similarity}
}

@article{zhuPersistentHomologyTutorial,
  title = {Persistent {{Homology Tutorial}}},
  author = {Zhu, Xiaojin},
  pages = {139},
  file = {/home/harrisonpl/Documents/PDFs/Zhu - Persistent Homology Tutorial.pdf},
  language = {en}
}

@article{zhuPersistentHomologyTutorial2013,
  ids = {Zhu2013},
  title = {Persistent {{Homology Tutorial Persistent}} Homology {{A}} Rapidly Growing Branch of Topology},
  author = {Zhu, Xiaojin},
  year = {2013},
  volume = {2013},
  file = {/home/harrisonpl/Documents/PDFs/Zhu - 2013 - Persistent Homology Tutorial Persistent homology A rapidly growing branch of.pdf}
}

@article{zhuProbabilisticModelMining2005,
  ids = {Zhu2005},
  title = {A Probabilistic Model for Mining Implicit 'chemical Compound-Gene' Relations from Literature},
  author = {Zhu, Shanfeng and Okuno, Yasushi and Tsujimoto, Gozoh and Mamitsuka, Hiroshi},
  year = {2005},
  volume = {21},
  pages = {ii245--251},
  issn = {13674803},
  doi = {10.1093/bioinformatics/bti1141},
  abstract = {Motivation: The importance of chemical compounds has been emphasized more in molecular biology, and 'chemical genomics' has attracted a great deal of attention in recent years. Thus an important issue in current molecular biology is to identify biological-related chemical compounds (more specifically, drugs) and genes. Co-occurrence of biological entities in the literature is a simple, comprehensive and popular technique to find the association of these entities. Our focus is to mine implicit 'chemical compound and gene' relations from the co-occurrence in the literature. Results: We propose a probabilistic model, called the mixture aspect model (MAM), and an algorithm for estimating its parameters to efficiently handle different types of co-occurrence datasets at once. We examined the performance of our approach not only by a cross-validation using the data generated from the MEDLINE records but also by a test using an independent human-curated dataset of the relationships between chemical compounds and genes in the ChEBI database. We performed experimentation on three different types of co-occurrence datasets (i.e. compound-gene, gene-gene and compound-compound co-occurrences) in both cases. Experimental results have shown that MAM trained by all datasets outperformed any simple model trained by other combinations of datasets with the difference being statistically significant in all cases. In particular, we found that incorporating compound-compound co-occurrences is the most effective in improving the predictive performance. We finally computed the likelihoods of all unknown compound-gene (more specifically, drug-gene) pairs using our approach and selected the top 20 pairs according to the likelihoods. We validated them from biological, medical and pharmaceutical viewpoints. \textcopyright{} The Author 2005. Published by Oxford University Press. All rights reserved.},
  file = {/home/harrisonpl/Documents/PDFs/Zhu et al. - 2005 - A probabilistic model for mining implicit 'chemical compound-gene' relations.pdf},
  journal = {Bioinformatics},
  keywords = {Artificial Intelligence,Biopolymers,Computer Simulation,Genes,Information Storage and Retrieval,Models,Natural Language Processing,Periodicals as Topic,Statistical,Structure-Activity Relationship},
  number = {SUPPL. 2},
  pmid = {16204113}
}

@article{zotero-17,
  abstract = {Here we describe the 2CT algorithm implemented in the ddCt package. The package is designed for the data analysis of quantitative real\{time PCR (qRT\{PCR) experiemtns in Bioconductor. With the ddCt package, one can acquire the relative expression of the target gene in di erent samples. This vignette mainly dicusses the principles of the ddCt algorithm and demonstrates the functionality of the package with a compact example. Another vignette in the package, rtPCR-usage, gives instructions to call the script for end\{to\{end analysis.\vphantom{\}\}\}\}}},
  keywords = {\#nosource}
}

@article{zotero-67,
  abstract = {Here we describe the 2CT algorithm implemented in the ddCt package. The package is designed for the data analysis of quantitative real\{time PCR (qRT\{PCR) experiemtns in Bioconductor. With the ddCt package, one can acquire the relative expression of the target gene in di erent samples. This vignette mainly dicusses the principles of the ddCt algorithm and demonstrates the functionality of the package with a compact example. Another vignette in the package, rtPCR-usage, gives instructions to call the script for end\{to\{end analysis.\vphantom{\}\}\}\}}},
  keywords = {\#nosource}
}

@article{zotero-71,
  abstract = {Here we describe the 2CT algorithm implemented in the ddCt package. The package is designed for the data analysis of quantitative real\{time PCR (qRT\{PCR) experiemtns in Bioconductor. With the ddCt package, one can acquire the relative expression of the target gene in di erent samples. This vignette mainly dicusses the principles of the ddCt algorithm and demonstrates the functionality of the package with a compact example. Another vignette in the package, rtPCR-usage, gives instructions to call the script for end\{to\{end analysis.\vphantom{\}\}\}\}}},
  keywords = {\#nosource}
}


